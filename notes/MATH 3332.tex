\documentclass{huhtakm-template-book-v2}
\usepackage{tikz, pgfplots}
\DeclareMathOperator{\dist}{dist}
\DeclareMathOperator{\Tr}{Tr}
\DeclareMathOperator{\supp}{supp}
\DeclareMathOperator{\sgn}{sgn}
\DeclareMathOperator*{\argmax}{argmax}
\DeclareMathOperator*{\argmin}{argmin}
\DeclareMathOperator*{\median}{median}
\setlength{\parindent}{0pt}
\title{
	\Huge MATH 3332: Data Analytic Tools
}
\author{
	HU-HTAKM\\
	\small Website: \url{https://htakm.github.io/htakm_test/}
}
\date{
	Last major change: October 18, 2025\\
	Last small update: October 18, 2025
}
\begin{document}
\maketitle
\tableofcontents

\chapter{Introduction}
    \label{Chapter 1: Introduction}
    Machine Learning is a type of artificial intelligence that focuses on the development of algorithms and models to perform tasks without being explicitly programmed to do so. Machine learning algorithms create a model based on sample data, known as training data. There are three common types of machine learning:
    \begin{enumerate}
        \item Supervised learning: Classification, Regression\\
        Data: $N$ input-output pairs
        \begin{equation*}
            (\mathbf{x}_{i}, \mathbf{y}_{i}) \qquad \text{for } \mathbf{x}_{i} \in X, \mathbf{y}_{i} \in Y, \ i = 1, \cdots, N.
        \end{equation*}
        Goal: Find a function map $f: X \to Y$ such that:
        \begin{equation*}
            f(\mathbf{x}_{i}) \approx \mathbf{y}_{i} \qquad \text{for } i = 1, \cdots, N.
        \end{equation*}
        If a new input $\mathbf{x}$ is provided, $f(\mathbf{x})$ should accurately predict the label of $\mathbf{x}$.
        \item Unsupervised learning: Clustering, Self-supervised Learning\\
        Data: $N$ inputs without labels
        \begin{equation*}
            \mathbf{x}_{i} \qquad \text{for } \mathbf{x}_{i} \in X, \ i = 1, \cdots, N.
        \end{equation*}
        Goal: Different applications have their own goals. For example, in the case of denoising, find a function map $f$ such that:
        \begin{equation*}
            f(\mathbf{x}_{i} + \boldsymbol{\varepsilon}_{i}) = \mathbf{x}_{i} \qquad \text{for } i = 1, \cdots, N,
        \end{equation*}
        where $\boldsymbol{\varepsilon}_{i}$ are noise vectors.
        \item Reinforcement learning (Reinforcement learning algorithms are usually iterative algorithms).
    \end{enumerate}
    In general, we want to find a good function $f$ that maps the training data well while generalizing to other inputs.
    \begin{defn}
        The set of all 'good' candidate functions (models) is called the \textbf{hypothesis space}.
    \end{defn}
    In our case studies, we will follow Pedro Domingos' definition of machine learning:
    \begin{equation*}
        \text{Learning} = \text{Representation} + \text{Evaluation} + \text{Optimization}.
    \end{equation*}
    \begin{enumerate}
        \item Representation: Mainly focuses on 'vector' representations.
        \begin{enumerate}
            \item How can we effectively represent the input data $\mathbf{x}_{i}$?
            \item How can we represent the function $f$?
        \end{enumerate}
        \item Evaluation: Evaluate the problem.
        \begin{enumerate}
            \item How do we define 'the best' function in the hypothesis space?\\
            We need to define a function $f': f \to \mathbb{R}$ to compare.
            \item How do we define 'the best' representation of the input data?
        \end{enumerate}
        \item Optimization: Find the optimal model.
        \begin{enumerate}
            \item How can we obtain the optimal solution numerically using a computer?
            \item Is convex optimization feasible? (Some problems involve non-convex optimization.)
        \end{enumerate}
    \end{enumerate}

\chapter{Vector spaces, metrics, limits, and convergence}
    \label{Chapter 2: Vector spaces, metrics, limits, and convergence}
\section{Vector spaces (linear space)}
    \begin{defn}
        A \textbf{vector space} over $\mathbb{R}$ is a set $V$ together with two operations:
        \begin{enumerate}
            \item Addition: For all $\mathbf{u}, \mathbf{v} \in V$, $\mathbf{u} + \mathbf{v} \in V$.
            \item Scalar multiplication: For all $\alpha \in \mathbb{R}$ and $\mathbf{v} \in V$, $\alpha \mathbf{v} \in V$.
        \end{enumerate}
        These two operations satisfy the following eight properties for all $\mathbf{u}, \mathbf{v}, \mathbf{w} \in V$ and $\alpha, \beta \in \mathbb{R}$:
        \begin{align*}
            \tag{Addition Commutativity}
            &(+1) & &\mathbf{u} + \mathbf{v} = \mathbf{v} + \mathbf{u},\\
            \tag{Addition Associativity}
            &(+2) & &(\mathbf{u} + \mathbf{v}) + \mathbf{w} = \mathbf{u} + (\mathbf{v} + \mathbf{w}),\\
            \tag{Zero Exists}
            &(+3) & &\text{There exists } \mathbf{0} \in V \text{ such that } \mathbf{u} + \mathbf{0} = \mathbf{u},\\
            \tag{Additive Inverse Exists}
            &(+4) & &\text{For every } \mathbf{u} \in V, \text{ there exists } \mathbf{u}' \in V \text{ such that } \mathbf{u} + \mathbf{u}' = \mathbf{0} \quad (\mathbf{u}' = -\mathbf{u}),\\
            \tag{Multiplication Associativity}
            &(\cdot 1) & &(\alpha \beta)\mathbf{u} = \alpha(\beta \mathbf{u}),\\
            \tag{Unity}
            &(\cdot 2) & & 1 \cdot \mathbf{u} = \mathbf{u},\\
            \tag{Distributivity 1}
            &(\cdot 3) & & \alpha(\mathbf{u} + \mathbf{v}) = \alpha \mathbf{u} + \alpha \mathbf{v},\\
            \tag{Distributivity 2}
            &(\cdot 4) & & (\alpha + \beta)\mathbf{u} = \alpha \mathbf{u} + \beta \mathbf{u}.
        \end{align*}
    \end{defn}
    \begin{rem}
        A vector space over the complex domain $\mathbb{C}$ can be defined in a similar way.
    \end{rem}
    \begin{eg}
        The set of real numbers $\mathbb{R}$, with the standard addition and multiplication of real numbers, forms a vector space.
    \end{eg}
    \begin{eg}
        The $n$-dimensional Euclidean space $\mathbb{R}^{n}$, with the following operations, forms a vector space over $\mathbb{R}$:
        \begin{align*}
            +&: & &\text{For all } \mathbf{x} = \begin{pmatrix}x_{1}\\\vdots\\x_{n}\end{pmatrix}, \mathbf{y} = \begin{pmatrix}y_{1}\\\vdots\\y_{n}\end{pmatrix} \in \mathbb{R}^{n},\ \mathbf{x} + \mathbf{y} = \begin{pmatrix}x_{1} + y_{1}\\\vdots\\x_{n} + y_{n}\end{pmatrix} \in \mathbb{R}^{n},\\
            \bullet&: & &\text{For all } \mathbf{x} = \begin{pmatrix}x_{1}\\\vdots\\x_{n}\end{pmatrix} \in \mathbb{R}^{n} \text{ and } \alpha \in \mathbb{R},\ \alpha \cdot \mathbf{x} = \begin{pmatrix}\alpha x_{1}\\\vdots\\\alpha x_{n}\end{pmatrix} \in \mathbb{R}^{n}.
        \end{align*}
    \end{eg}
    \begin{rem}
        Many types of input data can be modeled as vectors in $\mathbb{R}^{n}$, such as:
        \begin{enumerate}
            \item Digital signals of length $n$,
            \item Stock prices over $n$ time intervals,
            \item $n$ different features or attributes of a single object.
        \end{enumerate}
    \end{rem}
    \newpage
    
    \begin{eg}
        All real $m \times n$ matrices $\mathbb{R}^{m \times n}$ with
        \begin{align*}
            +&: & &\text{For all } \mathbf{A} = \begin{pmatrix}
                a_{11} & \hdots & a_{1n}\\
                \vdots & \ddots & \vdots\\
                a_{m1} & \hdots & a_{mn}
            \end{pmatrix}, \mathbf{B} = \begin{pmatrix}
                b_{11} & \hdots & b_{1n}\\
                \vdots & \ddots & \vdots\\
                b_{m1} & \hdots & b_{mn}
            \end{pmatrix} \in \mathbb{R}^{m \times n},\\
            & & &\text{we have } \mathbf{A} + \mathbf{B} = \begin{pmatrix}
                a_{11} + b_{11} & \hdots & a_{1n} + b_{1n}\\
                \vdots & \ddots & \vdots\\
                a_{m1} + b_{m1} & \hdots & a_{mn} + b_{mn}
            \end{pmatrix} \in \mathbb{R}^{m \times n},\\
            \bullet&: & &\text{For all } \mathbf{B} = \begin{pmatrix}
                b_{11} & \hdots & b_{1n}\\
                \vdots & \ddots & \vdots\\
                b_{m1} & \hdots & b_{mn}
            \end{pmatrix} \in \mathbb{R}^{m \times n} \text{ and } \alpha \in \mathbb{R}, \text{ we have } \alpha \cdot \mathbf{B} = \begin{pmatrix}
                \alpha b_{11} & \hdots & \alpha b_{1n}\\
                \vdots & \ddots & \vdots\\
                \alpha b_{m1} & \hdots & \alpha b_{mn}
            \end{pmatrix} \in \mathbb{R}^{m \times n}
        \end{align*}
        is a vector space over $\mathbb{R}$.
    \end{eg}
    \begin{rem}
        This vector space is equivalent to $\mathbb{R}^{mn}$:
        \begin{equation*}
            \begin{pmatrix}
                x_{11} & \hdots & x_{1n}\\
                x_{21} & \hdots & x_{2n}\\
                \vdots & \ddots & \vdots\\
                x_{m1} & \hdots & x_{mn}
            \end{pmatrix} \xrightarrow{\text{vectorization}} \begin{pmatrix}
                x_{11}\\\vdots\\x_{1n}\\x_{21}\\\vdots\\x_{2n}\\\vdots\\x_{mn}
            \end{pmatrix} \in \mathbb{R}^{mn}.
        \end{equation*}
    \end{rem}
    \begin{rem}
        An $m \times n$ matrix can be used to represent a black-and-white digital image.
    \end{rem}
    \begin{eg}
        All real 3-arrays of size $m \times n \times \ell$, $\mathbb{R}^{m \times n \times \ell}$, with
        \begin{align*}
            +&: & &\text{For all } X = (x_{ijk})_{i,j,k}, Y = (y_{ijk})_{i,j,k} \in \mathbb{R}^{m \times n \times \ell}, \text{ we have } X + Y = (x_{ijk} + y_{ijk})_{i,j,k} \in \mathbb{R}^{m \times n \times \ell},\\
            \bullet&: & &\text{For all } X = (x_{ijk})_{i,j,k} \in \mathbb{R}^{m \times n \times \ell} \text{ and } \alpha \in \mathbb{R}, \text{ we have } \alpha \cdot X = (\alpha x_{ijk})_{i,j,k} \in \mathbb{R}^{m \times n \times \ell}
        \end{align*}
        is a vector space over $\mathbb{R}$.
    \end{eg}
    \begin{rem}
        Similar to matrices, this vector space is equivalent to $\mathbb{R}^{mn\ell}$.
    \end{rem}
    \begin{rem}
        Many types of data can be modeled by this vector space $\mathbb{R}^{m \times n \times \ell}$, such as:
        \begin{enumerate}
            \item Color images with 3 color channels (RGB) ($\ell = 3$),
            \item Black-and-white videos with $\ell$ frames, each of size $m \times n$.
        \end{enumerate}
        More complex data, such as color videos, are represented as 4-arrays. These arrays are usually collectively called \textbf{tensors}.
    \end{rem}
    \begin{eg}
        Consider the set of all strings. We can quickly see that strings do not satisfy the commutativity property:
        \begin{equation*}
            \text{`Standing'} = \text{`Stand'} + \text{`ing'} \neq \text{`ing'} + \text{`Stand'} = \text{`ingStand'}.
        \end{equation*}
        Therefore, vector spaces cannot be used to model text data.
    \end{eg}
    \newpage
    
    \begin{eg}
        The set of all continuous functions on $[a, b]$, denoted by:
        \begin{equation*}
            \mathcal{C}[a, b] = \{f : f \text{ is a continuous function on } [a, b]\},
        \end{equation*}
        with, for all $t \in [a, b]$,
        \begin{align*}
            +&: & &\text{For all } f, g \in \mathcal{C}[a, b], \text{ we have } (f + g)(t) = f(t) + g(t) \in \mathcal{C}[a, b],\\
            \bullet&: & &\text{For all } f \in \mathcal{C}[a, b] \text{ and } \alpha \in \mathbb{R}, \text{ we have } (\alpha f)(t) = \alpha f(t) \in \mathcal{C}[a, b].
        \end{align*}
        is a vector space over $\mathbb{R}$. It is referred to as a \textbf{function space}.
    \end{eg}
    \begin{rem}
        $\mathcal{C}[a, b]$ can be considered as a hypothesis space of a learner with one input and one output. Given a dataset of $x_{i} \in [a, b]$ and $y_{i} \in \mathbb{R}$, find the function $f \in \mathcal{C}[a, b]$ such that $f(x_{i}) \approx y_{i}$ for all $i$.
    \end{rem}
    \begin{eg}
        The infinite sequences:
        \begin{equation*}
            \ell_{\infty} = \left\{\begin{pmatrix}
                a_{1}\\\vdots\\a_{n}\\\vdots
            \end{pmatrix} : \text{There exists } c < \infty \text{ such that } \abs{a_{i}} \leq c \text{ for any } i\right\},
        \end{equation*}
        with
        \begin{align*}
            +&: & &\text{For all } \mathbf{x} = \begin{pmatrix}x_{1}\\\vdots\\x_{n}\\\vdots\end{pmatrix}, \mathbf{y} = \begin{pmatrix}y_{1}\\\vdots\\y_{n}\\\vdots\end{pmatrix} \in \ell_{\infty}, \text{ we have } \mathbf{x} + \mathbf{y} = \begin{pmatrix}x_{1} + y_{1}\\\vdots\\x_{n} + y_{n}\\\vdots\end{pmatrix} \in \ell_{\infty},\\
            \bullet&: & &\text{For all } \mathbf{x} = \begin{pmatrix}x_{1}\\\vdots\\x_{n}\\\vdots\end{pmatrix} \in \ell_{\infty} \text{ and } \alpha \in \mathbb{R}, \text{ we have } \alpha \cdot \mathbf{x} = \begin{pmatrix}\alpha x_{1}\\\vdots\\\alpha x_{n}\\\vdots\end{pmatrix} \in \ell_{\infty}.
        \end{align*}
        is a vector space over $\mathbb{R}$.
    \end{eg}
    \newpage
    
\section{Metrics on vector space}
    We can convert some types of input data into vector space. However, how do we determine the distance between two vectors? Our goal is to define the distance in order to perform calculus.

    Let $V$ be a vector space and $\mathbf{u}, \mathbf{v} \in V$. We want to find a function such that:
    \begin{equation*}
        \dist(\mathbf{u}, \mathbf{v}) = \dist(\mathbf{0}, \mathbf{u} - \mathbf{v}) = \text{length of } \mathbf{u} - \mathbf{v}.
    \end{equation*}
    Therefore, to define the distance, we only need to define the length of vectors.
    \begin{defn}
        Let $V$ be a vector space over $\mathbb{R}$. A \textbf{norm} on $V$ is a function $\norm{\cdot}: V \to \mathbb{R}$ such that for $\mathbf{x}, \mathbf{y} \in V$,
        \begin{enumerate}
            \item $\norm{\mathbf{x}} \geq 0$ and $\norm{\mathbf{x}} = 0 \Longleftrightarrow \mathbf{x} = \mathbf{0}$.
            \item $\norm{\alpha \mathbf{x}} = \abs{\alpha} \norm{\mathbf{x}}$ for $\alpha \in \mathbb{R}$.
            \item $\norm{\mathbf{x} + \mathbf{y}} \leq \norm{\mathbf{x}} + \norm{\mathbf{y}}$. 
        \end{enumerate}
        The ordered pair $(V, \norm{\cdot})$ is called a \textbf{normed vector space}.
    \end{defn}
    \begin{rem}
        If it is clear from the context which norm is intended, then it is common to denote the normed vector space by $V$.
    \end{rem}
    \begin{rem}
        $\norm{\mathbf{x}}$ represents the \textbf{length} of $\mathbf{x}$.
    \end{rem}
    \begin{rem}
        The distance between $\mathbf{x}$ and $\mathbf{y}$ can now be defined as $\dist(\mathbf{x}, \mathbf{y}) = \norm{\mathbf{x} - \mathbf{y}}$.
    \end{rem}
    \begin{eg}
        If we set $V = \mathbb{R}$, the following can be norms on $\mathbb{R}$ for all $x \in \mathbb{R}$:
        \begin{enumerate}
            \item $\norm{x} = \abs{x}$,
            \item $\norm{x} = \frac{1}{2} \abs{x}$,
            \item $\norm{x} = c \abs{x}$ for some $c > 0$.
        \end{enumerate}
    \end{eg}
    \begin{rem}
        In fact, there are infinitely many norms on the same vector space.
    \end{rem}
    \begin{eg}
        For $\mathbf{x} \in \mathbb{R}^{n}$, the \textbf{Manhattan norm} ($1$-norm or $L_{1}$-norm) defined by:
        \begin{equation*}
            \pnorm[1]{\mathbf{x}} = \sum_{i=1}^{n} \abs{x_{i}}
        \end{equation*}
        is a norm on $\mathbb{R}^{n}$. The induced distance $\pnorm[1]{\mathbf{x} - \mathbf{y}}$ for $\mathbf{x}, \mathbf{y} \in V$ is called the \textbf{Manhattan distance}.
    \end{eg}
    \begin{proofing}
        \begin{enumerate}
            \item For any $\mathbf{x}, \in \mathbb{R}^{n}$,
            \begin{equation*}
                \pnorm[1]{\mathbf{x}} = \sum_{i=1}^{n} \abs{x_{i}} \geq 0.
            \end{equation*}
            Moreover,
            \begin{align*}
                \pnorm[1]{\mathbf{x}} = 0 &\Longleftrightarrow \sum_{i=1}^{n} \abs{x_{i}} = 0\\
                &\Longleftrightarrow \abs{x_{i}} = 0 \Longleftrightarrow x_{i} = 0 \qquad \text{for } i = 1, \cdots, n,\\
                &\Longleftrightarrow \mathbf{x} = \mathbf{0}.
            \end{align*}
            \item For any $\mathbf{x} \in \mathbb{R}^{n}$ and $\alpha \in \mathbb{R}$, 
            \begin{equation*}
                \pnorm[1]{\alpha \mathbf{x}} = \sum_{i=1}^{n} \abs{\alpha x_{i}} = \abs{\alpha} \sum_{i=1}^{n} \abs{x_{i}} = \abs{\alpha} \pnorm[1]{\mathbf{x}}.
            \end{equation*}
            \item Using the Triangle Inequality, for $\mathbf{x}, \mathbf{y} \in \mathbb{R}^{n}$,
            \begin{equation*}
                \pnorm[1]{\mathbf{x} + \mathbf{y}} = \sum_{i=1}^{n} \abs{x_{i} + y_{i}} \leq \sum_{i=1}^{n} (\abs{x_{i}} + \abs{y_{i}}) = \pnorm[1]{\mathbf{x}} + \pnorm[1]{\mathbf{y}}.
            \end{equation*}
        \end{enumerate}
        Therefore, by definition, the 1-norm is a norm on $\mathbb{R}^{n}$.
    \end{proofing}
    \newpage
    
    \begin{eg}
        For $\mathbf{x} \in \mathbb{R}^{n}$, the Euclidean norm ($2$-norm or $L_{2}$-norm) defined by:
        \begin{equation*}
            \pnorm[2]{\mathbf{x}} = \sqrt{\sum_{i=1}^{n}x_{i}^{2}}
        \end{equation*}
        is a norm on $\mathbb{R}^{n}$. The induced distance $\pnorm[2]{\mathbf{x} - \mathbf{y}}$ for $\mathbf{x}, \mathbf{y} \in V$ is called the \textbf{Euclidean distance}.
    \end{eg}
    \begin{proofing}
        \begin{enumerate}
            \item For $\mathbf{x} \in \mathbb{R}^{n}$,
            \begin{equation*}
                \pnorm[2]{\mathbf{x}} = \sqrt{\sum_{i=1}^{n}x_{i}^{2}} \geq 0.
            \end{equation*}
            Moreover,
            \begin{align*}
                \pnorm[2]{\mathbf{x}} = 0 &\Longleftrightarrow \sum_{i=1}^{n}x_{i}^{2} = 0\\
                &\Longleftrightarrow x_{i}^{2} = 0 \Longleftrightarrow x_{i} = 0 \qquad \text{for } i = 1, \cdots, n,\\
                &\Longleftrightarrow \mathbf{x} = \mathbf{0}.
            \end{align*}
            \item For any $\alpha \in \mathbb{R}$, $\mathbf{x} \in \mathbb{R}^{n}$,
            \begin{equation*}
                \pnorm[2]{\alpha \mathbf{x}} = \sqrt{\sum_{i=1}^{n}(\alpha x_{i})^{2}} = \sqrt{\alpha^{2}} \sqrt{\sum_{i=1}^{n}x_{i}^{2}} = \abs{\alpha} \pnorm[2]{\mathbf{x}}.
            \end{equation*}
            \item This is a bit more complicated, but it is similar to the proof of the Cauchy-Schwarz inequality. 
            
            For $\mathbf{x} \in \mathbb{R}^{n}$, if $\mathbf{y} = \mathbf{0}$, then:
            \begin{equation*}
            	\pnorm[2]{\mathbf{x} + \mathbf{y}} = \pnorm[2]{\mathbf{x}} = \pnorm[2]{\mathbf{x}} + \pnorm[2]{\mathbf{y}}
            \end{equation*}
            If $\mathbf{y} \neq 0$, then for any $t \in \mathbb{R}$,
            \begin{equation*}
                \pnorm[2]{\mathbf{x} + t\mathbf{y}}^{2} = \sum_{i=1}^{n}(x_{i} + ty_{i})^{2} = \left(\sum_{i=1}^{n}x_{i}^{2}\right) + t\left(2\sum_{i=1}^{n}x_{i}y_{i}\right) + t^{2}\left(\sum_{i=1}^{n}y_{i}^{2}\right).
            \end{equation*}
            By (1), $\pnorm[2]{\mathbf{x} + t\mathbf{y}}^{2} \geq 0$. Therefore, since $\sum_{i=1}^{n} y_{i}^{2}>0$, $\pnorm[2]{\mathbf{x} + t\mathbf{y}}^{2}$ is a quadratic function with at most one real root. Using the discriminant, we have:
            \begin{align*}
                \Delta &\leq 0,\\
                \left(2\sum_{i=1}^{n}x_{i}y_{i}\right)^{2} - 4\left(\sum_{i=1}^{n}x_{i}^{2}\right)\left(\sum_{i=1}^{n}y_{i}^{2}\right) &\leq 0,\\
                \left(\sum_{i=1}^{n}x_{i}y_{i}\right)^{2} &\leq \left(\sum_{i=1}^{n}x_{i}^{2}\right)\left(\sum_{i=1}^{n}y_{i}^{2}\right),\\
                \tag{Cauchy-Schwarz Inequality for $\mathbb{R}^{n}$}
                \sum_{i=1}^{n}x_{i}y_{i} &\leq \sqrt{\sum_{i=1}^{n}x_{i}^{2}\sum_{i=1}^{n}y_{i}^{2}}.
            \end{align*}
            Consequently, 
            \begin{align*}
                \pnorm[2]{\mathbf{x} + \mathbf{y}}^{2} &= \sum_{i=1}^{n}(x_{i} + y_{i})^{2}\\
                &= \sum_{i=1}^{n}x_{i}^{2} + 2\sum_{i=1}^{n}x_{i}y_{i} + \sum_{i=1}^{n}y_{i}^{2}\\
                &\leq \sum_{i=1}^{n}x_{i}^{2} + 2\sqrt{\sum_{i=1}^{n}x_{i}^{2}\sum_{i=1}^{n}y_{i}^{2}} + \sum_{i=1}^{n}y_{i}^{2} = \pnorm[2]{\mathbf{x}}^{2} + 2\pnorm[2]{\mathbf{x}}\pnorm[2]{\mathbf{y}} + \pnorm[2]{\mathbf{y}}^{2},\\
                \pnorm[2]{\mathbf{x} + \mathbf{y}} &\leq \pnorm[2]{\mathbf{x}} + \pnorm[2]{\mathbf{y}}.
            \end{align*}
        \end{enumerate}
        Therefore, by definition, the 2-norm is a norm on $\mathbb{R}^{n}$.
    \end{proofing}
    \newpage

    \begin{eg}
        For $\mathbf{x} \in \mathbb{R}^{n}$, the $p$-norm or $L_{p}$-norm with $p \geq 1$ defined by:
        \begin{equation*}
            \pnorm[p]{\mathbf{x}} = \left(\sum_{i=1}^{n}\abs{x_{i}}^{p}\right)^{\frac{1}{p}}
        \end{equation*}
        is a norm on $\mathbb{R}^{n}$.
    \end{eg}
    \begin{eg}
        For $\mathbf{x} \in \mathbb{R}^{n}$, the maximum norm ($L_{\infty}$-norm) defined by:
        \begin{equation*}
            \pnorm[\infty]{\mathbf{x}} = \lim_{p \to \infty}\pnorm[p]{\mathbf{x}} = \max_{1 \leq i \leq n}\abs{x_{i}}
        \end{equation*}
        is a norm on $\mathbb{R}^{n}$.
    \end{eg}
    \begin{figure}[h]
        \centering
        \begin{subfigure}[h]{0.3\textwidth}
            \centering
            \begin{tikzpicture}
                \node[anchor=north east] at (0,0) {O};
                \node[anchor=north west] at (2,0) {A};
                \node[anchor=south west] at (2,1) {B};
                \draw[help lines] 		(-1,-1) grid (3,3);
                \draw[thick, ->]		(-1,0) -- (3,0);
                \draw[thick, ->]		(0,-1) -- (0,3);
                \draw[thick, ->, blue]	(0,0) -- (2,1);
                \draw[thick, red]		(0,0) -- (2,0) -- (2,1);
            \end{tikzpicture}
        \end{subfigure}
        \begin{subfigure}[h]{0.4\textwidth}
            \centering
            \begin{itemize}
                \color{red}
                \item[] Red: Manhattan distance from O to B
                \begin{equation*}
                    \abs{\text{OA}} + \abs{\text{AB}}
                \end{equation*}
                \color{blue}
                \item[] Blue: Euclidean distance from O to B
                \begin{equation*}
                    \abs{\text{OB}}
                \end{equation*}
            \end{itemize}
        \end{subfigure}
        \caption{Difference between Manhattan distance and Euclidean distance}
    \end{figure}
    \begin{defn}
        The \textbf{open unit ball} of a norm $\norm{\cdot}$ is defined by:
        \begin{equation*}
            B = \{\mathbf{x} \in \mathbb{R}^{n} : \norm{\mathbf{x}} < 1\}.
        \end{equation*}
    \end{defn}
    \begin{eg}
        For $p$-norm, the open unit ball of $\pnorm[p]{\cdot}$ is defined by:
        \begin{equation*}
            B_{p} = \left\{\mathbf{x} \in \mathbb{R}^{n} : \pnorm[p]{\mathbf{x}} = \left(\sum_{i=1}^{n}\abs{x_{i}}^{p}\right)^{\frac{1}{p}} < 1\right\}.
        \end{equation*}
    \end{eg}
    \begin{figure}[h]
        \centering
        \begin{subfigure}[h]{0.3\textwidth}
            \centering
            \begin{tikzpicture}
                \draw[help lines] 	(-2,-2) grid (2,2);
                \draw[thick, ->]	(-2,0) -- (2,0);
                \draw[thick, ->]	(0,-2) -- (0,2);
                \draw[thick, red]	(0,0) circle (1cm);
                \draw[thick, blue]	(1,0) -- (0,1) -- (-1,0) -- (0,-1) -- (1,0);	
                \draw[thick, green] (1,1) -- (-1,1) -- (-1,-1) -- (1,-1) -- (1,1);
            \end{tikzpicture}
        \end{subfigure}
        \begin{subfigure}[h]{0.5\textwidth}
            \centering
            \begin{itemize}
                \color{red}
                \item[] $B_{2} = \{\mathbf{x} \in \mathbb{R}^{n} : \pnorm[2]{\mathbf{x}} < 1\}$
                \color{blue}
                \item[] $B_{1} = \{\mathbf{x} \in \mathbb{R}^{n} : \pnorm[1]{\mathbf{x}} < 1\}$
                \color{green}
                \item[] $B_{\infty} = \{\mathbf{x} \in \mathbb{R}^{n} : \pnorm[\infty]{\mathbf{x}} < 1\}$
            \end{itemize}
        \end{subfigure}
        \caption{Unit balls of $p$-norm for different $p$}
    \end{figure}
    \begin{thm}
        If $0 < q \leq p < \infty$, then for any $\mathbf{x} \in \mathbb{R}^{n}$,
        \begin{equation*}
            \pnorm[p]{\mathbf{x}} \leq \pnorm[q]{\mathbf{x}}.
        \end{equation*}
    \end{thm}
    \begin{rem}
        There are other norms on $\mathbb{R}^{n}$, not just $p$-norms.
    \end{rem}
    \newpage
    
    For a set of matrices $\mathbb{R}^{m \times n}$, we can view them as $\mathbb{R}^{mn}$.
    \begin{eg}
        For $\mathbf{A} \in \mathbb{R}^{m \times n}$, the \textbf{vector $p$-norm} with $p \geq 1$ defined by:
        \begin{equation*}
            \pnorm[p,\text{vec}]{\mathbf{A}} = \left(\sum_{i=1}^{m}\sum_{j=1}^{n}\abs{a_{ij}}^{p}\right)^{\frac{1}{p}}
        \end{equation*}
        is a norm on $\mathbb{R}^{m \times n}$.
    \end{eg}
    \begin{eg}
        For $\mathbf{A} \in \mathbb{R}^{m \times n}$, when $p = 2$, the \textbf{Frobenius norm} (vector $2$-norm) defined by:
        \begin{equation*}
            \pnorm[F]{\mathbf{A}} = \pnorm[2,\text{vec}]{\mathbf{A}} = \sqrt{\sum_{i=1}^{m}\sum_{j=1}^{n}a_{ij}^{2}}
        \end{equation*}
        is a norm on $\mathbb{R}^{m \times n}$.
    \end{eg}
    We can also view $\mathbb{R}^{m \times n}$ as a linear transformation $\mathbb{R}^{n} \to \mathbb{R}^{m}$. Given a vector $\mathbf{x} \in \mathbb{R}^{n}$, one can consider the function:
    \begin{equation*}
        f(\mathbf{x}) = \mathbf{Ax} \in \mathbb{R}^{m}
    \end{equation*}
    as a linear transformation from $\mathbb{R}^{n}$ to $\mathbb{R}^{m}$.
    \begin{eg}
        For $\mathbf{A} \in \mathbb{R}^{m \times n}$, the \textbf{matrix $p$-norm} with $p\geq 1$ defined by:
        \begin{equation*}
            \pnorm[p]{\mathbf{A}} = \max_{\substack{\mathbf{x} \neq \mathbf{0}\\\mathbf{x} \in \mathbb{R}^{n}}}\frac{\pnorm[p]{\mathbf{Ax}}}{\pnorm[p]{\mathbf{x}}} = \max_{\substack{\mathbf{x} \in \mathbb{R}^{n}\\\pnorm[p]{\mathbf{x}} = 1}}\pnorm[p]{\mathbf{Ax}}
        \end{equation*}
        is a norm on $\mathbb{R}^{m \times n}$.
    \end{eg}
    \begin{eg}
        For $\mathbf{A} \in \mathbb{R}^{m \times n}$, when $p = 1$, the matrix $1$-norm defined by:
        \begin{equation*}
            \pnorm[1]{\mathbf{A}} = \max_{\substack{\mathbf{x} \in \mathbb{R}^{n}\\\pnorm[1]{\mathbf{x}} = 1}}\pnorm[1]{\mathbf{Ax}} = \max_{1 \leq j \leq n}\sum_{i=1}^{m}\abs{a_{ij}}
        \end{equation*}
        is a norm on $\mathbb{R}^{m \times n}$.
    \end{eg}
    \begin{eg}
        \label{Chapter 2 (Example) Matrix 2-norm}
        For $\mathbf{A} \in \mathbb{R}^{m \times n}$, when $p = 2$, the matrix $2$-norm can be described with the following properties:
        \begin{align*}
            \pnorm[2]{\mathbf{A}}^{2} &= \max_{\substack{\mathbf{x} \in \mathbb{R}^{n}\\\pnorm[2]{\mathbf{x}} = 1}}\pnorm[2]{\mathbf{Ax}}^{2} = \max_{\substack{\mathbf{x} \in \mathbb{R}^{n}\\\pnorm[2]{\mathbf{x}} = 1}}\mathbf{x}^{T}\mathbf{A}^{T}\mathbf{Ax} = \text{max eigenvalue of } \mathbf{A}^{T}\mathbf{A},\\
            \pnorm[2]{\mathbf{A}} &= \sqrt{\text{max eigenvalue of } \mathbf{A}^{T}\mathbf{A}} = \text{max singular value of } \mathbf{A}.
        \end{align*}
        Therefore, the matrix $2$-norm is also called the \textbf{operator norm} of $\mathbf{A}$.
    \end{eg}
    \begin{figure}[h]
        \centering
        \begin{subfigure}[h]{0.3\textwidth}
            \centering
            \begin{tikzpicture}
                \draw[thick, ->]	(-2,0) -- (2,0);
                \draw[thick, ->]	(0,-2) -- (0,2);
                \draw[thick, red]	(0,0) circle (1cm);
            \end{tikzpicture}
            
            $B_{2}$
        \end{subfigure}
        $\xrightarrow{\mathbf{A}}$
        \begin{subfigure}[h]{0.3\textwidth}
            \centering
            \begin{tikzpicture}
                \draw[thick, ->]	(-2,0) -- (2,0);
                \draw[thick, ->]	(0,-2) -- (0,2);
                \draw[thick, red, rotate=20]	(0,0) ellipse (1.5cm and 1cm);
                \draw[thick, blue, rotate=20]	(0,0) -- (1.5,0) node[above, midway] {$\pnorm[2]{\mathbf{A}}$};
            \end{tikzpicture}
            
            $\{\mathbf{Ax} : \mathbf{x} \in B_{2}\}$
        \end{subfigure}
        \caption{Meaning of matrix $2$-norm when $n = 2$ and $m = 2$}
    \end{figure}
    \newpage

    In addition, the matrix $p$-norm can be generalized with two different $p$-norms.
    \begin{eg}
        For $\mathbf{A} \in \mathbb{R}^{m \times n}$, the matrix norm induced by $p$-norm in $\mathbb{R}^{n}$ and $q$-norm in $\mathbb{R}^{m}$ defined by:
        \begin{equation*}
            \pnorm[p \to q]{\mathbf{A}} = \max_{\substack{\mathbf{x} \in \mathbb{R}^{n}\\\pnorm[p]{\mathbf{x}} = 1}}\pnorm[q]{\mathbf{Ax}}
        \end{equation*}
        is a norm on $\mathbb{R}^{m \times n}$. Moreover, the matrix $p$-norm is a special case of this norm.
    \end{eg}
    Matrix norms do not need to be defined using vector norms.
    \begin{eg}
        For $\mathbf{A} \in \mathbb{R}^{m \times n}$, the \textbf{nuclear norm} (trace norm or Ky Fan $n$-norm) defined by:
        \begin{equation*}
            \pnorm[*]{\mathbf{A}} = \Tr(\sqrt{\mathbf{A}^{T}\mathbf{A}})
        \end{equation*}
        is a norm on $\mathbb{R}^{m \times n}$.
    \end{eg}
    How about norms for continuous functions?
    \begin{eg}
        For $f \in \mathcal{C}[a, b]$, where the set is defined as:
        \begin{equation*}
            \mathcal{C}[a, b] = \{f : f \text{ is a continuous function on } [a, b]\},
        \end{equation*}
        the \textbf{maximum norm} (Chebyshev norm) defined by:
        \begin{equation*}
            \pnorm[\infty]{f} = \max_{t \in [a, b]}\abs{f(t)}
        \end{equation*}
        is a norm on $\mathcal{C}[a, b]$.
    \end{eg}
    \begin{eg}
        For $f \in \mathcal{C}[a, b]$, the \textbf{$p$-norm} with $p\geq 1$ defined by:
        \begin{equation*}
            \pnorm[p]{f} = \left(\int_{a}^{b}\abs{f(t)}^{p}\,dt\right)^{\frac{1}{p}}
        \end{equation*}
        is a norm on $\mathcal{C}[a, b]$.
    \end{eg}
    Is there a norm for the set of vectors with infinite dimensions?
    \begin{eg}
        For $\mathbf{x} \in \ell_{\infty}$, where the set is defined as:
        \begin{equation*}
            \ell_{\infty} = \left\{\begin{pmatrix}
                a_{1}\\
                \vdots\\
                a_{n}\\
                \vdots
            \end{pmatrix} : \text{There exists $c < \infty$ such that $\abs{a_{i}} \leq c$ for any $i$}\right\},
        \end{equation*}
        the \textbf{supremum norm} defined by:
        \begin{equation*}
            \pnorm[\infty]{\mathbf{x}} = \sup_{i}\abs{x_{i}}
        \end{equation*}
        is a norm on $\ell_{\infty}$.
    \end{eg}
    \begin{eg}
        For $\mathbf{x} \in \ell_{p}$ with $p\geq 1$, where the set is defined as:
        \begin{equation*}
            \ell_{p} = \{\mathbf{x} \in \ell_{\infty} : \pnorm[p]{\mathbf{x}} < \infty\} \subset \ell_{\infty},
        \end{equation*}
        the $p$-norm defined by:
        \begin{equation*}
            \pnorm[p]{\mathbf{x}} = \left(\sum_{i=1}^{\infty}\abs{x_{i}}^{p}\right)^{\frac{1}{p}}
        \end{equation*}
        is a norm on $\ell_{p}$. However, it is important to note that it is not a norm on $\ell_{\infty}$.
    \end{eg}
    \begin{rem}
        For the same vector space, we can define infinitely many norms on it. The simplest would be by adding or subtracting different norms.
    \end{rem}
    \newpage
    
\section{Limit and convergence on normed vector space}
    In this section, assume that $V$ is a vector space over $\mathbb{R}$ with norm $\norm{\cdot}$.

    In data analysis, many algorithms are iterative algorithms, which generate $n$ sequences of vectors:
    \begin{equation*}
        \{\mathbf{x}_{i}^{(k)}\} \subset V, \qquad i = 1, \cdots, n,
    \end{equation*}
    where $V$ is endowed with a norm $\norm{\cdot}$. As the iteration continues, it is important that the output stays within the hypothesis space. If the algorithm generates an output that is outside the expected domain, then it is not considered a good solution.
    \begin{defn}
        Let $\mathbf{x} \in V$. We say the sequence $\{\mathbf{x}^{(k)}\} \in V$ \textbf{converges} to $\mathbf{x}$, denoted by $\mathbf{x}^{(k)} \to \mathbf{x}$, if:
        \begin{equation*}
            \lim_{k \to \infty}\snorm{\mathbf{x}^{(k)} - \mathbf{x}} = 0.
        \end{equation*}
        More rigorously, the sequence $\{\mathbf{x}^{(k)}\}$ \textbf{converges} to $\mathbf{x}$ if, for any $\varepsilon > 0$, there exists $N$ such that for any $n \geq N$,
        \begin{equation*}
            \snorm{\mathbf{x}^{(n)} - \mathbf{x}} < \varepsilon.
        \end{equation*}
    \end{defn}
    \begin{eg}
        Consider $V = \mathbb{R}^{n}$ with $\pnorm[2]{\cdot}$. Let:
        \begin{align*}
            \mathbf{x}^{(k)} &= \begin{pmatrix}
                \frac{1}{k}\\
                \vdots\\
                \frac{n}{k}
            \end{pmatrix}, & \mathbf{x} &= \mathbf{0}.
        \end{align*}
        Then we have:
        \begin{align*}
            \spnorm[2]{\mathbf{x}^{(k)} - \mathbf{x}} &= \spnorm[2]{\mathbf{x}^{(k)}} = \sqrt{\sum_{i=1}^{n}\left(\frac{i}{k}\right)^{2}} = \frac{1}{k}\sqrt{\sum_{i=1}^{n}i^{2}},\\
            \lim_{k \to \infty}\spnorm[2]{\mathbf{x}^{(k)} - \mathbf{x}} &= \lim_{k \to \infty}\frac{1}{k}\sqrt{\sum_{i=1}^{n}i^{2}} = 0.
        \end{align*}
        Therefore, $\mathbf{x}^{(k)} \to \mathbf{x}$.
    \end{eg}
    \begin{eg}
        Consider $V = \mathcal{C}[0, 1]$ with $\pnorm[\infty]{\cdot}$. Let:
        \begin{equation*}
            f^{(k)}(t) = \frac{\sin(2\pi kt)}{k^{2}} \in \mathcal{C}[0, 1].
        \end{equation*}
        Let $0$ be the zero function. We can easily find that $0 \in \mathcal{C}[0, 1]$. Then we have:
        \begin{align*}
            \spnorm[\infty]{f^{(k)} - 0} &= \spnorm[\infty]{f^{(k)}} = \max_{t \in [0, 1]}\abs{\frac{\sin(2\pi kt)}{k^{2}}} = \frac{1}{k^{2}},\\
            \lim_{k \to \infty}\spnorm[\infty]{f^{(k)} - 0} &= \lim_{k \to \infty}\frac{1}{k^{2}} = 0.
        \end{align*}
        Therefore, $f^{(k)} \to 0$.
    \end{eg}
    \begin{figure}[h]
        \centering
        \begin{subfigure}[h]{0.2\textwidth}
            \centering
            \begin{tikzpicture}
                \draw[thick, ->]	(-0.5,0) -- (1.5,0);
                \draw[thick, red, domain=0:1]	plot (\x, {sin(deg(2*pi*\x))});
            \end{tikzpicture}
            
            $k = 1$
        \end{subfigure}
        \begin{subfigure}[h]{0.2\textwidth}
            \centering
            \begin{tikzpicture}
                \draw[thick, ->]	(-0.5,0) -- (1.5,0);
                \draw[thick, red, domain=0:1]	plot (\x, {sin(deg(2*pi*2*\x))/4});
            \end{tikzpicture}
            
            $k = 2$
        \end{subfigure}
        \begin{subfigure}[h]{0.2\textwidth}
            \centering
            \begin{tikzpicture}
                \draw[thick, ->]	(-0.5,0) -- (1.5,0);
                \draw[thick, red, domain=0:1]	plot (\x, {sin(deg(2*pi*3*\x))/9});
            \end{tikzpicture}
            
            $k = 3$
        \end{subfigure}
        $\cdots$
        \begin{subfigure}[h]{0.2\textwidth}
            \centering
            \begin{tikzpicture}
                \draw[thick, ->]	(-0.5,0) -- (1.5,0);
                \draw[thick, red, domain=0:1]	plot (\x, {0});
            \end{tikzpicture}
            
            $k = \infty$
        \end{subfigure}
        \caption{As $k$ increases, the wave keeps shrinking in amplitude.}
    \end{figure}
    \newpage

    \begin{rem}
        Convergence depends on the norm used.
    \end{rem}
    \begin{eg}
        Consider $V = \ell_{p}$ for any $p$ with $\pnorm[p]{\cdot}$. Let:
        \begin{align*}
            \mathbf{x}^{(k)} &= \begin{pmatrix}
                \frac{1}{k}\\
                \vdots\\
                \frac{1}{k}\\
                0\\
                \vdots
            \end{pmatrix}~\begin{array}{@{} c @{}}
                \\
                k\text{-terms}\\[1.5ex]
                \\
                \\
                \mathstrut
            \end{array} = \sum_{i=1}^{k}\frac{1}{k}\mathbf{e}_{i} \in \ell_{p}, & \mathbf{x} &= \begin{pmatrix}
                0\\
                0\\
                \vdots\\
                0\\
                \vdots
            \end{pmatrix} = \mathbf{0} \in \ell_{p}.
        \end{align*}
        \begin{itemize}
            \item[] When $p = 2$,
            \begin{align*}
                \spnorm[2]{\mathbf{x}^{(k)} - \mathbf{x}} &= \spnorm[2]{\mathbf{x}^{(k)}} = \sqrt{\sum_{i=1}^{k}\frac{1}{k^{2}}} = \frac{1}{\sqrt{k}},\\
                \lim_{k \to \infty}\spnorm[2]{\mathbf{x}^{(k)} - \mathbf{x}} &= \lim_{k \to \infty}\frac{1}{\sqrt{k}} = 0.
            \end{align*}
            Therefore, $\mathbf{x}^{(k)} \to \mathbf{x}$ with the norm $\spnorm[2]{\cdot}$.
            \item[] When $p = \infty$,
            \begin{align*}
                \spnorm[\infty]{\mathbf{x}^{(k)} - \mathbf{x}} &= \spnorm[\infty]{\mathbf{x}^{(k)}} = \frac{1}{k},\\
                \lim_{k \to \infty}\spnorm[\infty]{\mathbf{x}^{(k)} - \mathbf{x}} &= \lim_{k \to \infty}\frac{1}{k} = 0.
            \end{align*}
            Therefore, $\mathbf{x}^{(k)} \to \mathbf{x}$ with the norm $\spnorm[\infty]{\cdot}$.
            \item[] When $p = 1$,
            \begin{align*}
                \spnorm[1]{\mathbf{x}^{(k)} - \mathbf{x}} &= \spnorm[1]{\mathbf{x}^{(k)}} = \sum_{i=1}^{k}\frac{1}{k} = 1,\\
                \lim_{k \to \infty}\spnorm[1]{\mathbf{x}^{(k)} - \mathbf{x}} &= \lim_{k \to \infty}1 = 1 \neq 0.
            \end{align*}
            Therefore, $\mathbf{x}^{(k)} \not\to \mathbf{x}$ with the norm $\pnorm[1]{\cdot}$.
        \end{itemize}
    \end{eg}
    \begin{rem}
        The limit may not be in the same vector space. If this happens, the normed vector space is called \textbf{incomplete}.
    \end{rem}
    \begin{eg}
        Consider $V = \ell_{1}$ with $\pnorm[\infty]{\cdot}$. Let:
        \begin{align*}
            \mathbf{x}^{(k)} &= \begin{pmatrix}
                1\\
                \frac{1}{2}\\
                \vdots\\
                \frac{1}{k}\\
                0\\
                \vdots
            \end{pmatrix}=\sum_{i=1}^{k}\frac{1}{i}\mathbf{e}_{i} \in \ell_{1}, & \mathbf{x} &= \begin{pmatrix}
                1\\
                \frac{1}{2}\\
                \vdots\\
                \frac{1}{k}\\
                \frac{1}{k+1}\\
                \vdots
            \end{pmatrix}=\sum_{i=1}^{\infty}\frac{1}{i}\mathbf{e}_{i}.
        \end{align*}
        Then we have:
        \begin{align*}
            \lim_{k \to \infty}\spnorm[\infty]{\mathbf{x}^{(k)} - \mathbf{x}} = \lim_{k \to \infty}\pnorm[\infty]{\begin{pmatrix}
                    0\\
                    \vdots\\
                    0\\
                    \frac{1}{k+1}\\
                    \vdots
            \end{pmatrix}} = \lim_{k \to \infty}\frac{1}{k+1} = 0.
        \end{align*}
        However, $\sum_{i=1}^{\infty}\frac{1}{i} = \infty$, and thus $\mathbf{x} \not\in \ell_{1}$. Therefore, we cannot say that $\mathbf{x}^{(k)} \to \mathbf{x}$ with the norm $\pnorm[\infty]{\cdot}$.
    \end{eg}
    \newpage
    
    With the definition of convergence, we can define the completeness of a vector space.
    \begin{defn}
        The sequence $\{\mathbf{x}^{(k)}\} \subset V$ is a Cauchy sequence if, for any $\varepsilon > 0$, there exists $N$ such that for any $n, m \geq N$, 
        \begin{equation*}
            \snorm{\mathbf{x}^{(n)} - \mathbf{x}^{(m)}} < \varepsilon.
        \end{equation*}
    \end{defn}
    \begin{thm}
        If $\mathbf{x}^{(k)} \to \mathbf{x}$ in $(V, \norm{\cdot})$, then $\{\mathbf{x}^{(k)}\}$ is a Cauchy sequence.
    \end{thm}
    \begin{proofing}
        If $\mathbf{x}^{(k)} \to \mathbf{x}$, then for all $\varepsilon > 0$, there exists $N$ such that for all $n \geq N$,
        \begin{equation*}
            \snorm{\mathbf{x}^{(n)} - \mathbf{x}} < \frac{\varepsilon}{2}.
        \end{equation*}
        Therefore, for all $n, m \geq N$,
        \begin{equation*}
            \snorm{\mathbf{x}^{(n)} - \mathbf{x}^{(m)}} \leq \snorm{\mathbf{x}^{(n)} - \mathbf{x}} + \snorm{\mathbf{x}^{(m)} - \mathbf{x}} < \varepsilon.
        \end{equation*}
    \end{proofing}
    \begin{rem}
        The converse is not necessarily true.
    \end{rem}
    \begin{defn}
        A normed vector space $(V, \norm{\cdot})$ is \textbf{complete} if the limit of all Cauchy sequences in $V$ is in $V$.
    \end{defn}
    \begin{defn}
        A complete normed vector space is called a \textbf{Banach space}.
    \end{defn}
    \begin{eg}
        $\mathbb{R}^{n}$, $\mathbb{R}^{m \times n}$, or $\mathbb{R}^{m \times n \times \ell}$ with any norm is a Banach space.
    \end{eg}
    \begin{eg}
        $\mathcal{C}[a, b]$ with $\pnorm[\infty]{\cdot}$ is a Banach space.
    \end{eg}
    \begin{eg}
        For $p \geq 1$, including $p = \infty$, $(\ell_{p}, \pnorm[p]{\cdot})$ is a Banach space.
    \end{eg}
    \begin{rem}
    	We can always include all the limits of the Cauchy sequences to convert an incomplete normed vector space into a complete one.
    \end{rem}
    \begin{eg}
        $(\ell_{1}, \pnorm[\infty]{\cdot})$ is an incomplete normed vector space. Its completion is $\ell_{\infty}$.
    \end{eg}
    \begin{eg}
        For $p \geq 1$, $(\mathcal{C}[a, b], \pnorm[p]{\cdot})$ is an incomplete normed vector space. Its completion is $L^{p}[a, b]$.
    \end{eg}
    In practical cases, how do we check the convergence of a given sequence?
    \begin{eg}
        From an iterative algorithm, we generate a sequence of vectors $\{\mathbf{x}^{(k)}\}$. Our goal is to check if this sequence converges. Pick a threshold $\varepsilon > 0$. We can check computationally that for large $n, m$,
        \begin{equation*}
            \snorm{\mathbf{x}^{(n)} - \mathbf{x}^{(m)}} < \varepsilon.
        \end{equation*}
        Practically, to reduce computational cost, we usually only check for large $n$,
        \begin{equation*}
            \snorm{\mathbf{x}^{(k)} - \mathbf{x}^{(k-1)}} < \varepsilon.
        \end{equation*}
    \end{eg}
    \newpage

\section{Finite Dimensional Vector Spaces}
    In most cases, we deal with finite-dimensional vector spaces.
    \begin{rem}
        Every finite-dimensional vector space with any norm is complete. That is, any finite-dimensional vector space is Banach.
    \end{rem}
    \begin{rem}
        For a finite-dimensional vector space $V$, all norms are equivalent. For any two norms $\norm{\cdot}_{A}$ and $\norm{\cdot}_{B}$, there exist $c_{1}, c_{2} > 0$ such that:
        \begin{equation*}
            c_{1}\norm{\mathbf{x}}_{A} \leq \norm{\mathbf{x}}_{B} \leq c_{2}\norm{\mathbf{x}}_{A}, \qquad \text{for all } \mathbf{x} \in V.
        \end{equation*}
    \end{rem}
    \begin{thm}
        The limit of the same finite-dimensional sequence under any norm is the same. That means, given two finite-dimensional normed vector spaces $(V, \pnorm[A]{\cdot})$ and $(V, \pnorm[B]{\cdot})$, for any sequence $\{\mathbf{x}^{(k)}\}$ and $\mathbf{x} \in V$,
        \begin{equation*}
            \mathbf{x}^{(k)} \to \mathbf{x} \ \text{in} \ \pnorm[A]{\cdot} \Longleftrightarrow \mathbf{x}^{(k)} \to \mathbf{x} \ \text{in} \ \pnorm[B]{\cdot}.
        \end{equation*}
    \end{thm}
    \begin{proofing}
        Since $\mathbf{x}^{(k)} \to \mathbf{x}$ in $\pnorm[A]{\cdot}$,
        \begin{equation*}
            \lim_{k \to \infty}\spnorm[A]{\mathbf{x}^{(k)} - \mathbf{x}} = 0.
        \end{equation*}
        Therefore, there exist $c_{1}, c_{2} > 0$ such that:
        \begin{equation*}
            c_{1}\spnorm[A]{\mathbf{x}^{(k)} - \mathbf{x}} \leq \spnorm[B]{\mathbf{x}^{(k)} - \mathbf{x}} \leq c_{2}\spnorm[A]{\mathbf{x}^{(k)} - \mathbf{x}}.
        \end{equation*}
        Taking $k \to \infty$, we have:
        \begin{equation*}
            0 \leq c_{1}\lim_{k \to \infty}\spnorm[A]{\mathbf{x}^{(k)} - \mathbf{x}} \leq \lim_{k \to \infty}\spnorm[B]{\mathbf{x}^{(k)} - \mathbf{x}} \leq c_{2}\lim_{k \to \infty}\spnorm[A]{\mathbf{x}^{(k)} - \mathbf{x}} = 0.
        \end{equation*}
        By the Squeeze Theorem, we find that:
        \begin{equation*}
            \lim_{k \to \infty}\spnorm[B]{\mathbf{x}^{(k)} - \mathbf{x}} = 0.
        \end{equation*}
        To conclude, $\mathbf{x}^{(k)} \to \mathbf{x}$ in $\spnorm[B]{\cdot}$. The proof for the converse is similar.
    \end{proofing}
    \begin{eg}
        Consider $V = \mathbb{R}^{n}$ with $\pnorm[1]{\cdot}$, $\pnorm[2]{\cdot}$, and $\pnorm[\infty]{\cdot}$.
        \begin{enumerate}
            \item $\pnorm[1]{\cdot}$ and $\pnorm[2]{\cdot}$ are equivalent because for $\mathbf{x} \in \mathbb{R}^{n}$,
            \begin{equation*}
                \pnorm[2]{\mathbf{x}} \leq \pnorm[1]{\mathbf{x}} \leq \sqrt{n}\pnorm[2]{\mathbf{x}}.
            \end{equation*}
            \item $\pnorm[2]{\cdot}$ and $\pnorm[\infty]{\cdot}$ are equivalent because for $\mathbf{x} \in \mathbb{R}^{n}$,
            \begin{equation*}
                \pnorm[\infty]{\mathbf{x}} \leq \pnorm[2]{\mathbf{x}} \leq \sqrt{n}\pnorm[\infty]{\mathbf{x}}.
            \end{equation*}
            \item $\pnorm[1]{\cdot}$ and $\pnorm[\infty]{\cdot}$ are equivalent because for $\mathbf{x} \in \mathbb{R}^{n}$,
            \begin{equation*}
                \pnorm[\infty]{\mathbf{x}} \leq \pnorm[1]{\mathbf{x}} \leq n\pnorm[\infty]{\mathbf{x}}.
            \end{equation*}
        \end{enumerate}
    \end{eg}
    \newpage
    
    \begin{rem}
        Based on the theorem, the convergence speed depends on the norms used.
    \end{rem}
    \begin{eg}
        Consider $V = \mathbb{R}^{2}$. Let:
        \begin{equation*}
            \mathbf{x}^{(k)} = \frac{1}{k}\begin{pmatrix}
                \cos\left(\frac{(2k-1)\pi}{4}\right)\\
                \sin\left(\frac{(2k-1)\pi}{4}\right)
            \end{pmatrix} \in \mathbb{R}^{2}.
        \end{equation*}
        We can easily find that $\mathbf{x}^{(k)} \to \mathbf{0}$ with the norms $\pnorm[1]{\cdot}$ and $\pnorm[2]{\cdot}$. However,
        \begin{align*}
            \spnorm[1]{\mathbf{x}^{(k)} - \mathbf{0}} &= \frac{\sqrt{2}}{k}, & \spnorm[2]{\mathbf{x}^{(k)} - \mathbf{0}} &= \frac{1}{k}.
        \end{align*}
        To achieve $\varepsilon$-precision:
        \begin{align*}
            \spnorm[1]{\mathbf{x}^{(k)} - \mathbf{0}} < \varepsilon &\Longleftrightarrow \frac{\sqrt{2}}{k_{1}} < \varepsilon \Longrightarrow k_{1} > \frac{\sqrt{2}}{\varepsilon},\\
            \spnorm[2]{\mathbf{x}^{(k)} - \mathbf{0}} < \varepsilon &\Longleftrightarrow \frac{1}{k_{2}} < \varepsilon \Longrightarrow k_{2} > \frac{1}{\varepsilon}.
        \end{align*}
        The norm $\pnorm[1]{\cdot}$ takes about $\sqrt{2}$ times as many iterations as the norm $\pnorm[2]{\cdot}$ to check convergence using $\varepsilon$ as the threshold.
    \end{eg}
    \begin{rem}
        In the case of infinite-dimensional vector spaces, not all norms are equivalent.
    \end{rem}
    \begin{eg}
        Consider $V = \ell_{1}$. Let:
        \begin{equation*}
            \mathbf{x}^{(k)} = \begin{pmatrix}
                1\\
                \vdots\\
                1\\
                0\\
                \vdots
            \end{pmatrix}~\begin{array}{@{} c @{}}
	            \\
	            k\text{-terms}\\[1.5ex]
	            \\
	            \\
	            \mathstrut
            \end{array}=\sum_{i=1}^{k}\mathbf{e}_{i} \in \ell_{1}.
        \end{equation*}
        Consider the norms $\pnorm[\infty]{\cdot}$ and $\pnorm[1]{\cdot}$. We find that:
        \begin{align*}
            \spnorm[\infty]{\mathbf{x}^{(k)}} &= 1, & \spnorm[1]{\mathbf{x}^{(k)}} &= k, & \lim_{k \to \infty}\frac{\spnorm[1]{\mathbf{x}^{(k)}}}{\spnorm[\infty]{\mathbf{x}^{(k)}}} &= \lim_{k \to \infty}k = \infty.
        \end{align*}
        Therefore, the two norms are not equivalent.
    \end{eg}
    
    Read Appendix \ref{Case Study A: Clustering, K-means, K-medians} (Clustering, K-means, K-medians) to see the case study for Chapter \ref{Chapter 2: Vector spaces, metrics, limits, and convergence}.
    
\chapter{Inner products, Hilbert Spaces}
	\label{Chapter 3: Inner products, Hilbert Spaces}
\section{Inner product}
    How do we describe whether two vectors are correlated? We cannot use norms to describe this because they are scaling-sensitive. As such, we define the inner product to quantify the relationship between two vectors.
    \begin{defn}
        Let $V$ be a vector space over $\mathbb{R}$. An \textbf{inner product} over $\mathbb{R}$ is a binary operator $\inprod{\cdot}{\cdot} : (V, V) \to \mathbb{R}$ such that for $\mathbf{x}, \mathbf{y}, \mathbf{z} \in V$:
        \begin{enumerate}
            \item $\inprod{\mathbf{x}}{\mathbf{x}} \geq 0$ and $\inprod{\mathbf{x}}{\mathbf{x}} = 0 \Longleftrightarrow \mathbf{x} = \mathbf{0}$.
            \item $\inprod{\alpha\mathbf{x} + \beta\mathbf{y}}{\mathbf{z}} = \alpha\inprod{\mathbf{x}}{\mathbf{z}} + \beta\inprod{\mathbf{y}}{\mathbf{z}}$ for all $\alpha, \beta \in \mathbb{R}$.
            \item $\inprod{\mathbf{x}}{\mathbf{y}} = \inprod{\mathbf{y}}{\mathbf{x}}$.
        \end{enumerate}
    \end{defn}
    \begin{rem}
        Property (2) and (3) are equivalent to the following: for $\mathbf{x}, \mathbf{y}, \mathbf{z} \in V$ and $\alpha, \beta \in \mathbb{R}$,
        \begin{equation*}
            \inprod{\mathbf{x}}{\alpha\mathbf{y} + \beta\mathbf{z}} = \alpha\inprod{\mathbf{x}}{\mathbf{y}} + \beta\inprod{\mathbf{x}}{\mathbf{z}}.
        \end{equation*}
    \end{rem}
    \begin{rem}
        Inner products can also be defined over $\mathbb{C}$, but property (3) would change to:
        \begin{equation*}
            \inprod{\mathbf{x}}{\mathbf{y}} = \overline{\inprod{\mathbf{y}}{\mathbf{x}}}.
        \end{equation*}
    \end{rem}
    \begin{eg}
        For $\mathbf{x}, \mathbf{y} \in \mathbb{R}^{n}$, the \textbf{dot product} for $\mathbb{R}^{n}$ defined by:
        \begin{equation*}
            \inprod{\mathbf{x}}{\mathbf{y}} = \sum_{i=1}^{n}x_{i}y_{i} = \mathbf{x}^{T}\mathbf{y}
        \end{equation*}
        is the standard inner product in $\mathbb{R}^{n}$.
    \end{eg}
    \begin{eg}
        For a symmetric positive definite matrix $\mathbf{A} \in \mathbb{R}^{n \times n}$ and $\mathbf{x}, \mathbf{y} \in \mathbb{R}^{n}$, the "weighted" inner product defined by:
        \begin{equation*}
            \inprod{\mathbf{x}}{\mathbf{y}}_{\mathbf{A}} = \mathbf{x}^{T}\mathbf{A}\mathbf{y}
        \end{equation*}
        is an inner product on $\mathbb{R}^{n}$. The standard inner product is a special case of this inner product where $\mathbf{A} = \mathbf{I}$. However, if $\mathbf{A}$ is a symmetric positive semi-definite matrix, then the weighted inner product is not a true inner product. Instead, it is a \textbf{semi-inner-product}.
    \end{eg}
    \begin{proofing}
        \begin{enumerate}
            \item For $\mathbf{x} \in \mathbb{R}^{n}$:
            \begin{equation*}
                \inprod{\mathbf{x}}{\mathbf{x}}_{\mathbf{A}} = \mathbf{x}^{T}\mathbf{A}\mathbf{x} \geq 0.
            \end{equation*}
            Moreover:
            \begin{equation*}
                \inprod{\mathbf{x}}{\mathbf{x}}_{\mathbf{A}} = 0 \Longleftrightarrow \mathbf{x}^{T}\mathbf{A}\mathbf{x} = 0 \Longleftrightarrow \mathbf{x} = \mathbf{0}.
            \end{equation*}
            \item For any $\mathbf{x}, \mathbf{y}, \mathbf{z} \in \mathbb{R}^{n}$ and $\alpha, \beta \in \mathbb{R}$:
            \begin{equation*}
                \inprod{\alpha\mathbf{x} + \beta\mathbf{y}}{\mathbf{z}}_{\mathbf{A}} = (\alpha\mathbf{x} + \beta\mathbf{y})^{T}\mathbf{A}\mathbf{z} = \alpha\mathbf{x}^{T}\mathbf{A}\mathbf{z} + \beta\mathbf{y}^{T}\mathbf{A}\mathbf{z} = \alpha\inprod{\mathbf{x}}{\mathbf{z}}_{\mathbf{A}} + \beta\inprod{\mathbf{y}}{\mathbf{z}}_{\mathbf{A}}.
            \end{equation*}
            \item For any $\mathbf{x}, \mathbf{y} \in \mathbb{R}^{n}$:
            \begin{equation*}
                \inprod{\mathbf{x}}{\mathbf{y}}_{\mathbf{A}} = \mathbf{x}^{T}\mathbf{A}\mathbf{y} = (\mathbf{x}^{T}\mathbf{A}\mathbf{y})^{T} = \mathbf{y}^{T}\mathbf{A}^{T}\mathbf{x} = \inprod{\mathbf{y}}{\mathbf{x}}_{\mathbf{A}}.
            \end{equation*}
        \end{enumerate}
    \end{proofing}
    \newpage
    
    \begin{eg}
        For $\mathbf{A}, \mathbf{B} \in \mathbb{R}^{m \times n}$, the inner product defined by:
        \begin{equation*}
            \inprod{\mathbf{A}}{\mathbf{B}} = \sum_{i=1}^{m}\sum_{j=1}^{n}a_{ij}b_{ij} = \Tr(\mathbf{A}^{T}\mathbf{B}) = \Tr(\mathbf{B}^{T}\mathbf{A}) = \Tr(\mathbf{A}\mathbf{B}^{T}) = \Tr(\mathbf{B}\mathbf{A}^{T})
        \end{equation*}
        is the standard inner product in $\mathbb{R}^{m \times n}$.
    \end{eg}
    \begin{eg}
        For $\mathbf{x}, \mathbf{y} \in \ell_{2}$, the inner product defined by:
        \begin{equation*}
            \inprod{\mathbf{x}}{\mathbf{y}} = \sum_{i=1}^{\infty}x_{i}y_{i}
        \end{equation*}
        is the standard inner product in $\ell_{2}$.
    \end{eg}
    \begin{eg}
        For $f, g \in \mathcal{C}[a, b]$, the inner product defined by:
        \begin{equation*}
            \inprod{f}{g} = \int_{a}^{b}f(t)g(t)\,dt
        \end{equation*}
        is the standard inner product in $\mathcal{C}[a, b]$.
    \end{eg}

\section{Properties of inner products}
    Let $V$ be a vector space over $\mathbb{R}$ with an inner product $\inprod{\cdot}{\cdot}$.

    Using the definition of inner products, we can discuss some properties of inner products.
    \begin{thm}
        For any $\mathbf{x} \in V$, we have:
        \begin{equation*}
            \inprod{\mathbf{x}}{\mathbf{0}} = \inprod{\mathbf{0}}{\mathbf{x}} = 0.
        \end{equation*}
    \end{thm}
    \begin{proofing}
        \begin{equation*}
            \inprod{\mathbf{x}}{\mathbf{0}} = \inprod{\mathbf{x}}{\mathbf{x} - \mathbf{x}} = \inprod{\mathbf{x}}{\mathbf{x}} - \inprod{\mathbf{x}}{\mathbf{x}} = 0.
        \end{equation*}
    \end{proofing}
    In the previous chapter, we proved the Cauchy-Schwarz Inequality in $\mathbb{R}^{n}$ when verifying whether the $2$-norm in $\mathbb{R}^{n}$ is a norm. We can generalize this inequality to all inner products.
    \begin{thm}\named{Cauchy-Schwarz Inequality}
        For all $\mathbf{x}, \mathbf{y} \in V$:
        \begin{equation*}
            \abs{\inprod{\mathbf{x}}{\mathbf{y}}}^{2} \leq \inprod{\mathbf{x}}{\mathbf{x}}\inprod{\mathbf{y}}{\mathbf{y}}.
        \end{equation*}
        Equality holds if and only if $\mathbf{y} = \alpha\mathbf{x}$ for some $\alpha \in \mathbb{R}$.
    \end{thm}
    \begin{proofing}
        For $\mathbf{x} \in V$, if $\mathbf{y} = \mathbf{0}$, then:
        \begin{equation*}
            \abs{\inprod{\mathbf{x}}{\mathbf{y}}}^{2} = \abs{\inprod{\mathbf{x}}{\mathbf{0}}}^{2} = 0 = \inprod{\mathbf{x}}{\mathbf{x}}\inprod{\mathbf{y}}{\mathbf{y}}.
        \end{equation*}
        If $\mathbf{y} \neq \mathbf{0}$, then for any $\lambda \in \mathbb{R}$:
        \begin{equation*}
            0 \leq \inprod{\mathbf{x} + \lambda\mathbf{y}}{\mathbf{x} + \lambda\mathbf{y}} = \inprod{\mathbf{x}}{\mathbf{x}} + \lambda\inprod{\mathbf{x}}{\mathbf{y}} + \lambda\inprod{\mathbf{y}}{\mathbf{x}} + \lambda^{2}\inprod{\mathbf{y}}{\mathbf{y}} = \inprod{\mathbf{x}}{\mathbf{x}} + \lambda(2\inprod{\mathbf{x}}{\mathbf{y}}) + \lambda^{2}\inprod{\mathbf{y}}{\mathbf{y}}.
        \end{equation*}
        Since $\inprod{\mathbf{y}}{\mathbf{y}} > 0$, $\inprod{\mathbf{x} + \lambda\mathbf{y}}{\mathbf{x} + \lambda\mathbf{y}}$ is a quadratic function with at most one real root. Using the discriminant, we have:
        \begin{align*}
            \Delta = (2\inprod{\mathbf{x}}{\mathbf{y}})^{2} - 4\inprod{\mathbf{x}}{\mathbf{x}}\inprod{\mathbf{y}}{\mathbf{y}} &\leq 0,\\
            \abs{\inprod{\mathbf{x}}{\mathbf{y}}}^{2} &\leq \inprod{\mathbf{x}}{\mathbf{x}}\inprod{\mathbf{y}}{\mathbf{y}}.
        \end{align*}
        For the equality case, notice that $\Delta = 0$ if and only if:
        \begin{equation*}
            \inprod{\mathbf{x} + \lambda\mathbf{y}}{\mathbf{x} + \lambda\mathbf{y}} = \inprod{\mathbf{x}}{\mathbf{x}} + 2\lambda\inprod{\mathbf{x}}{\mathbf{y}} + \lambda^{2}\inprod{\mathbf{y}}{\mathbf{y}} = \left(\sqrt{\inprod{\mathbf{x}}{\mathbf{x}}} + \lambda\sqrt{\inprod{\mathbf{y}}{\mathbf{y}}}\right)^{2} - 2\lambda\sqrt{\inprod{\mathbf{x}}{\mathbf{x}}\inprod{\mathbf{y}}{\mathbf{y}}} + 2\lambda\inprod{\mathbf{x}}{\mathbf{y}} = 0.
        \end{equation*}
        Solving this equation gives $\lambda = -\sqrt{\frac{\inprod{\mathbf{x}}{\mathbf{x}}}{\inprod{\mathbf{y}}{\mathbf{y}}}}$, and we find $\mathbf{y} = \sqrt{\frac{\inprod{\mathbf{y}}{\mathbf{y}}}{\inprod{\mathbf{x}}{\mathbf{x}}}}\mathbf{x}$ by definition.

        Substituting $\mathbf{y} = \alpha\mathbf{x}$ for any $\alpha \in \mathbb{R}$ shows that $\Delta = 0$ if and only if $\mathbf{y} = \alpha\mathbf{x}$.
    \end{proofing}
    \newpage
    
    With the Cauchy-Schwarz Inequality, we can construct a norm using an inner product.
    \begin{thm}
        The \textbf{norm induced by the inner product} is a norm defined by, for any $\mathbf{x} \in V$:
        \begin{equation*}
            \norm{\mathbf{x}} = \sqrt{\inprod{\mathbf{x}}{\mathbf{x}}}.
        \end{equation*}
    \end{thm}
    \begin{proofing}
        \begin{enumerate}
            \item For any $\mathbf{x} \in V$:
            \begin{equation*}
                \norm{\mathbf{x}} = \sqrt{\inprod{\mathbf{x}}{\mathbf{x}}} \geq 0.
            \end{equation*}
            Moreover:
            \begin{equation*}
                \norm{\mathbf{x}} = 0 \Longleftrightarrow \inprod{\mathbf{x}}{\mathbf{x}} = 0 \Longleftrightarrow \mathbf{x} = \mathbf{0}.
            \end{equation*}
            \item For any $\mathbf{x} \in V$ and $\alpha \in \mathbb{R}$:
            \begin{equation*}
                \norm{\alpha\mathbf{x}} = \sqrt{\inprod{\alpha\mathbf{x}}{\alpha\mathbf{x}}} = \sqrt{\alpha^{2}}\sqrt{\inprod{\mathbf{x}}{\mathbf{x}}} = \abs{\alpha}\norm{\mathbf{x}}.
            \end{equation*}
            \item For any $\mathbf{x}, \mathbf{y} \in V$:
            \begin{align*}
                \norm{\mathbf{x} + \mathbf{y}}^{2} &= \inprod{\mathbf{x} + \mathbf{y}}{\mathbf{x} + \mathbf{y}}\\
                &= \inprod{\mathbf{x}}{\mathbf{x}} + 2\inprod{\mathbf{x}}{\mathbf{y}} + \inprod{\mathbf{y}}{\mathbf{y}}\\
                \tag{$c \leq \abs{c}$}
                &\leq \norm{\mathbf{x}}^{2} + \norm{\mathbf{y}}^{2} + 2\abs{\inprod{\mathbf{x}}{\mathbf{y}}}\\
                \tag{Cauchy-Schwarz Inequality}
                &\leq \norm{\mathbf{x}}^{2} + \norm{\mathbf{y}}^{2} + 2\norm{\mathbf{x}}\norm{\mathbf{y}} = (\norm{\mathbf{x}} + \norm{\mathbf{y}})^{2},\\
                \norm{\mathbf{x} + \mathbf{y}} &\leq \norm{\mathbf{x}} + \norm{\mathbf{y}}.
            \end{align*}
        \end{enumerate}
    \end{proofing}
    \begin{rem}
        Inner product spaces are a subset of normed vector spaces. Not all normed vector spaces can define an inner product.
    \end{rem}
    \begin{rem}
        The Cauchy-Schwarz Inequality can be rewritten as, for $\mathbf{x}, \mathbf{y} \in V$:
        \begin{equation*}
            \abs{\inprod{\mathbf{x}}{\mathbf{y}}} \leq \norm{\mathbf{x}}\norm{\mathbf{y}}.
        \end{equation*}
    \end{rem}
    \begin{eg}
        Consider $V = \mathbb{R}^{n}$ with the standard inner product:
        \begin{equation*}
            \inprod{\mathbf{x}}{\mathbf{y}} = \mathbf{x}^{T}\mathbf{y}, \qquad \text{for } \mathbf{x}, \mathbf{y} \in \mathbb{R}^{n}.
        \end{equation*}
        The induced norm is:
        \begin{equation*}
            \norm{\mathbf{x}} = \sqrt{\inprod{\mathbf{x}}{\mathbf{x}}} = \sqrt{\mathbf{x}^{T}\mathbf{x}} = \sqrt{\sum_{i=1}^{n}x_{i}^{2}} = \pnorm[2]{\mathbf{x}}, \qquad \text{for } \mathbf{x} \in \mathbb{R}^{n}.
        \end{equation*}
    \end{eg}
    \begin{rem}
        For $V = \mathbb{R}^{n}$, among all $p$-norms, only the $2$-norm can be induced by an inner product.
    \end{rem}
    \newpage
    
    \begin{eg}
    	For a symmetric positive definite matrix $\mathbf{A} \in \mathbb{R}^{n \times n}$, consider $V = \mathbb{R}^{n}$ with the weighted inner product:
    	\begin{equation*}
    		\inprod{\mathbf{x}}{\mathbf{y}}_{\mathbf{A}} = \mathbf{x}^{T}\mathbf{A}\mathbf{y}, \qquad \text{for } \mathbf{x}, \mathbf{y} \in \mathbb{R}^{n}.
    	\end{equation*}
    	The induced norm is:
    	\begin{equation*}
    		\pnorm[\mathbf{A}]{\mathbf{x}} = \sqrt{\mathbf{x}^{T}\mathbf{A}\mathbf{x}} = \sqrt{\sum_{i=1}^{n}\sum_{j=1}^{n}a_{ij}x_{i}x_{j}}.
    	\end{equation*}
    	If $\mathbf{A}$ is symmetric positive semi-definite, then the induced norm is not a true norm. Instead, it is a semi-norm (discussed in Appendix \ref{Case Study C: Metric Learning}).
    \end{eg}
    \begin{eg}
        Consider $V = \mathbb{R}^{m \times n}$ with the standard inner product:
        \begin{equation*}
            \inprod{\mathbf{A}}{\mathbf{B}} = \sum_{i=1}^{m}\sum_{j=1}^{n}a_{ij}b_{ij}, \qquad \text{for } \mathbf{A}, \mathbf{B} \in \mathbb{R}^{m \times n}.
        \end{equation*}
        The induced norm is:
        \begin{equation*}
            \norm{\mathbf{A}} = \sqrt{\inprod{\mathbf{A}}{\mathbf{A}}} = \sqrt{\sum_{i=1}^{m}\sum_{j=1}^{n}a_{ij}^{2}} = \pnorm[F]{\mathbf{A}} = \pnorm[2, \text{vec}]{\mathbf{A}}, \qquad \text{for } \mathbf{A} \in \mathbb{R}^{m \times n}.
        \end{equation*}
    \end{eg}
    \begin{eg}
        Consider $V = \ell_{2}$ with the standard inner product:
        \begin{equation*}
            \inprod{\mathbf{x}}{\mathbf{y}} = \sum_{i=1}^{\infty}x_{i}y_{i}, \qquad \text{for } \mathbf{x}, \mathbf{y} \in \ell_{2}.
        \end{equation*}
        The induced norm is:
        \begin{equation*}
            \norm{\mathbf{x}} = \sqrt{\sum_{i=1}^{\infty}x_{i}^{2}} = \pnorm[2]{\mathbf{x}}, \qquad \text{for } \mathbf{x} \in \ell_{2}.
        \end{equation*}
    \end{eg}
    \begin{eg}
        Consider $V = \mathcal{C}[a, b]$ with the standard inner product:
        \begin{equation*}
            \inprod{f}{g} = \int_{a}^{b}f(t)g(t)\,dt, \qquad \text{for } f, g \in \mathcal{C}[a, b].
        \end{equation*}
        The induced norm is:
        \begin{equation*}
            \norm{f} = \sqrt{\int_{a}^{b}(f(t))^{2}\,dt} = \pnorm[2]{f}, \qquad \text{for } f \in \mathcal{C}[a, b].
        \end{equation*}
    \end{eg}
    \newpage

    What conditions are required to define an inner product based on the normed vector space?
    \begin{thm}
        Let $(V,\norm{\cdot})$ be a normed vector space over $\mathbb{R}$. The norm $\norm{\cdot}$ is induced by an inner product if and only if the parallelogram law holds. This means that for all $\mathbf{x}, \mathbf{y} \in V$,
        \begin{equation*}
            \norm{\mathbf{x}+\mathbf{y}}^{2} + \norm{\mathbf{x}-\mathbf{y}}^{2} = 2\norm{\mathbf{x}}^{2} + 2\norm{\mathbf{y}}^{2}.
        \end{equation*}
    \end{thm}
    \begin{proofing}
        \begin{itemize}
            \item[$\Longrightarrow$] Suppose that the norm is induced by an inner product $\inprod{\cdot}{\cdot}$. This means for any $\mathbf{x}, \mathbf{y} \in V$:
            \begin{align*}
                \norm{\mathbf{x}+\mathbf{y}}^{2} &= \inprod{\mathbf{x}+\mathbf{y}}{\mathbf{x}+\mathbf{y}} = \norm{\mathbf{x}}^{2} + 2\inprod{\mathbf{x}}{\mathbf{y}} + \norm{\mathbf{y}}^{2},\\
                \norm{\mathbf{x}-\mathbf{y}}^{2} &= \inprod{\mathbf{x}-\mathbf{y}}{\mathbf{x}-\mathbf{y}} = \norm{\mathbf{x}}^{2} - 2\inprod{\mathbf{x}}{\mathbf{y}} + \norm{\mathbf{y}}^{2}.
            \end{align*}
            Therefore,
            \begin{equation*}
                \norm{\mathbf{x}+\mathbf{y}}^{2} + \norm{\mathbf{x}-\mathbf{y}}^{2} = 2\norm{\mathbf{x}}^{2} + 2\norm{\mathbf{y}}^{2}.
            \end{equation*}
            \item[$\Longleftarrow$] Suppose that the parallelogram law holds. We may define a binary operator as:
            \begin{equation*}
                \inprod{\mathbf{x}}{\mathbf{y}} = \frac{1}{4}(\norm{\mathbf{x}+\mathbf{y}}^{2} - \norm{\mathbf{x}-\mathbf{y}}^{2}), \qquad \text{for } \mathbf{x}, \mathbf{y} \in V.
            \end{equation*}
            We check whether this is an inner product.
            \begin{enumerate}
                \item For any $\mathbf{x} \in V$,
                \begin{equation*}
                    \inprod{\mathbf{x}}{\mathbf{x}} = \frac{1}{4}(\norm{2\mathbf{x}}^{2} + \norm{\mathbf{0}}^{2}) = \norm{\mathbf{x}}^{2} \geq 0.
                \end{equation*}
                Moreover,
                \begin{equation*}
                    \inprod{\mathbf{x}}{\mathbf{x}} = \norm{\mathbf{x}}^{2} = 0 \Longleftrightarrow \mathbf{x} = \mathbf{0}.
                \end{equation*}
                \item Since proving homogeneity extending to $\mathbb{R}$ is out of scope, we will only prove additivity here. 
                
                For any $\mathbf{x}, \mathbf{y}, \mathbf{z} \in V$, by the parallelogram law,
                \begin{align*}
                    \norm{\mathbf{x}+\mathbf{y}+\mathbf{z}}^{2} &= 2\norm{\mathbf{x}}^{2} + 2\norm{\mathbf{y}+\mathbf{z}}^{2} - \norm{\mathbf{x}-\mathbf{y}-\mathbf{z}}^{2},\\
                    &= 2\norm{\mathbf{y}}^{2} + 2\norm{\mathbf{x}+\mathbf{z}}^{2} - \norm{-\mathbf{x}+\mathbf{y}-\mathbf{z}}^{2},\\
                    \norm{\mathbf{x}+\mathbf{y}-\mathbf{z}}^{2} &= 2\norm{\mathbf{x}}^{2} + 2\norm{\mathbf{y}-\mathbf{z}}^{2} - \norm{\mathbf{x}-\mathbf{y}+\mathbf{z}}^{2},\\
                    &= 2\norm{\mathbf{y}}^{2} + 2\norm{\mathbf{x}-\mathbf{z}}^{2} - \norm{-\mathbf{x}+\mathbf{y}+\mathbf{z}}^{2}.
                \end{align*}
                Combining the formulas, we have:
                \begin{align*}
                    \norm{\mathbf{x}+\mathbf{y}+\mathbf{z}}^{2} &= \norm{\mathbf{x}}^{2} + \norm{\mathbf{y}}^{2} + \norm{\mathbf{x}+\mathbf{z}}^{2} + \norm{\mathbf{y}+\mathbf{z}}^{2} - \frac{1}{2}\norm{\mathbf{x}-\mathbf{y}-\mathbf{z}}^{2} - \frac{1}{2}\norm{-\mathbf{x}+\mathbf{y}-\mathbf{z}}^{2},\\
                    \norm{\mathbf{x}+\mathbf{y}-\mathbf{z}}^{2} &= \norm{\mathbf{x}}^{2} + \norm{\mathbf{y}}^{2} + \norm{\mathbf{x}-\mathbf{z}}^{2} + \norm{\mathbf{y}-\mathbf{z}}^{2} - \frac{1}{2}\norm{\mathbf{x}-\mathbf{y}+\mathbf{z}}^{2} - \frac{1}{2}\norm{-\mathbf{x}+\mathbf{y}+\mathbf{z}}^{2},\\
                    &= \norm{\mathbf{x}}^{2} + \norm{\mathbf{y}}^{2} + \norm{\mathbf{x}-\mathbf{z}}^{2} + \norm{\mathbf{y}-\mathbf{z}}^{2} - \frac{1}{2}\norm{-\mathbf{x}+\mathbf{y}-\mathbf{z}}^{2} - \frac{1}{2}\norm{\mathbf{x}-\mathbf{y}-\mathbf{z}}^{2}.
                \end{align*}
                Therefore,
                \begin{equation*}
                    \inprod{\mathbf{x}+\mathbf{y}}{\mathbf{z}} = \frac{1}{4}(\norm{\mathbf{x}+\mathbf{y}+\mathbf{z}}^{2} - \norm{\mathbf{x}+\mathbf{y}-\mathbf{z}}^{2}) = \frac{1}{4}(\norm{\mathbf{x}+\mathbf{z}}^{2} - \norm{\mathbf{x}-\mathbf{z}}^{2} + \norm{\mathbf{y}+\mathbf{z}}^{2} - \norm{\mathbf{y}-\mathbf{z}}^{2}) = \inprod{\mathbf{x}}{\mathbf{z}} + \inprod{\mathbf{y}}{\mathbf{z}}.
                \end{equation*}
                \item For any $\mathbf{x}, \mathbf{y} \in V$,
                \begin{equation*}
                    \inprod{\mathbf{y}}{\mathbf{x}} = \frac{1}{4}(\norm{\mathbf{y}+\mathbf{x}}^{2} - \norm{\mathbf{y}-\mathbf{x}}^{2}) = \frac{1}{4}(\norm{\mathbf{x}+\mathbf{y}}^{2} - \norm{\mathbf{x}-\mathbf{y}}^{2}) = \inprod{\mathbf{x}}{\mathbf{y}}.
                \end{equation*}
            \end{enumerate}
            Therefore, since additionally $\sqrt{\inprod{\mathbf{x}}{\mathbf{x}}} = \norm{\mathbf{x}}$, the binary operator is an inner product that induces the norm.
        \end{itemize}
    \end{proofing}
    \newpage

    Similar to normed vector spaces, we can define the completeness of a vector space with an inner product.
    \begin{defn}
        A complete inner product space is called a \textbf{Hilbert space}.
    \end{defn}
    \begin{eg}
        $\mathbb{R}^{n}$ with the standard inner product or the weighted inner product for any symmetric positive definite $\mathbf{A} \in \mathbb{R}^{n \times n}$:
        \begin{equation*}
            \inprod{\mathbf{x}}{\mathbf{y}} = \mathbf{x}^{T}\mathbf{y}, \qquad \inprod{\mathbf{x}}{\mathbf{y}}_{\mathbf{A}} = \mathbf{x}^{T}\mathbf{A}\mathbf{y}, \qquad \text{for } \mathbf{x}, \mathbf{y} \in \mathbb{R}^{n},
        \end{equation*}
        are Hilbert spaces.
    \end{eg}
    \begin{eg}
        $\mathbb{R}^{m \times n}$ with the standard inner product:
        \begin{equation*}
            \inprod{\mathbf{A}}{\mathbf{B}} = \Tr(\mathbf{A}^{T}\mathbf{B}), \qquad \mathbf{A}, \mathbf{B} \in \mathbb{R}^{m \times n},
        \end{equation*}
        is a Hilbert space.
    \end{eg}
    \begin{eg}
        $\ell_{2}$ with the standard inner product:
        \begin{equation*}
            \inprod{\mathbf{x}}{\mathbf{y}} = \sum_{i=1}^{\infty}x_{i}y_{i}, \qquad \mathbf{x}, \mathbf{y} \in \ell_{2},
        \end{equation*}
        is a Hilbert space.
    \end{eg}
    \begin{eg}
        $\mathcal{C}[a, b]$ with the standard inner product:
        \begin{equation*}
            \inprod{f}{g} = \int_{a}^{b}f(t)g(t)\,dt, \qquad f, g \in \mathcal{C}[a, b],
        \end{equation*}
        is not a Hilbert space. Its completion is $L^{2}(a, b)$.
    \end{eg}
    \newpage
    
\section{Orthogonality}
    By the Cauchy-Schwarz Inequality, for all non-zero $\mathbf{x}, \mathbf{y} \in V$:
    \begin{equation*}
        -\norm{\mathbf{x}}\norm{\mathbf{y}} \leq \inprod{\mathbf{x}}{\mathbf{y}} \leq \norm{\mathbf{x}}\norm{\mathbf{y}} \Longleftrightarrow -1 \leq \frac{\inprod{\mathbf{x}}{\mathbf{y}}}{\norm{\mathbf{x}}\norm{\mathbf{y}}} \leq 1.
    \end{equation*}
    Considering when the Cauchy-Schwarz Inequality achieves equality:
    \begin{enumerate}
        \item If $\frac{\inprod{\mathbf{x}}{\mathbf{y}}}{\norm{\mathbf{x}}\norm{\mathbf{y}}} = 1$, then $\mathbf{y} = \alpha\mathbf{x}$ with $\alpha > 0$. (If $\alpha \leq 0$, then $\inprod{\mathbf{x}}{\mathbf{y}} = \alpha\inprod{\mathbf{x}}{\mathbf{x}} = \alpha\norm{\mathbf{x}}^{2} \leq 0$.)
        \begin{figure}[h]
            \centering
            \begin{tikzpicture}
                \draw[thick, ->]		(0,0) -- (1,0.5) node[above, midway] {$\mathbf{x}$};
                \draw[thick, red, ->]	(0.5,0) -- (2.5,1) node[below, midway] {$\mathbf{y}$};
            \end{tikzpicture}
            \caption{$\mathbf{y} = 2\mathbf{x}$}
        \end{figure}
        \item If $\frac{\inprod{\mathbf{x}}{\mathbf{y}}}{\norm{\mathbf{x}}\norm{\mathbf{y}}} = -1$, then $\mathbf{y} = \alpha\mathbf{x}$ with $\alpha < 0$. (If $\alpha \geq 0$, then $\inprod{\mathbf{x}}{\mathbf{y}} = \alpha\inprod{\mathbf{x}}{\mathbf{x}} = \alpha\norm{\mathbf{x}}^{2} \geq 0$.)
        \begin{figure}[h]
            \centering
            \begin{tikzpicture}
                \draw[thick, ->]		(0,0) -- (1,0.5) node[above, midway] {$\mathbf{x}$};
                \draw[thick, red, ->]	(0,0) -- (-2,-1) node[below, midway] {$\mathbf{y}$};
            \end{tikzpicture}
            \caption{$\mathbf{y} = -2\mathbf{x}$}
        \end{figure}
    \end{enumerate}
    \begin{defn}
        The \textbf{angle} between nonzero $\mathbf{x}, \mathbf{y} \in V$ is defined by:
        \begin{equation*}
            \angle(\mathbf{x}, \mathbf{y}) = \arccos\left(\frac{\inprod{\mathbf{x}}{\mathbf{y}}}{\norm{\mathbf{x}}\norm{\mathbf{y}}}\right).
        \end{equation*}
    \end{defn}
    With angles defined, we can define orthogonality.
    \begin{defn}
        For $\mathbf{x}, \mathbf{y} \in V$:
        \begin{enumerate}
            \item If $\abs{\frac{\inprod{\mathbf{x}}{\mathbf{y}}}{\norm{\mathbf{x}}\norm{\mathbf{y}}}} = 1$, then $\mathbf{x}$ and $\mathbf{y}$ are the \textbf{most correlated}.
            \item If $\abs{\frac{\inprod{\mathbf{x}}{\mathbf{y}}}{\norm{\mathbf{x}}\norm{\mathbf{y}}}} = 0$ ($\inprod{\mathbf{x}}{\mathbf{y}} = 0$), then $\mathbf{x}$ and $\mathbf{y}$ are the \textbf{least correlated}. We say $\mathbf{x}$ and $\mathbf{y}$ are \textbf{orthogonal}.
        \end{enumerate}
    \end{defn}
    \begin{figure}[h]
        \centering
        \begin{subfigure}{0.3\textwidth}
            \centering
            \begin{tikzpicture}
                \draw[thick, ->]	(0,0) -- (0,1) node[above] {$\mathbf{x}$};
                \draw[thick, ->]	(0,0) -- (1,0) node[right] {$\mathbf{y}$};
                \draw[] (0,0) rectangle ++(0.2, 0.2);
            \end{tikzpicture}
            
            $\angle(\mathbf{x}, \mathbf{y}) = \frac{\pi}{2}$
            \caption{The least correlated}
        \end{subfigure}
        \begin{subfigure}{0.6\textwidth}
            \centering
            \begin{subfigure}{0.4\textwidth}
                \centering
                \begin{tikzpicture}
                    \draw[thick, ->]		(0,0) -- (1,0.5) node[above, midway] {$\mathbf{x}$};
                    \draw[thick, red, ->]	(0.5,0) -- (2.5,1) node[below, midway] {$\mathbf{y}$};
                \end{tikzpicture}
                
                $\angle(\mathbf{x}, \mathbf{y}) = 0$
            \end{subfigure}
            \begin{subfigure}{0.4\textwidth}
                \centering
                \begin{tikzpicture}
                    \draw[thick, ->]		(0,0) -- (1,0.5) node[above, midway] {$\mathbf{x}$};
                    \draw[thick, red, ->]	(0,0) -- (-2,-1) node[below, midway] {$\mathbf{y}$};
                \end{tikzpicture}
                
                $\angle(\mathbf{x}, \mathbf{y}) = \pi$
            \end{subfigure}
            \caption{The most correlated}
        \end{subfigure}
    \end{figure}
    Based on orthogonality, we have the following theorem.
    \begin{thm}\named{Pythagorean Theorem}
        For $\mathbf{x}, \mathbf{y} \in V$, $\mathbf{x}$ and $\mathbf{y}$ are orthogonal if and only if:
        \begin{equation*}
            \norm{\mathbf{x} + \mathbf{y}}^{2} = \norm{\mathbf{x}}^{2} + \norm{\mathbf{y}}^{2}.
        \end{equation*}
    \end{thm}
    \begin{proofing}
        If $\mathbf{x}$ and $\mathbf{y}$ are orthogonal, then $\inprod{\mathbf{x}}{\mathbf{y}} = 0$. Therefore:
        \begin{equation*}
            \norm{\mathbf{x} + \mathbf{y}}^{2} = \inprod{\mathbf{x} + \mathbf{y}}{\mathbf{x} + \mathbf{y}} = \inprod{\mathbf{x}}{\mathbf{x}} + 2\inprod{\mathbf{x}}{\mathbf{y}} + \inprod{\mathbf{y}}{\mathbf{y}} = \inprod{\mathbf{x}}{\mathbf{x}} + \inprod{\mathbf{y}}{\mathbf{y}} = \norm{\mathbf{x}}^{2} + \norm{\mathbf{y}}^{2}.
        \end{equation*}
        If $\norm{\mathbf{x} + \mathbf{y}}^{2} = \norm{\mathbf{x}}^{2} + \norm{\mathbf{y}}^{2}$, then:
        \begin{equation*}
            0 = \norm{\mathbf{x} + \mathbf{y}}^{2} - \norm{\mathbf{x}}^{2} - \norm{\mathbf{y}}^{2} = \inprod{\mathbf{x} + \mathbf{y}}{\mathbf{x} + \mathbf{y}} - \inprod{\mathbf{x}}{\mathbf{x}} - \inprod{\mathbf{y}}{\mathbf{y}} = 2\inprod{\mathbf{x}}{\mathbf{y}}.
        \end{equation*}
        Therefore, $\inprod{\mathbf{x}}{\mathbf{y}} = 0$, and thus $\mathbf{x}$ and $\mathbf{y}$ are orthogonal.
    \end{proofing}
    Read Appendix \ref{Case Study B: Kernel K-means/Kernel Trick} (Kernel K-means/Kernel Trick) and \ref{Case Study C: Metric Learning} (Metric Learning) to see the case studies for Chapter \ref{Chapter 3: Inner products, Hilbert Spaces}.

\chapter{Linear Functions and Differentiation}
    \label{Chapter 4: Linear Functions and Differentiation}
    In Chapter \ref{Chapter 2: Vector spaces, metrics, limits, and convergence}, we observed that the norm does not preserve vector addition but does preserve scalar multiplication. In Chapter \ref{Chapter 3: Inner products, Hilbert Spaces}, by fixing one of the vectors, we demonstrated linearity. Here, we aim to investigate the behavior of linear functions in vector spaces.
\section{Linear Functions}
    \begin{defn}
        Let $V$ be a vector space over $\mathbb{R}$. A function $f:V\to\mathbb{R}$ is \textbf{linear} if, for all $\mathbf{x},\mathbf{y}\in V$ and $\alpha,\beta\in\mathbb{R}$,
        \begin{equation*}
            f(\alpha\mathbf{x}+\beta\mathbf{y})=\alpha f(\mathbf{x})+\beta f(\mathbf{y}).
        \end{equation*}
    \end{defn}
    \begin{eg}
        The mean of a vector (not to be confused with mean vectors): for all $\mathbf{x}\in\mathbb{R}^{n}$,
        \begin{equation*}
            f(\mathbf{x})=\frac{1}{n}\sum_{i=1}^{n}x_{i}
        \end{equation*}
        is a linear function.
    \end{eg}
    \begin{proofing}
        For all $\mathbf{x},\mathbf{y}\in\mathbb{R}^{n}$ and $\alpha,\beta\in\mathbb{R}$,
        \begin{equation*}
            f(\alpha\mathbf{x}+\beta\mathbf{y})=\frac{1}{n}\sum_{i=1}^{n}(\alpha x_{i}+\beta y_{i})=\frac{\alpha}{n}\sum_{i=1}^{n}x_{i}+\frac{\beta}{n}\sum_{i=1}^{n}y_{i}=\alpha f(\mathbf{x})+\beta f(\mathbf{y}).
        \end{equation*}
    \end{proofing}
    \begin{eg}
        The maximum entry of a vector: for all $\mathbf{x}\in\mathbb{R}^{n}$,
        \begin{equation*}
            f(\mathbf{x})=\max_{1\leq i\leq n}x_{i}
        \end{equation*}
        is not a linear function.
    \end{eg}
    \begin{proofing}
        Assume that $\mathbf{x}=\begin{pmatrix}1\\0\end{pmatrix}$, $\mathbf{y}=\begin{pmatrix}0\\1\end{pmatrix}$, $\alpha=1$, $\beta=1$. We have:
        \begin{align*}
            f(\alpha\mathbf{x}+\beta\mathbf{y})&=f\left(\begin{pmatrix}1\\1\end{pmatrix}\right)=1, & \alpha f(\mathbf{x})+\beta f(\mathbf{y})&=f\left(\begin{pmatrix}1\\0\end{pmatrix}\right)+f\left(\begin{pmatrix}0\\1\end{pmatrix}\right)=2\neq f(\alpha\mathbf{x}+\beta\mathbf{y}),
        \end{align*}
        which violates the definition of a linear function.
    \end{proofing}
    \begin{eg}
        \label{Chapter 4 (Example) Inner product with fixed vector is a linear function}
        Let $V$ be a normed vector space with an inner product $\inprod{\cdot}{\cdot}$ and let $\mathbf{a}\in V$. The function $f:V\to\mathbb{R}$ defined by:
        \begin{equation*}
            f(\mathbf{x})=\inprod{\mathbf{a}}{\mathbf{x}} \qquad \text{for } \mathbf{x}\in V
        \end{equation*} 
        is a linear function.
    \end{eg}
    \begin{proofing}
        For all $\mathbf{x},\mathbf{y}\in V$ and $\alpha,\beta\in\mathbb{R}$,
        \begin{equation*}
            f(\alpha\mathbf{x}+\beta\mathbf{y})=\inprod{\mathbf{a}}{\alpha\mathbf{x}+\beta\mathbf{y}}=\alpha\inprod{\mathbf{a}}{\mathbf{x}}+\beta\inprod{\mathbf{a}}{\mathbf{y}}=\alpha f(\mathbf{x})+\beta f(\mathbf{y}).
        \end{equation*}
    \end{proofing}
    \newpage

    \begin{eg}
        A functional $F:\mathcal{C}[-1,1]\to\mathbb{R}$ defined by:
        \begin{equation*}
            F(f)=f(0) \qquad \text{for } f\in\mathcal{C}[-1,1]
        \end{equation*}
        is a linear function.
    \end{eg}
    \begin{proofing}
        For all $f,g\in\mathcal{C}[-1,1]$ and $\alpha,\beta\in\mathbb{R}$,
        \begin{equation*}
            F(\alpha f+\beta g)=(\alpha f+\beta g)(0)=\alpha f(0)+\beta g(0)=\alpha F(f)+\beta F(g).
        \end{equation*}
    \end{proofing}
    \begin{eg}
        A functional $F:\mathcal{C}[a,b]\to\mathbb{R}$ defined by:
        \begin{equation*}
            F(f)=\int_{a}^{b}f(t)\,dt \qquad \text{for } f\in\mathcal{C}[a,b]
        \end{equation*}
        is a linear function.
    \end{eg}
    \begin{proofing}
        For all $f,g\in\mathcal{C}[a,b]$ and $\alpha,\beta\in\mathbb{R}$,
        \begin{equation*}
            F(\alpha f+\beta g)=\int_{a}^{b}(\alpha f+\beta g)(t)\,dt=\int_{a}^{b}(\alpha f(t)+\beta g(t))\,dt=\alpha\int_{a}^{b}f(t)\,dt+\beta\int_{a}^{b}g(t)\,dt=\alpha F(f)+\beta F(g).
        \end{equation*}
    \end{proofing}
    \begin{thm}
        For any vector space $V$, any norm function $\norm{\cdot}$ on $V$ is not a linear function.
    \end{thm}
    \begin{proofing}
        By the absolute homogeneity property of the norm, for all $\mathbf{x}\in V$,
        \begin{equation*}
            \norm{-\mathbf{x}}=\norm{\mathbf{x}}.
        \end{equation*}
        Assume that the norm function is linear. Then:
        \begin{equation*}
            \norm{-\mathbf{x}}=\norm{-\mathbf{x}+0\mathbf{x}}=-\norm{\mathbf{x}}+0\norm{\mathbf{x}}=-\norm{\mathbf{x}},
        \end{equation*}
        which results in a contradiction. Therefore, any norm function is not a linear function.
    \end{proofing}
    The following properties can be easily derived from the definition.
    \begin{thm}
        A linear function $f$ has the following properties:
        \begin{enumerate}
            \item Homogeneity: For all $\mathbf{x}\in V$ and $\alpha\in\mathbb{R}$,
            \begin{equation*}
                f(\alpha\mathbf{x})=\alpha f(\mathbf{x}).
            \end{equation*}
            \item Additivity: For any $\mathbf{x},\mathbf{y}\in V$,
            \begin{equation*}
                f(\mathbf{x}+\mathbf{y})=f(\mathbf{x})+f(\mathbf{y}).
            \end{equation*}
        \end{enumerate}
    \end{thm}
    \begin{proofing}
        \begin{enumerate}
            \item Setting $\beta=0$, for any $\mathbf{x}\in V$ and $\alpha\in\mathbb{R}$,
            \begin{equation*}
                f(\alpha\mathbf{x})=f(\alpha\mathbf{x}+0\mathbf{y})=\alpha f(\mathbf{x})+0f(\mathbf{y})=\alpha f(\mathbf{x}), \qquad \text{for } \mathbf{y}\in V.
            \end{equation*}
            \item Setting $\alpha=\beta=1$, for any $\mathbf{x},\mathbf{y}\in V$,
            \begin{equation*}
                f(\mathbf{x}+\mathbf{y})=f(1\mathbf{x}+1\mathbf{y})=1f(\mathbf{x})+1f(\mathbf{y})=f(\mathbf{x})+f(\mathbf{y}).
            \end{equation*}
        \end{enumerate}
    \end{proofing}
    \begin{rem}
        By induction, for $\mathbf{x}_{1},\cdots,\mathbf{x}_{k}\in V$ and $\alpha_{1},\cdots,\alpha_{k}\in \mathbb{R}$,
        \begin{equation*}
            f(\alpha_{1}\mathbf{x}_{1}+\cdots+\alpha_{k}\mathbf{x}_{k})=\alpha_{1}f(\mathbf{x}_{1})+\cdots+\alpha_{k}f(\mathbf{x}_{k}).
        \end{equation*}
    \end{rem}
    \newpage

    Let $H$ be a Hilbert space. From Example \ref{Chapter 4 (Example) Inner product with fixed vector is a linear function}, we have shown that for all $\mathbf{a}\in H$, the function:
    \begin{equation*}
        f(\mathbf{x})=\inprod{\mathbf{a}}{\mathbf{x}} \qquad \text{for } \mathbf{x}\in H
    \end{equation*}
    is a linear function. Is it true that for all linear functions $f:H\to\mathbb{R}$, there exists a fixed vector $\mathbf{a}\in H$ such that the function can be written in inner product form? The answer is yes!
    \begin{thm}\named{Riesz Representation Theorem}
        Let $H$ be a Hilbert space with an inner product $\inprod{\cdot}{\cdot}$. The function $f:H\to\mathbb{R}$ is linear and bounded if and only if:
        \begin{equation*}
            f(\mathbf{x})=\inprod{\mathbf{a}}{\mathbf{x}}, \qquad \text{for } \mathbf{x}\in H
        \end{equation*}
        for some unique $\mathbf{a}\in H$, called the \textbf{Riesz representation} of $f$.
    \end{thm}
    \begin{proofing}[Proof (When $H=\mathbb{R}^{n}$)]
        For all $\mathbf{x}\in\mathbb{R}^{n}$, we have:
        \begin{equation*}
            \mathbf{x}=x_{1}\mathbf{e}_{1}+\cdots+x_{n}\mathbf{e}_{n}.
        \end{equation*}
        Therefore, we have:
        \begin{equation*}
            f(\mathbf{x})=f(x_{1}\mathbf{e}_{1}+\cdots+x_{n}\mathbf{e}_{n})=x_{1}f(\mathbf{e}_{1})+\cdots+x_{n}f(\mathbf{e}_{n})=\inprod{\begin{pmatrix}
                f(\mathbf{e}_{1})\\\vdots\\f(\mathbf{e}_{n})
            \end{pmatrix}}{\mathbf{x}} \Longrightarrow \mathbf{a}=\begin{pmatrix}
                f(\mathbf{e}_{1})\\\vdots\\f(\mathbf{e}_{n})
            \end{pmatrix}.
        \end{equation*}
        Suppose that $\mathbf{a}$ is not unique. This means there exist $\mathbf{a},\mathbf{b}\in\mathbb{R}^{n}$ such that:
        \begin{equation*}
            f(\mathbf{x})=\inprod{\mathbf{a}}{\mathbf{x}}=\inprod{\mathbf{b}}{\mathbf{x}} \qquad \text{for } \mathbf{x}\in\mathbb{R}^{n}.
        \end{equation*}
        If we choose $\mathbf{x}=\mathbf{e}_{i}$ for $i=1,\cdots,n$,
        \begin{align*}
            f(\mathbf{e}_{i})=\inprod{\mathbf{a}}{\mathbf{e}_{i}}=\inprod{\mathbf{b}}{\mathbf{e}_{i}} &\Longrightarrow a_{i}=b_{i}, \qquad \text{for } i=1,\cdots,n,\\
            &\Longrightarrow \mathbf{a}=\mathbf{b}.
        \end{align*}
        This results in a contradiction. Therefore, $\mathbf{a}$ is unique.
    \end{proofing}
    \begin{rem}
        The "boundedness" implied in this theorem does not mean the codomain of the function is a bounded set. Given a function $f:X\to Y$ that maps between two normed vector spaces, if $X$ has a norm $\pnorm[X]{\cdot}$ and $Y$ has a norm $\pnorm[Y]{\cdot}$, then there exists some $M>0$ such that:
        \begin{equation*}
            \pnorm[Y]{f(\mathbf{x})}\leq M\pnorm[X]{\mathbf{x}}, \qquad \text{for } \mathbf{x}\in X.
        \end{equation*}
        In this theorem, if $H$ induces a norm $\pnorm[H]{\cdot}$, then there exists some $M>0$ such that:
        \begin{equation*}
            \abs{f(\mathbf{x})}\leq M\pnorm[H]{\mathbf{x}}, \qquad \text{for } \mathbf{x}\in H.
        \end{equation*}
    \end{rem}
    \begin{rem}
        The smallest value $M$ is often called the \textbf{operator norm}. Recall Example \ref{Chapter 2 (Example) Matrix 2-norm}. It can be generalized to:
        \begin{equation*}
            M=\pnorm[op]{f}=\sup\{\pnorm[Y]{f(\mathbf{x})}:\mathbf{x}\in X \text{ with } \pnorm[X]{\mathbf{x}}\leq 1\},
        \end{equation*}
        which depends on the choice of norms.
    \end{rem}
    \begin{rem}
        Any linear function defined on a finite-dimensional normed vector space is always bounded.
    \end{rem}
    \begin{eg}
        The mean of a vector $\mathbf{x}\in\mathbb{R}^{n}$ can be represented by:
        \begin{equation*}
            f(\mathbf{x})=\frac{1}{n}\sum_{i=1}^{n}x_{i}=\inprod{\frac{1}{n}\begin{pmatrix}
                    1\\
                    \vdots\\
                    1
                \end{pmatrix}}{\mathbf{x}}.
        \end{equation*}
    \end{eg}
    \begin{eg}
        The trace of a matrix $\mathbf{A}\in\mathbb{R}^{n\times n}$ can be represented by:
        \begin{equation*}
            \Tr(\mathbf{A})=\sum_{i=1}^{n}a_{ii}=\inprod{\mathbf{I}}{\mathbf{A}}.
        \end{equation*}
    \end{eg}
    \newpage

    \begin{rem}
        In infinite-dimensional Hilbert spaces, there exist linear but unbounded functions.
    \end{rem}
    \begin{eg}
        We consider $L^{2}(-1,1)$, which is the completion of $\mathcal{C}[-1,1]$ under:
        \begin{align*}
            \inprod{f}{g}&=\int_{-1}^{1}f(t)g(t)\,dt, & \pnorm[2]{f}&=\sqrt{\inprod{f}{f}}.
        \end{align*}
        Let $F(f)=f(0)$ for all $f\in L^{2}(-1,1)$. Consider that:
        \begin{equation*}
            f(t)=\begin{cases}
                1, &t\neq 0,\\
                +\infty, &t=0
            \end{cases}, \qquad \text{for } t\in(-1,1).
        \end{equation*}
        Therefore, $F(f)=f(0)=+\infty$. There is, in fact, no inner product representation for $F(f)=f(0)$.
    \end{eg}
    \begin{eg}
        We consider $G:L^{2}(-1,1)\to\mathbb{R}$ defined by:
        \begin{equation*}
            G(f)=\int_{-1}^{1}f(t)\,dt, \qquad \text{for } f\in L^{2}(-1,1).
        \end{equation*} 
        For any $f,g\in L^{2}(-1,1)$ and $\alpha,\beta\in\mathbb{R}$,
        \begin{equation*}
            G(\alpha f+\beta g)=\int_{-1}^{1}(\alpha f+\beta g)(t)\,dt=\alpha\int_{-1}^{1}f(t)\,dt+\beta\int_{-1}^{1}g(t)\,dt=\alpha G(f)+\beta G(g).
        \end{equation*}
        Therefore, we can find that $G$ is linear. We can also see that for any $f\in L^{2}(-1,1)$,
        \begin{equation*}
            G(f)=\int_{-1}^{1}f(t)\,dt=\int_{-1}^{1}1\cdot f(t)\,dt=\inprod{1}{f}\leq\pnorm[2]{f}\sqrt{\int_{-1}^{1}1^{2}\,dt}=\sqrt{2}\pnorm[2]{f}.
        \end{equation*}
        Therefore, $G$ is also bounded. In fact, we have found that:
        \begin{equation*}
            G(f)=\inprod{1}{f}.
        \end{equation*}
    \end{eg}
    \begin{eg}
        We consider $\ell_{2}$, which is the completion of the space of all finite sequences under:
        \begin{align*}
            \inprod{\mathbf{x}}{\mathbf{y}}&=\sum_{i=1}^{\infty}x_{i}y_{i}, & \pnorm[2]{\mathbf{x}}&=\sqrt{\inprod{\mathbf{x}}{\mathbf{x}}}.
        \end{align*}
        Let $H:\ell_{2}\to\mathbb{R}$ be defined by:
        \begin{equation*}
            H(\mathbf{x})=x_{1}, \qquad \text{for } \mathbf{x}\in\ell_{2}.
        \end{equation*}
        For any $\mathbf{x},\mathbf{y}\in\ell_{2}$ and $\alpha,\beta\in\mathbb{R}$,
        \begin{equation*}
            H(\alpha\mathbf{x}+\beta\mathbf{y})=\alpha x_{1}+\beta y_{1}=\alpha H(\mathbf{x})+\beta H(\mathbf{y}).
        \end{equation*}
        Therefore, we can find that $H$ is linear. We can also see that for any $\mathbf{x}\in\ell_{2}$,
        \begin{equation*}
            H(\mathbf{x})=x_{1}\leq\sqrt{\sum_{i=1}^{\infty}x_{i}^{2}}=\pnorm[2]{\mathbf{x}}.
        \end{equation*}
        Therefore, $H$ is also bounded. In fact, we have found that:
        \begin{equation*}
            H(\mathbf{x})=\inprod{\begin{pmatrix}
                    1\\
                    0\\
                    0\\
                    \vdots
                \end{pmatrix}}{\mathbf{x}}.
        \end{equation*}
    \end{eg}
    \newpage

    Additionally, we have a function that is similar to a linear function but shifted in the output space.
    \begin{defn}
        Let $V$ be a vector space over $\mathbb{R}$. A function $f:V\to\mathbb{R}$ is \textbf{affine} if it can be written as:
        \begin{equation*}
            f(\mathbf{x})=g(\mathbf{x})+b, \qquad \text{for } \mathbf{x}\in V,
        \end{equation*}
        where $g:V\to\mathbb{R}$ is linear and $b\in\mathbb{R}$.
    \end{defn}
    We can also derive the following properties from the definition.
    \begin{thm}
        \label{Chapter 4 (Theorem) Properties of affine functions}
        An affine function $f$ has the following properties:
        \begin{enumerate}
            \item For any $\alpha,\beta\in\mathbb{R}$ such that $\alpha+\beta=1$,
            \begin{equation*}
                f(\alpha\mathbf{x}+\beta\mathbf{y})=\alpha f(\mathbf{x})+\beta f(\mathbf{y}).
            \end{equation*}
            \item If $H$ is a Hilbert space and $f$ is bounded, then $f$ is affine if and only if:
            \begin{equation*}
                f(\mathbf{x}) = \inprod{\mathbf{a}}{\mathbf{x}}+b,
            \end{equation*}
            for some $\mathbf{a}\in H$ and $b\in\mathbb{R}$.
        \end{enumerate}
    \end{thm}
    \begin{proofing}
        \begin{enumerate}
            \item There exists a linear function $g$ such that:
            \begin{equation*}
                f(\mathbf{x})=g(\mathbf{x})+b, \qquad \text{for } \mathbf{x}\in V,
            \end{equation*}
            where $b\in\mathbb{R}$. If $\alpha+\beta=1$,
            \begin{align*}
                f(\alpha\mathbf{x}+\beta\mathbf{y})&=g(\alpha\mathbf{x}+\beta\mathbf{y})+b,\\
                \tag{$\alpha+\beta=1$}
                &=\alpha g(\mathbf{x})+\beta g(\mathbf{y})+(\alpha+\beta)b,\\
                &=\alpha(g(\mathbf{x})+b)+\beta(g(\mathbf{y})+b),\\
                &=\alpha f(\mathbf{x})+\beta f(\mathbf{y}).
            \end{align*}
            \item \begin{itemize}
            	\item[$\Longrightarrow$] If $f$ is affine, then there exists a linear function $g$ such that:
            	\begin{align*}
            		f(\mathbf{x})&=g(\mathbf{x})+b, & g(\mathbf{x})&=f(\mathbf{x})-b, \qquad \text{for } \mathbf{x}\in V,
            	\end{align*}
            	where $b\in\mathbb{R}$. Since $f$ is bounded, there exists some $M_{f}>0$ such that:
            	\begin{align*}
            		\abs{f(\mathbf{x})}&\leq M_{f}\pnorm[H]{\mathbf{x}}, & b=\abs{f(\mathbf{0})}&\leq M_{f}\pnorm[H]{\mathbf{0}}=0.
            	\end{align*}
            	By the Riesz Representation Theorem, there exists some $\mathbf{a}\in H$ such that:
            	\begin{equation*}
            		g(\mathbf{x})=\inprod{\mathbf{a}}{\mathbf{x}}, \qquad \text{for } \mathbf{x}\in H.
            	\end{equation*}
            	Therefore,
            	\begin{equation*}
            		f(\mathbf{x})=\inprod{\mathbf{a}}{\mathbf{x}}+b, \qquad \text{for } \mathbf{x}\in H.
            	\end{equation*}
            	\item[$\Longleftarrow$] If there exists some $\mathbf{a}\in H$ and $b\in\mathbb{R}$ such that:
            	\begin{equation*}
            		f(\mathbf{x})=\inprod{\mathbf{a}}{\mathbf{x}}+b, \qquad \text{for } \mathbf{x}\in H,
            	\end{equation*}
            	then, by Example \ref{Chapter 4 (Example) Inner product with fixed vector is a linear function}, $\inprod{\mathbf{a}}{\mathbf{x}}$ is a linear function. Therefore, $f$ is affine.
            \end{itemize}
        \end{enumerate}
    \end{proofing}
    \newpage

\section{Hyperplane}
    \begin{defn}
        Let $V$ be a vector space over $\mathbb{R}$. A subset $U \subset V$ is a \textbf{(linear) subspace} of $V$ if, for any $\mathbf{u},\mathbf{v}\in U$ and $\alpha, \beta \in \mathbb{R}$,
        \begin{equation*}
            \alpha\mathbf{u} + \beta\mathbf{v} \in U.
        \end{equation*}
    \end{defn}
    \begin{defn}
        Let $V$ be a vector space over $\mathbb{R}$. A set $W$ is an \textbf{affine subspace} if:
        \begin{equation*}
            W = \mathbf{a}+U = \{\mathbf{a}+\mathbf{u}:\mathbf{u}\in U\},
        \end{equation*}
        where $\mathbf{a} \in V$ and $U$ is a linear subspace of $V$.
    \end{defn}
    Let $H$ be a Hilbert space and $\mathbf{a}\in H$. We denote:
    \begin{equation*}
        S_{\mathbf{a},b} = \{\mathbf{x} \in H:\inprod{\mathbf{a}}{\mathbf{x}}=b\} \subset H.
    \end{equation*}
    Consider $b = 0$. For all $\mathbf{x}, \mathbf{y} \in S_{\mathbf{a},0}$ and $\alpha, \beta \in \mathbb{R}$,
    \begin{equation*}
        \inprod{\mathbf{a}}{\alpha\mathbf{x} + \beta\mathbf{y}} = \alpha\inprod{\mathbf{a}}{\mathbf{x}} + \beta\inprod{\mathbf{a}}{\mathbf{y}} = 0.
    \end{equation*}
    This means that $\alpha\mathbf{x} + \beta\mathbf{y} \in S_{\mathbf{a},0}$. Therefore, $S_{\mathbf{a},0}$ is a linear subspace of $H$.

    For a general $b$, let $\mathbf{x}_{0}\in S_{\mathbf{a},b}$. Then we have:
    \begin{equation*}
        \inprod{\mathbf{a}}{\mathbf{x}_{0}}=b.
    \end{equation*}
    For all $\mathbf{x} \in S_{\mathbf{a},b}$,
    \begin{align*}
        \inprod{\mathbf{a}}{\mathbf{x} - \mathbf{x}_{0}} = \inprod{\mathbf{a}}{\mathbf{x}} - \inprod{\mathbf{a}}{\mathbf{x}_{0}} = b-b = 0 &\Longrightarrow \mathbf{x}-\mathbf{x}_{0}\in S_{\mathbf{a},0},\\
        &\Longrightarrow \mathbf{x} \in \mathbf{x}_{0} + S_{\mathbf{a},0},\\
        &\Longrightarrow S_{\mathbf{a},b} \subset \mathbf{x}_{0} + S_{\mathbf{a},0}.
    \end{align*}
    For all $\mathbf{x} \in S_{\mathbf{a},0}$,
    \begin{align*}
        \inprod{\mathbf{a}}{\mathbf{x} + \mathbf{x}_{0}} = \inprod{\mathbf{a}}{\mathbf{x}} + \inprod{\mathbf{a}}{\mathbf{x}_{0}} = 0 + b = b &\Longrightarrow \mathbf{x} + \mathbf{x}_{0} \in S_{\mathbf{a},b},\\
        &\Longrightarrow S_{\mathbf{a},0} + \mathbf{x}_{0} \subset S_{\mathbf{a},b}.
    \end{align*}
    Therefore, we have $S_{\mathbf{a},b} = \mathbf{x}_{0} + S_{\mathbf{a},0}$, and thus $S_{\mathbf{a},b}$ is an affine subspace. We can define such a set as a hyperplane.
    \begin{defn}
        A \textbf{hyperplane} $S$ in the Hilbert space $H$ is defined by:
        \begin{equation*}
            S = \{\mathbf{x} \in H:\inprod{\mathbf{a}}{\mathbf{x}} = b\} \subset H,
        \end{equation*}
        where $\mathbf{a} \in H$ and $b \in \mathbb{R}$. 
    \end{defn}
    \begin{rem}
        If $H$ has a dimension of $n$, then the hyperplane $S$ has a dimension of $n-1$ or a codimension of $1$.
    \end{rem}
    \begin{figure}[h!]
        \centering
        \begin{tikzpicture}
            \draw[blue] (-2, 2) node {$H$};
            \draw[thick] (-2,-0.5) -- (1,-0.5) -- (2,0.5) node[below right, midway] {$S_{\mathbf{a},0}$} -- (-1,0.5) -- (-2,-0.5);
            \draw[thick, red] (-0.5,1) -- (2.5,1) -- (3.5,2) node[below right, midway] {$S_{\mathbf{a},b}$} -- (0.5,2) -- (-0.5,1);
            \draw[thick, blue, ->] (0,0) node[left] {$O$} -- (1.5,1.5) node[right] {$\mathbf{x}_{0}$};
        \end{tikzpicture}
        \caption{Hyperplanes}
    \end{figure}
    \newpage

    For any vectors that do not lie on the hyperplane, we can project those vectors onto the hyperplane.
    \begin{defn}
        Consider a hyperplane $S \subset H$. For any $\mathbf{y} \in H$, the vector on $S$ that is closest to $\mathbf{y}$ is called the \textbf{projection} of $\mathbf{y}$ onto $S$, denoted by $P_{S}\mathbf{y}$. Equivalently:
        \begin{equation*}
            P_{S}\mathbf{y} = \argmin_{\mathbf{x} \in S}\norm{\mathbf{y} - \mathbf{x}}.
        \end{equation*} 
    \end{defn}
    How do we find the explicit form of such an equation?
    \begin{thm}
        \label{Chapter 4 (Theorem) Criteria for Projection Vector}
        Given a hyperplane $S \subset H$, the vector $\mathbf{z} \in H$ is a solution of:
        \begin{equation*}
            \min_{\mathbf{x} \in S}\norm{\mathbf{y}-\mathbf{x}}
        \end{equation*}
        if and only if $\mathbf{z} \in S$ and $\inprod{\mathbf{z}-\mathbf{y}}{\mathbf{x}-\mathbf{z}}=0$ for all $\mathbf{x} \in S$.
    \end{thm}
    \begin{proofing}
        \begin{itemize}
            \item[$\Longrightarrow$] Assume that $\mathbf{z}$ is a solution of the minimization equation. Then $\mathbf{z} \in S$. For all $\mathbf{x} \in S$ and $t \in \mathbb{R}$,
            \begin{align*}
                \inprod{\mathbf{a}}{\mathbf{z}+t(\mathbf{x}-\mathbf{z})} &= \inprod{\mathbf{a}}{\mathbf{z}} + t(\inprod{\mathbf{a}}{\mathbf{x}} - \inprod{\mathbf{a}}{\mathbf{z}}),\\
                &= b + t(b-b),\\
                &= b.
            \end{align*}
            Therefore, $\mathbf{z} + t(\mathbf{x}-\mathbf{z}) \in S$. We have:
            \begin{align*}
                \norm{\mathbf{z}-\mathbf{y}}^{2} &\leq \norm{\mathbf{z}+t(\mathbf{x}-\mathbf{z})-\mathbf{y}}^{2} = \norm{\mathbf{z}-\mathbf{y}}^{2} + 2t\inprod{\mathbf{z}-\mathbf{y}}{\mathbf{x}-\mathbf{z}} + t^{2}\norm{\mathbf{x}-\mathbf{z}}^{2},\\
                -t^{2}\norm{\mathbf{x}-\mathbf{z}}^{2} &\leq 2t\inprod{\mathbf{z}-\mathbf{y}}{\mathbf{x}-\mathbf{z}}.
            \end{align*}
            Assume that $t>0$. As $t \to 0^{+}$,
            \begin{equation*}
                \inprod{\mathbf{z}-\mathbf{y}}{\mathbf{x}-\mathbf{z}} \geq -\lim_{t \to 0^{+}}\frac{t}{2}\norm{\mathbf{x}-\mathbf{z}}^{2} = 0.
            \end{equation*}
            Assume that $t<0$. As $t \to 0^{-}$,
            \begin{equation*}
                \inprod{\mathbf{z}-\mathbf{y}}{\mathbf{x}-\mathbf{z}} \leq -\lim_{t \to 0^{-}}\frac{t}{2}\norm{\mathbf{x}-\mathbf{z}}^{2} = 0.
            \end{equation*}
            Therefore, $\inprod{\mathbf{z}-\mathbf{y}}{\mathbf{x}-\mathbf{z}} = 0$ for all $\mathbf{x} \in S$.
            \item[$\Longleftarrow$] Assume that $\mathbf{z} \in S$ and $\inprod{\mathbf{z}-\mathbf{y}}{\mathbf{x}-\mathbf{z}} = 0$ for all $\mathbf{x} \in S$. This means that:
            \begin{align*}
                \norm{\mathbf{x}-\mathbf{y}}^{2} &= \norm{(\mathbf{x}-\mathbf{z}) + (\mathbf{z}-\mathbf{y})}^{2},\\
                &= \norm{\mathbf{x}-\mathbf{z}}^{2} + 2\inprod{\mathbf{x}-\mathbf{z}}{\mathbf{z}-\mathbf{y}} + \norm{\mathbf{z}-\mathbf{y}}^{2},\\
                &= \norm{\mathbf{x}-\mathbf{z}}^{2} + \norm{\mathbf{z}-\mathbf{y}}^{2},\\
                &\geq \norm{\mathbf{z}-\mathbf{y}}^{2}.
            \end{align*}
            Therefore, we can find that:
            \begin{equation*}
                \mathbf{z} = \argmin_{\mathbf{x} \in S}\norm{\mathbf{x}-\mathbf{y}}^{2}.
            \end{equation*}
        \end{itemize}
    \end{proofing}
    \newpage

    Using the last theorem, we can obtain the explicit form of the projection.
    \begin{thm}
        Let $H$ be a Hilbert space and $S$ be the hyperplane defined by:
        \begin{equation*}
            S = \{\mathbf{x}\in H:\inprod{\mathbf{a}}{\mathbf{x}} = b\},
        \end{equation*}
        where $\mathbf{a} \in H$ and $b \in \mathbb{R}$. Given $\mathbf{y} \in H$, we have a unique solution:
        \begin{equation*}
            P_{S}\mathbf{y} = \argmin_{\mathbf{x} \in S}\norm{\mathbf{y}-\mathbf{x}} = \mathbf{y} - \left(\frac{\inprod{\mathbf{a}}{\mathbf{y}}-b}{\norm{\mathbf{a}}^{2}}\right)\mathbf{a}.
        \end{equation*}
    \end{thm}
    \begin{proofing}
        Let $\mathbf{z} = \mathbf{y} - \left(\frac{\inprod{\mathbf{a}}{\mathbf{y}}-b}{\norm{\mathbf{a}}^{2}}\right)\mathbf{a}$.
        \begin{align*}
            \inprod{\mathbf{a}}{\mathbf{z}} &= \inprod{\mathbf{a}}{\mathbf{y}} - \frac{\inprod{\mathbf{a}}{\mathbf{y}}-b}{\norm{\mathbf{a}}^{2}}\inprod{\mathbf{a}}{\mathbf{a}},\\
            &= \inprod{\mathbf{a}}{\mathbf{y}} - (\inprod{\mathbf{a}}{\mathbf{y}} - b),\\
            &= b.
        \end{align*}
        Therefore, $\mathbf{z} \in S$. For any $\mathbf{x} \in S$,
        \begin{align*}
            \inprod{\mathbf{z}-\mathbf{y}}{\mathbf{x}-\mathbf{z}} &= \inprod{-\frac{\inprod{\mathbf{a}}{\mathbf{y}}-b}{\norm{\mathbf{a}}^{2}}\mathbf{a}}{\mathbf{x}-\mathbf{z}},\\
            &= -\frac{\inprod{\mathbf{a}}{\mathbf{y}}-b}{\norm{\mathbf{a}}^{2}}(\inprod{\mathbf{a}}{\mathbf{x}} - \inprod{\mathbf{a}}{\mathbf{z}}),\\
            &= -\frac{\inprod{\mathbf{a}}{\mathbf{y}}-b}{\norm{\mathbf{a}}^{2}}(b-b),\\
            &= 0.
        \end{align*}
        Therefore, by Theorem \ref{Chapter 4 (Theorem) Criteria for Projection Vector}, $\mathbf{z}$ is a solution to:
        \begin{equation*}
            \min_{\mathbf{x} \in S}\norm{\mathbf{y}-\mathbf{x}}.
        \end{equation*}
        We prove that this solution is unique.
        
        Suppose that it has two distinct solutions $\mathbf{z}_{1}$ and $\mathbf{z}_{2}$. Then it implies that $\mathbf{z}_{1}, \mathbf{z}_{2} \in S$. By Theorem \ref{Chapter 4 (Theorem) Criteria for Projection Vector},
        \begin{align*}
            \inprod{\mathbf{z}_{1}-\mathbf{y}}{\mathbf{z}_{2}-\mathbf{z}_{1}} &= 0, & \inprod{\mathbf{z}_{2}-\mathbf{y}}{\mathbf{z}_{1}-\mathbf{z}_{2}} &= \inprod{\mathbf{y}-\mathbf{z}_{2}}{\mathbf{z}_{2}-\mathbf{z}_{1}} = 0.
        \end{align*}
        Adding the two identities, we have:
        \begin{align*}
            \inprod{\mathbf{z}_{1}-\mathbf{y}}{\mathbf{z}_{2}-\mathbf{z}_{1}} + \inprod{\mathbf{y}-\mathbf{z}_{2}}{\mathbf{z}_{2}-\mathbf{z}_{1}} = \inprod{\mathbf{z}_{1}-\mathbf{z}_{2}}{\mathbf{z}_{2}-\mathbf{z}_{1}} = 0 &\Longleftrightarrow -\norm{\mathbf{z}_{1}-\mathbf{z}_{2}}^{2} = 0,\\
            &\Longleftrightarrow \mathbf{z}_{1} = \mathbf{z}_{2}.
        \end{align*}
        This results in a contradiction. Therefore, the solution $\mathbf{z}$ is unique.
    \end{proofing}
	\newpage

\section{Linear approximation and Differentiation}
    Recall that for a function $f:\mathbb{R} \to \mathbb{R}$, its derivative at $x$ is defined as:
    \begin{equation*}
        f'(x) = \lim_{y \to x}\frac{f(y) - f(x)}{y - x}.
    \end{equation*}
    This is equivalent to:
    \begin{equation*}
        \lim_{y \to x}\frac{\abs{f(y) - (f(x) + f'(x)(y-x))}}{\abs{y-x}} = 0.
    \end{equation*}
    Let $g(y) = f(x) + f'(x)(y-x)$. This function satisfies the following properties:
    \begin{enumerate}
        \item It is affine.
        \item It passes through $(x, f(x))$.
        \item 
        \begin{equation*}
            \lim_{y \to x}\frac{\abs{f(y) - g(y)}}{\abs{y-x}} = 0,
        \end{equation*}
        i.e., the error $f(y) - g(y) = o(\abs{y-x})$.
    \end{enumerate}
    We can extend this to functions $f: H \to \mathbb{R}$, where $H$ is a Hilbert space. Consider $\mathbf{x} \in H$. We want to find a function $g: H \to \mathbb{R}$ such that:
    \begin{enumerate}
        \item It is affine and bounded.
        \item It passes through $(\mathbf{x}, f(\mathbf{x}))$.
        \item 
        \begin{equation*}
            \lim_{\mathbf{y} \to \mathbf{x}} \frac{\abs{f(\mathbf{y}) - g(\mathbf{y})}}{\norm{\mathbf{y}-\mathbf{x}}} = 0.
        \end{equation*}
    \end{enumerate}
    By Theorem \ref{Chapter 4 (Theorem) Properties of affine functions}(2), let $g(\mathbf{y}) = \inprod{\mathbf{v}}{\mathbf{y}} + b$, where $\mathbf{v} \in H$ and $b \in \mathbb{R}$. We may define $\mathbf{v}$ and $b$ such that:
    \begin{equation*}
        g(\mathbf{y}) = \inprod{\mathbf{v}}{\mathbf{x}} + b = f(\mathbf{x}).
    \end{equation*}
    Therefore, the error limit becomes:
    \begin{equation*}
        \lim_{\mathbf{y} \to \mathbf{x}} \frac{\abs{f(\mathbf{y})-(f(\mathbf{x})+\inprod{\mathbf{v}}{\mathbf{y}-\mathbf{x}})}}{\norm{\mathbf{y}-\mathbf{x}}} = 0.
    \end{equation*}
    We have obtained the following definition.
    \begin{defn}
        Let $H$ be a Hilbert space. Let $f: H \to \mathbb{R}$ be a function and $\mathbf{x} \in H$. Then, $f$ is said to be \textbf{Fr\'echet differentiable} at $\mathbf{x}$ if there exists $\mathbf{v} \in H$ such that:
        \begin{equation*}
            \lim_{\mathbf{y} \to \mathbf{x}} \frac{\abs{f(\mathbf{y})-(f(\mathbf{x})
            +\inprod{\mathbf{v}}{\mathbf{y}-\mathbf{x}})}}{\norm{\mathbf{y}-\mathbf{x}}} = 0.
        \end{equation*}
        If $f$ is differentiable at $\mathbf{x}$, then $\mathbf{v}$ is called the gradient at $\mathbf{x}$, denoted by $\nabla f(\mathbf{x})$.
    \end{defn}
    \begin{rem}
        With the definition of convergence for vectors, we can rewrite the formula as:
        \begin{equation*}
            \lim_{\norm{\mathbf{y}-\mathbf{x}} \to 0} \frac{\abs{f(\mathbf{y})-(f(\mathbf{x})
                    +\inprod{\mathbf{v}}{\mathbf{y}-\mathbf{x}})}}{\norm{\mathbf{y}-\mathbf{x}}} = 0.
        \end{equation*}
    \end{rem}
    \begin{rem}
        Fr\'echet differentiation depends on the inner product on $H$.
    \end{rem}
    \newpage

    \begin{eg}
        Consider $f(\mathbf{x}) = \norm{\mathbf{x}}^{2}$, where $\norm{\cdot}$ is the norm on $H$. At $\mathbf{x} \in H$, for any $\mathbf{y} \in H$,
        \begin{align*}
            f(\mathbf{y}) = \norm{\mathbf{y}}^{2} &= \norm{\mathbf{x} + (\mathbf{y}-\mathbf{x})}^{2}\\
            &= \norm{\mathbf{x}}^{2} + 2\inprod{\mathbf{x}}{\mathbf{y}-\mathbf{x}} + \norm{\mathbf{y}-\mathbf{x}}^{2}\\
            &= f(\mathbf{x}) + \inprod{2\mathbf{x}}{\mathbf{y}-\mathbf{x}} + \norm{\mathbf{y}-\mathbf{x}}^{2}.
        \end{align*}
        We find that:
        \begin{equation*}
            \lim_{\norm{\mathbf{y}-\mathbf{x}} \to 0} \frac{\abs{f(\mathbf{y}) - (f(\mathbf{x}) + \inprod{2\mathbf{x}}{\mathbf{y}-\mathbf{x}})}}{\norm{\mathbf{y}-\mathbf{x}}} = \lim_{\norm{\mathbf{y}-\mathbf{x}} \to 0} \frac{\norm{\mathbf{y}-\mathbf{x}}^{2}}{\norm{\mathbf{y}-\mathbf{x}}} = 0.
        \end{equation*}
        Therefore, $\nabla f(\mathbf{x}) = 2\mathbf{x}$.
    \end{eg}
    \begin{eg}
        Consider $f(\mathbf{x})=\inprod{\mathbf{a}}{\mathbf{x}}$ for some $\mathbf{a} \in H$. At $\mathbf{x} \in H$, for any $\mathbf{y} \in H$,
        \begin{align*}
            f(\mathbf{y}) = \inprod{\mathbf{a}}{\mathbf{y}} &= \inprod{\mathbf{a}}{\mathbf{x}} + \inprod{\mathbf{a}}{\mathbf{y}-\mathbf{x}}\\
            &= f(\mathbf{x}) + \inprod{\mathbf{a}}{\mathbf{y}-\mathbf{x}} + 0.
        \end{align*}
        We find that:
        \begin{equation*}
            \lim_{\norm{\mathbf{y}-\mathbf{x}} \to 0} \frac{\abs{f(\mathbf{y})-(f(\mathbf{x})+\inprod{\mathbf{a}}{\mathbf{y}-\mathbf{x}})}}{\norm{\mathbf{y}-\mathbf{x}}} = 0.
        \end{equation*}
        Therefore, $\nabla f(\mathbf{x}) = \mathbf{a}$.
    \end{eg}
    \begin{eg}
        Consider $f(\mathbf{x}) = \norm{\mathbf{x}-\mathbf{a}}^{2}$, where $\norm{\cdot}$ is the norm on $H$ and $\mathbf{a} \in H$. At $\mathbf{x} \in H$, for any $\mathbf{y} \in H$,
        \begin{align*}
            f(\mathbf{y}) &= \norm{\mathbf{y}-\mathbf{a}}^{2}\\
            &= \norm{\mathbf{x}-\mathbf{a}+\mathbf{y}-\mathbf{x}}^{2}\\
            &= \norm{\mathbf{x}-\mathbf{a}}^{2} + 2\inprod{\mathbf{x}-\mathbf{a}}{\mathbf{y}-\mathbf{x}} + \norm{\mathbf{y}-\mathbf{x}}^{2}\\
            &= f(\mathbf{x}) + \inprod{2(\mathbf{x}-\mathbf{a})}{\mathbf{y}-\mathbf{x}} + \norm{\mathbf{y}-\mathbf{x}}^{2}.
        \end{align*}
        We find that:
        \begin{equation*}
            \lim_{\norm{\mathbf{y}-\mathbf{x}} \to 0} \frac{\abs{f(\mathbf{y}) - (f(\mathbf{x}) + \inprod{2(\mathbf{x}-\mathbf{a})}{\mathbf{y}-\mathbf{x}})}}{\norm{\mathbf{y}-\mathbf{x}}} = \lim_{\norm{\mathbf{y}-\mathbf{x}} \to 0} \frac{\norm{\mathbf{y}-\mathbf{x}}^{2}}{\norm{\mathbf{y}-\mathbf{x}}} = 0.
        \end{equation*}
        Therefore, $\nabla f(\mathbf{x}) = 2(\mathbf{x}-\mathbf{a})$.
    \end{eg}
    There are some properties of Fr\'echet differentiation.
    \begin{thm}
        \label{Chapter 4 (Theorem) Properties of Frechet differentiation}
        Fr\'echet differentiation has the following properties:
        \begin{enumerate}
            \item Fr\'echet differentiation is linear in $f$, i.e., for any $\alpha,\beta \in \mathbb{R}$ and $f,g: H \to \mathbb{R}$, 
            \begin{equation*}
                \nabla(\alpha f + \beta g)(\mathbf{x}) = \alpha \nabla f(\mathbf{x}) + \beta \nabla g(\mathbf{x}), \qquad \text{for } \mathbf{x} \in H,
            \end{equation*}
            provided that $\nabla f(\mathbf{x})$ and $\nabla g(\mathbf{x})$ exist.
            \item \named{Chain rule} Let $f: H \to \mathbb{R}$ and $g: \mathbb{R} \to \mathbb{R}$. Then $g \circ f: H \to \mathbb{R}$ and:
            \begin{equation*}
                \nabla(g \circ f)(\mathbf{x}) = g'(f(\mathbf{x})) \cdot \nabla f(\mathbf{x}), \qquad \text{for } \mathbf{x} \in H,
            \end{equation*}
            provided that $g$ and $f$ are differentiable at $f(\mathbf{x})$ and $\mathbf{x}$, respectively.
        \end{enumerate}
    \end{thm}
    \newpage

    \begin{proofing}
        \begin{enumerate}
            \item By definition, for any $\mathbf{x} \in  H$,
            \begin{align*}
                \lim_{\norm{\mathbf{y}-\mathbf{x}} \to 0} \frac{\abs{f(\mathbf{y})-(f(\mathbf{x}) + \inprod{\nabla f(\mathbf{x})}{\mathbf{y}-\mathbf{x}})}}{\norm{\mathbf{y}-\mathbf{x}}} &= 0, & \lim_{\norm{\mathbf{y}-\mathbf{x}} \to 0} \frac{\abs{g(\mathbf{y})-(g(\mathbf{x}) + \inprod{\nabla g(\mathbf{x})}{\mathbf{y}-\mathbf{x}})}}{\norm{\mathbf{y}-\mathbf{x}}} &= 0.
            \end{align*}
            We check if $\alpha \nabla f(\mathbf{x}) + \beta \nabla g(\mathbf{x})$ is the gradient of $\alpha f + \beta g$ at $\mathbf{x}$.
            \begin{align*}
                0 &\leq \lim_{\norm{\mathbf{y}-\mathbf{x}} \to 0} \frac{\abs{(\alpha f + \beta g)(\mathbf{y})-((\alpha f + \beta g)(\mathbf{x}) + \inprod{\alpha \nabla f(\mathbf{x}) + \beta \nabla g(\mathbf{x})}{\mathbf{y}-\mathbf{x}})}}{\norm{\mathbf{y}-\mathbf{x}}}\\
                &\leq \lim_{\norm{\mathbf{y}-\mathbf{x}} \to 0} \frac{\abs{\alpha (f(\mathbf{y}) - (f(\mathbf{x}) + \inprod{\nabla f(\mathbf{x})}{\mathbf{y}-\mathbf{x}})) + \beta (g(\mathbf{y}) - (g(\mathbf{x}) + \inprod{\nabla g(\mathbf{x})}{\mathbf{y}-\mathbf{x}}))}}{\norm{\mathbf{y}-\mathbf{x}}}\\
                \tag{Triangle Inequality}
                &\leq \lim_{\norm{\mathbf{y}-\mathbf{x}} \to 0} \left(\abs{\alpha} \frac{\abs{f(\mathbf{y}) - (f(\mathbf{x}) + \inprod{\nabla f(\mathbf{x})}{\mathbf{y}-\mathbf{x}})}}{\norm{\mathbf{y}-\mathbf{x}}} + \abs{\beta} \frac{\abs{g(\mathbf{y}) - (g(\mathbf{x}) + \inprod{\nabla g(\mathbf{x})}{\mathbf{y}-\mathbf{x}})}}{\norm{\mathbf{y}-\mathbf{x}}}\right) = 0.
            \end{align*}
            Therefore, $\nabla(\alpha f + \beta g)(\mathbf{x}) = \alpha \nabla f(\mathbf{x}) + \beta \nabla g(\mathbf{x})$.
            \item By definition, if $f$ is differentiable at $\mathbf{x} \in H$ and $g$ is differentiable at $t \in \mathbb{R}$,
            \begin{align*}
                \lim_{\norm{\mathbf{y}-\mathbf{x}} \to 0} \frac{\abs{f(\mathbf{y})-(f(\mathbf{x}) + \inprod{\nabla f(\mathbf{x})}{\mathbf{y}-\mathbf{x}})}}{\norm{\mathbf{y}-\mathbf{x}}} &= 0, & \lim_{s \to t} \frac{g(s) - (g(t) + g'(t)(s-t))}{\abs{s-t}} &= 0.
            \end{align*}
            We check if $g'(f(\mathbf{x}))\cdot\nabla f(\mathbf{x})$ is the gradient of $g \circ f$ at $\mathbf{x}$.
            \begin{align*}
                0 &\leq \lim_{\norm{\mathbf{y}-\mathbf{x}} \to 0} \frac{\abs{g(f(\mathbf{y})) - (g(f(\mathbf{x})) + \inprod{g'(f(\mathbf{x})) \cdot \nabla f(\mathbf{x})}{\mathbf{y} - \mathbf{x}})}}{\norm{\mathbf{y}-\mathbf{x}}}\\
                &\leq \lim_{\norm{\mathbf{y}-\mathbf{x}} \to 0} \frac{\abs{g(f(\mathbf{y})) - (g(f(\mathbf{x})) + g'(f(\mathbf{x}))(f(\mathbf{y})-f(\mathbf{x})) - g'(f(\mathbf{x}))(f(\mathbf{y})-f(\mathbf{x})) + \inprod{g'(f(\mathbf{x})) \cdot \nabla f(\mathbf{x})}{\mathbf{y} - \mathbf{x}})}}{\norm{\mathbf{y}-\mathbf{x}}}\\
                \tag{Triangle Inequality}
                &\leq \lim_{\norm{\mathbf{y}-\mathbf{x}} \to 0} \left(\frac{\abs{g(f(\mathbf{y})) - (g(f(\mathbf{x})) + g'(f(\mathbf{x}))(f(\mathbf{y})-f(\mathbf{x}))}}{\norm{\mathbf{y}-\mathbf{x}}} + \abs{g'(f(\mathbf{x}))}\frac{\abs{f(\mathbf{y})-(f(\mathbf{x}) + \inprod{\nabla f(\mathbf{x})}{\mathbf{y} - \mathbf{x}})}}{\norm{\mathbf{y}-\mathbf{x}}}\right)\\
                &\leq \lim_{\norm{\mathbf{y}-\mathbf{x}} \to 0} \frac{\abs{g(f(\mathbf{y})) - (g(f(\mathbf{x})) + g'(f(\mathbf{x}))(f(\mathbf{y})-f(\mathbf{x}))}}{\norm{\mathbf{y}-\mathbf{x}}}\\
                &\leq \lim_{\norm{\mathbf{y}-\mathbf{x}} \to 0} \frac{\abs{g(f(\mathbf{y})) - (g(f(\mathbf{x})) + g'(f(\mathbf{x}))(f(\mathbf{y})-f(\mathbf{x}))}}{\abs{f(\mathbf{y})-f(\mathbf{x})}}\left(\frac{\abs{f(\mathbf{y})-f(\mathbf{x})}}{\norm{\mathbf{y}-\mathbf{x}}}\right).
            \end{align*}
            Now the only thing left to prove is that $f(\mathbf{y}) \to f(\mathbf{x})$ when $\norm{\mathbf{y} - \mathbf{x}} \to 0$. Recall that:
            \begin{equation*}
                f(\mathbf{y}) - (f(\mathbf{x}) + \inprod{\nabla f(\mathbf{x})}{\mathbf{y}-\mathbf{x}}) = o(\norm{\mathbf{y} - \mathbf{x}}).
            \end{equation*}
            Rearrange the terms:
            \begin{align*}
                0 \leq \abs{f(\mathbf{y}) - f(\mathbf{x})} &= \abs{\inprod{\nabla f(\mathbf{x})}{\mathbf{y}-\mathbf{x}} + o(\norm{\mathbf{y}-\mathbf{x}})}\\
                \tag{Triangle Inequality}
                &\leq \abs{\inprod{\nabla f(\mathbf{x})}{\mathbf{y}-\mathbf{x}}} + o(\norm{\mathbf{y}-\mathbf{x}})\\
                \tag{Cauchy-Schwarz Inequality}
                &\leq \norm{\nabla f(\mathbf{x})}\norm{\mathbf{y}-\mathbf{x}} + o(\norm{\mathbf{y}-\mathbf{x}})\\
                &\leq \norm{\nabla f(\mathbf{x})}\norm{\mathbf{y}-\mathbf{x}} + c\norm{\mathbf{y}-\mathbf{x}}, \qquad \text{for some } c \in \mathbb{R}\\
                0 \leq \frac{\abs{f(\mathbf{y}) - f(\mathbf{x})}}{\norm{\mathbf{y}-\mathbf{x}}} &\leq \norm{\nabla f(\mathbf{x})} + c.
            \end{align*}
            This means as $\norm{\mathbf{y}-\mathbf{x}} \to 0$, $f(\mathbf{y})$ has to converge to $f(\mathbf{x})$. By applying $s = f(\mathbf{y})$ and $t = f(\mathbf{x})$,
            \begin{equation*}
                \lim_{\norm{\mathbf{y}-\mathbf{x}} \to 0} \frac{\abs{g(f(\mathbf{y})) - (g(f(\mathbf{x})) + g'(f(\mathbf{x}))(f(\mathbf{y})-f(\mathbf{x}))}}{\abs{f(\mathbf{y})-f(\mathbf{x})}} = 0.
            \end{equation*}
            Therefore, $\nabla(g \circ f)(\mathbf{x}) = g'(f(\mathbf{x})) \cdot \nabla f(\mathbf{x})$.
        \end{enumerate}
    \end{proofing}
    \newpage

    \begin{eg}
        Consider $f(\mathbf{x})=\norm{\mathbf{x}}$, where $\norm{\cdot}$ is the norm on $H$. Find its derivative for any $\mathbf{x} \in H$. Let:
        \begin{align*}
            f_{1}(\mathbf{x}) &= \norm{\mathbf{x}}^{2}, \qquad \text{for } \mathbf{x} \in H, & f_{2}(t) &= \sqrt{t}, \qquad \text{for } t \in \mathbb{R}_{\geq 0}, & f(\mathbf{x}) = f_{2}(f_{1}(\mathbf{x})).
        \end{align*}
        When $\mathbf{x} \neq 0$, both $f_{1}$ and $f_{2}$ are differentiable. By the chain rule,
        \begin{equation*}
            \nabla f(\mathbf{x}) = f_{2}'(f_{1}(\mathbf{x})) \cdot \nabla f_{1}(\mathbf{x}) = \frac{1}{2\sqrt{\norm{\mathbf{x}}^{2}}}(2\mathbf{x}) = \frac{\mathbf{x}}{\norm{\mathbf{x}}}.
        \end{equation*}
        When $\mathbf{x} = \mathbf{0}$, $f_{1}(\mathbf{0}) = 0$. $f_{2}$ is not differentiable at $f_{1}(\mathbf{0})$. We cannot apply the chain rule.
        
        In fact, $f$ is not differentiable at $\mathbf{x} = \mathbf{0}$.
    \end{eg}
    We can find Fr\'echet differentiation element-wise if the Hilbert space we are considering is $\mathbb{R}^{n}$.
    \begin{thm}
        \label{Chapter 4 (Theorem) Elementwise Gradient}
        For functions $f:\mathbb{R}^{n} \to \mathbb{R}$, where $\mathbb{R}^{n}$ is with the standard inner product, if $f$ is differentiable at $\mathbf{x} \in \mathbb{R}^{n}$, then:
        \begin{equation*}
            \nabla f(\mathbf{x}) = \begin{pmatrix}
                \pdv*{f(\mathbf{x})}{x_{1}}\\
                \vdots\\
                \pdv*{f(\mathbf{x})}{x_{n}}
            \end{pmatrix}.
        \end{equation*}
    \end{thm}
    \begin{proofing}
        Since $f$ is differentiable at $\mathbf{x}$,
        \begin{equation*}
            \lim_{\mathbf{y} \to \mathbf{x}} \frac{\abs{f(\mathbf{y})-(f(\mathbf{x})+\inprod{\nabla f(\mathbf{x})}{\mathbf{y}-\mathbf{x}})}}{\norm{\mathbf{y}-\mathbf{x}}} = 0.
        \end{equation*}
        Choose $\mathbf{y} = \mathbf{x} + t\mathbf{e}_{i}$ where $t \in \mathbb{R}$ and let $g(t) = f(\mathbf{x}+t\mathbf{e}_{i})$. We have:
        \begin{align*}
            \lim_{t \to 0} \frac{\abs{f(\mathbf{x}+t\mathbf{e}_{i})-(f(\mathbf{x})+t\inprod{\nabla f(\mathbf{x})}{\mathbf{e}_{i}})}}{\abs{t}} &= 0,\\
            \lim_{t \to 0} \frac{\abs{g(t)-(g(0)+\inprod{\nabla f(\mathbf{x})}{\mathbf{e}_{i}}(t-0))}}{\abs{t-0}} &= 0.
        \end{align*}
        Therefore, by definition, $g'(0) = \inprod{\nabla f(\mathbf{x})}{\mathbf{e}_{i}}$. Moreover,
        \begin{equation*}
            \inprod{\nabla f(\mathbf{x})}{\mathbf{e}_{i}} = \left.\odv*{g(t)}{t}\right|_{t=0} = \left.\odv*{f(\mathbf{x}+t\mathbf{e}_{i})}{t}\right|_{t=0} = \pdv*{f(\mathbf{x})}{x_{i}}.
        \end{equation*}
        Thus,
        \begin{equation*}
            \nabla f(\mathbf{x}) = \begin{pmatrix}
                \pdv*{f(\mathbf{x})}{x_{1}}\\
                \vdots\\
                \pdv*{f(\mathbf{x})}{x_{n}}
            \end{pmatrix}.
        \end{equation*}
    \end{proofing}
    \begin{rem}
        Fr\'echet differentiation is consistent with the standard differentiation in multi-variable calculus if $\mathbb{R}^{n}$ is with the standard inner product.
    \end{rem}
    \newpage

    Based on the last theorem, we used $\mathbf{y} = \mathbf{x}+t\mathbf{e}_{i}$. Can we extend $\mathbf{e}_{i}$ to any vector?
    \begin{lem}
    	\label{Chapter 4 (Lemma) Derivative on t=0}
        Let $f:H \to \mathbb{R}$. Assume that $f$ is differentiable at $\mathbf{x} \in H$. For any $\mathbf{u} \in H$,
        \begin{equation*}
            \left.\odv*{f(\mathbf{x} + t\mathbf{u})}{t}\right|_{t=0} = \inprod{\nabla f(\mathbf{x})}{\mathbf{u}}.
        \end{equation*}
    \end{lem}
    \begin{proofing}
        The proof is similar to the last theorem. Since $f$ is differentiable at $\mathbf{x}$,
        \begin{equation*}
            \lim_{\mathbf{y} \to \mathbf{x}} \frac{\abs{f(\mathbf{y})-(f(\mathbf{x})+\inprod{\nabla f(\mathbf{x})}{\mathbf{y}-\mathbf{x}})}}{\norm{\mathbf{y}-\mathbf{x}}} = 0.
        \end{equation*}
        Choose $\mathbf{y} = \mathbf{x} + t\mathbf{u}$ where $t \in \mathbb{R}$ and let $g(t) = f(\mathbf{x}+t\mathbf{u})$. We have:
        \begin{align*}
            \lim_{t \to 0} \frac{\abs{f(\mathbf{x}+t\mathbf{u})-(f(\mathbf{x})+t\inprod{\nabla f(\mathbf{x})}{\mathbf{u}})}}{\abs{t}} &= 0,\\
            \lim_{t \to 0} \frac{\abs{g(t)-(g(0)+\inprod{\nabla f(\mathbf{x})}{\mathbf{u}}(t-0))}}{\abs{t-0}} &= 0.
        \end{align*}
        Therefore, by definition, $g'(0) = \inprod{\nabla f(\mathbf{x})}{\mathbf{u}}$. Moreover,
        \begin{equation*}
            \inprod{\nabla f(\mathbf{x})}{\mathbf{u}} = \left.\odv*{g(t)}{t}\right|_{t=0} = \left.\odv*{f(\mathbf{x}+t\mathbf{u})}{t}\right|_{t=0}.
        \end{equation*}
    \end{proofing}
    \begin{rem}
        If $\norm{\mathbf{u}} = 1$, then $\inprod{\nabla f(\mathbf{x})}{\mathbf{u}}$ is the \textbf{directional derivative} along the $\mathbf{u}$ direction.
    \end{rem}
    We can generalize this to all $t$.
    \begin{thm}
        \label{Chapter 4 (Theorem) Derivative on not t=0}
        Let $f: H \to \mathbb{R}$. For any $\mathbf{x}, \mathbf{u} \in H$ and $t \in \mathbb{R}$,
        \begin{equation*}
            \odv*{f(\mathbf{x} + t\mathbf{u})}{t} = \inprod{\nabla f(\mathbf{x} + t\mathbf{u})}{\mathbf{u}},
        \end{equation*}
        if $f$ is differentiable at $\mathbf{x} + t\mathbf{u}$.
    \end{thm}
    \begin{proofing}
        Let $g(t) = f(\mathbf{x} + t\mathbf{u})$. Fix an arbitrary $t_{0}$, we have:
        \begin{equation*}
            g(t_{0} + t) = f(\mathbf{x} + t_{0}\mathbf{u} + t\mathbf{u}).
        \end{equation*} 
        From Lemma \ref{Chapter 4 (Lemma) Derivative on t=0}, if $f$ is differentiable at $\mathbf{z} \in H$, then:
        \begin{equation*}
            \odv*{g'(t_{0})}{t} = \left.\odv*{f(\mathbf{x} + t_{0}\mathbf{u} + t\mathbf{u})}{t}\right|_{t=0} = \inprod{\nabla f(\mathbf{x} + t_{0}\mathbf{u})}{\mathbf{u}}.
        \end{equation*}
        Therefore, since $t_{0}$ is arbitrary,
        \begin{equation*}
            \odv*{f(\mathbf{x} + t\mathbf{u})}{t} = \inprod{\nabla f(\mathbf{x} + t\mathbf{u})}{\mathbf{u}}.
        \end{equation*}
    \end{proofing}
    Recall the Taylor expansion for functions $f:\mathbb{R} \to \mathbb{R}$:
    \begin{equation*}
        f(y) = f(x) + f'(x)(y-x) + o((y-x)^{k}), \qquad \text{for } y \in \mathbb{R}.
    \end{equation*}
    Similarly, we can extend it to $f: H \to \mathbb{R}$ by Fr\'echet differentiation.
    \begin{thm}
        Assume that $f: H \to \mathbb{R}$ is differentiable at $\mathbf{x} \in H$. The \textbf{Taylor expansion} of $f$ is:
        \begin{equation*}
            f(\mathbf{y}) = f(\mathbf{x}) + \inprod{\nabla f(\mathbf{x})}{\mathbf{y}-\mathbf{x}} + o(\snorm{\mathbf{y}-\mathbf{x}}), \qquad \text{for } \mathbf{y} \in H.
        \end{equation*}
    \end{thm}
    \newpage

    Up to this point, we have only considered the Hilbert space $H$. What about functions on Banach spaces, which may not have an inner product? Let $f: V \to \mathbb{R}$, where $V$ is a Banach space. Let $\mathbf{x} \in V$. We want to find $g: V \to \mathbb{R}$ such that:
    \begin{enumerate}
        \item It is affine.
        \item $g(\mathbf{x}) = f(\mathbf{x})$.
        \item The error is $o(\norm{\mathbf{y}-\mathbf{x}})$.
    \end{enumerate}
    We cannot use the inner product. Instead, we let $g(\mathbf{x}) = L\mathbf{x} + \mathbf{a}$ for all $\mathbf{x} \in V$, where $L: V \to \mathbb{R}$ is linear and $\mathbf{a} \in \mathbb{R}$. Therefore,
    \begin{equation*}
        g(\mathbf{y}) = L \mathbf{y} + \mathbf{a} = L \mathbf{y} - L \mathbf{x} + L \mathbf{x} + \mathbf{a} = L (\mathbf{y}-\mathbf{x}) + f(\mathbf{x}).
    \end{equation*}
    The error limit becomes:
    \begin{equation*}
        \lim_{\snorm{\mathbf{y}-\mathbf{x}} \to 0} \frac{\abs{f(\mathbf{y}) - (f(\mathbf{x})+L (\mathbf{y}-\mathbf{x}))}}{\snorm{\mathbf{y}-\mathbf{x}}}.
    \end{equation*}
    We now have a more general definition for differentiation.
    \begin{defn}
        Let $V$ be a Banach space. Let $f: V \to \mathbb{R}$ be a function and $\mathbf{x} \in V$. Then, $f$ is said to be \textbf{(Fr\'echet) differentiable} at $\mathbf{x}$ if there exists a linear function $L: V \to \mathbb{R}$ such that:
        \begin{equation*}
            \lim_{\snorm{\mathbf{y}-\mathbf{x}} \to 0} \frac{\abs{f(\mathbf{y}) - (f(\mathbf{x})+L (\mathbf{y}-\mathbf{x}))}}{\snorm{\mathbf{y}-\mathbf{x}}} = 0.
        \end{equation*}
        The linear function $L$ is called the \textbf{differentiation} of $f$ at $\mathbf{x}$, defined by: 
        \begin{equation*}
            D f(\mathbf{x}) = L.
        \end{equation*}
    \end{defn} 
    We will continue discussing this differentiation in the next chapter. Read Appendix \ref{Case Study D: Linear Regression} (Linear Regression), \ref{Case Study E: Kernel Ridge Regression} (Kernel Ridge Regression), \ref{Case Study F: Linear Classification} (Linear Classification), \ref{Case Study G: Solvability and Optimality} (Solvability and Optimality), and \ref{Case Study H: Gradient Descent} (Gradient Descent) to see the case studies for Chapter \ref{Chapter 4: Linear Functions and Differentiation}.

\chapter{Linear Transformations or Linear Operators}
    \label{Chapter 5: Linear Transformations or Linear Operators}
    In the last chapter, we discussed linear functions $f: V \to \mathbb{R}$ in more detail. What about operators that map from one vector space to another? This chapter focuses on such operators, allowing us to generalize the differentiation of functions.

\section{Linear Transformations or Linear Operators}
    \begin{defn}
        Let $V, W$ be two vector spaces. A map $L: V \to W$ is a \textbf{linear transformation / linear operator} if, for all $\mathbf{x}, \mathbf{y} \in V$ and $\alpha, \beta \in \mathbb{R}$,
        \begin{equation*}
            L(\alpha \mathbf{x} + \beta \mathbf{y}) = \alpha L \mathbf{x} + \beta L \mathbf{y}.
        \end{equation*}
    \end{defn}
    \begin{rem}
        Parentheses are often omitted. For example, $L \mathbf{x}$ means $L(\mathbf{x})$.
    \end{rem}
    \begin{eg}
        Let $f: V \to \mathbb{R}$ be a linear function. Then $f$ is a linear transformation.
    \end{eg}
    \begin{eg}
        \label{Chapter 5 (Example) Linear Transformation to Matrix}
        Let $\mathbf{A} \in \mathbb{R}^{m \times n}$. Define $L: \mathbb{R}^{n} \to \mathbb{R}^{m}$ by:
        \begin{equation*}
            L \mathbf{x} = \mathbf{A}\mathbf{x}, \qquad \text{for } \mathbf{x} \in \mathbb{R}^{n}.
        \end{equation*}
        It is a linear transformation because, for $\mathbf{x}, \mathbf{y} \in \mathbb{R}^{n}$ and $\alpha, \beta \in \mathbb{R}$,
        \begin{equation*}
            \mathbf{A}(\alpha \mathbf{x} + \beta \mathbf{y}) = \alpha \mathbf{A}\mathbf{x} + \beta \mathbf{A}\mathbf{y}.
        \end{equation*}
        In fact, for any linear transformation $L: \mathbb{R}^{n} \to \mathbb{R}^{m}$, there exists $\mathbf{A} \in \mathbb{R}^{m \times n}$ such that:
        \begin{equation*}
            L \mathbf{x} = \mathbf{A}\mathbf{x}, \qquad \text{for } \mathbf{x} \in \mathbb{R}^{n}.
        \end{equation*}
    \end{eg}
    \begin{eg}
        Let $\mathbf{a}_{1}, \cdots, \mathbf{a}_{k} \in H$. Define $L: H \to \mathbb{R}^{k}$ by:
        \begin{equation*}
            L \mathbf{x} = \begin{pmatrix}
                \inprod{\mathbf{a}_{1}}{\mathbf{x}}\\
                \vdots\\
                \inprod{\mathbf{a}_{k}}{\mathbf{x}}
            \end{pmatrix} \in \mathbb{R}^{k}, \qquad \text{for } \mathbf{x} \in H.
        \end{equation*}
        Then $L$ is a linear transformation.
    \end{eg}
    \newpage

    \begin{eg}
        Let:
        \begin{align*}
            V &= \{f: f \text{ and } f' \text{ are continuous on } [a,b]\},\\ 
            W &= \{f: f \text{ is continuous on } [a,b]\}.
        \end{align*}
        Define $D: V \to W$ by:
        \begin{equation*}
            D f = f', \qquad \text{for } f \in V.
        \end{equation*}
        Then $D$ is a linear transformation.
    \end{eg}
    \begin{eg}
        Consider $\mathcal{C}[-1,1]$ and $\mathcal{C}[0,2]$. Define $T: \mathcal{C}[-1,1] \to \mathcal{C}[0,2]$ by:
        \begin{equation*}
            (T f)(t) = f(t-1), \qquad \text{for any } t \in [0,2], \qquad \text{for } f \in \mathcal{C}[-1,1].
        \end{equation*}
        Then $T$ is linear.
    \end{eg}
    We have the following properties:
    \begin{thm}
        Let $V, W$ be two non-trivial vector spaces. If $V$ and $W$ are finite-dimensional, then for any linear transformation $T: V \to W$, there exists a unique matrix $\mathbf{A}$ such that:
        \begin{equation*}
            T \mathbf{v} = \mathbf{A}\mathbf{v}, \qquad \text{for } \mathbf{v} \in V.
        \end{equation*}
    \end{thm}
    \begin{rem}
        Due to this fact, matrices are often used to represent linear transformations.
    \end{rem}

    \begin{defn}
        Let $V, W$ be two vector spaces, and let $\mathbf{A}, \mathbf{B}: V \to W$ be linear operators. We define $\mathbf{A}+\mathbf{B}: V \to W$ by:
        \begin{equation*}
            (\mathbf{A}+\mathbf{B})\mathbf{x} = \mathbf{A}\mathbf{x} + \mathbf{B}\mathbf{x}, \qquad \text{for } \mathbf{x} \in V.
        \end{equation*}
        For any $\alpha \in \mathbb{R}$, we define $\alpha \mathbf{A}: V \to W$ by:
        \begin{equation*}
            (\alpha \mathbf{A})\mathbf{x} = \alpha \mathbf{A}\mathbf{x}.
        \end{equation*}
    \end{defn}

    We may define some norms for the linear operators.
    \begin{thm}
        For any linear operator $\mathbf{A}: V \to W$,
        \begin{equation*}
            \norm{\mathbf{A}} = \sup_{\pnorm[V]{\mathbf{x}} = 1} \pnorm[W]{\mathbf{A}\mathbf{x}}.
        \end{equation*}
        This is a norm. If $\norm{\mathbf{A}} < +\infty$, then $\mathbf{A}$ is said to be bounded.
    \end{thm}
    \begin{proofing}
        \begin{enumerate}
            \item We can easily find that:
            \begin{equation*}
                \norm{\mathbf{A}} = \sup_{\pnorm[V]{\mathbf{x}} = 1} \pnorm[W]{\mathbf{A}\mathbf{x}} \geq 0.
            \end{equation*}
            Moreover,
            \begin{align*}
                \norm{\mathbf{A}} = 0 &\Longleftrightarrow \sup_{\pnorm[V]{\mathbf{x}} = 1} \pnorm[W]{\mathbf{A}\mathbf{x}} = 0\\
                &\Longleftrightarrow \pnorm[W]{\mathbf{A}\mathbf{x}} = 0, \qquad \text{for } \mathbf{x} \in V \text{ satisfying } \pnorm[V]{\mathbf{x}} = 1\\
                &\Longleftrightarrow \pnorm[W]{\mathbf{A}\frac{\mathbf{y}}{\pnorm[V]{\mathbf{y}}}} = 0 \Longleftrightarrow \mathbf{A}\mathbf{y} = \mathbf{0}, \qquad \text{for } \mathbf{y} \in V\\
                &\Longleftrightarrow \mathbf{A} = \mathbf{O}.
            \end{align*}
            \item For any $\mathbf{A}: V \to W$, $\alpha \in \mathbb{R}$,
            \begin{equation*}
                \norm{\alpha \mathbf{A}} = \sup_{\pnorm[V]{\mathbf{x}} = 1} \pnorm[W]{(\alpha \mathbf{A})\mathbf{x}} = \abs{\alpha} \sup_{\pnorm[V]{\mathbf{x}} = 1} \pnorm[W]{\mathbf{A}\mathbf{x}} = \abs{\alpha} \norm{\mathbf{A}}.
            \end{equation*}
            \item For any linear operators $\mathbf{A}, \mathbf{B}: V \to W$,
            \begin{equation*}
                \norm{\mathbf{A} + \mathbf{B}} = \sup_{\pnorm[V]{\mathbf{x}} = 1} \pnorm[W]{\mathbf{A}\mathbf{x} + \mathbf{B}\mathbf{x}} \leq \sup_{\pnorm[V]{\mathbf{x}} = 1} (\pnorm[W]{\mathbf{A}\mathbf{x}} + \pnorm[W]{\mathbf{B}\mathbf{x}}) \leq \sup_{\pnorm[V]{\mathbf{x}} = 1} \pnorm[W]{\mathbf{A}\mathbf{x}} + \sup_{\pnorm[V]{\mathbf{x}} = 1} \pnorm[W]{\mathbf{B}\mathbf{x}} = \norm{\mathbf{A}} + \norm{\mathbf{B}}.
            \end{equation*}
        \end{enumerate}
        Therefore, it is a norm.
    \end{proofing}
    \newpage

    In addition, it satisfies additional properties.
    \begin{thm}
        \label{Chapter 5 (Theorem) Consistent Matrix Norm}
        For any linear operator $\mathbf{A}: V \to W$,
        \begin{equation*}
            \pnorm[W]{\mathbf{A}\mathbf{x}} \leq \norm{\mathbf{A}}\pnorm[V]{\mathbf{x}}, \qquad \text{for } \mathbf{x} \in V.
        \end{equation*}
    \end{thm}
    \begin{proofing}
        If $\mathbf{x} = \mathbf{0}$, then:
        \begin{equation*}
            \pnorm[W]{\mathbf{A}\mathbf{x}} = 0 \leq 0 = \norm{\mathbf{A}}\pnorm[V]{\mathbf{x}}.
        \end{equation*}
        If $\mathbf{x} \neq 0$, then:
        \begin{equation*}
            \norm{\mathbf{A}} = \sup_{\pnorm[V]{\mathbf{z}} = 1} \pnorm[W]{\mathbf{A}\mathbf{z}} \geq \pnorm[W]{\mathbf{A}\frac{\mathbf{x}}{\pnorm[V]{\mathbf{x}}}} = \frac{1}{\pnorm[V]{\mathbf{x}}}\pnorm[W]{\mathbf{A}\mathbf{x}}.
        \end{equation*}
        After rearranging, we have $\norm{\mathbf{A}}\pnorm[V]{\mathbf{x}} \geq \pnorm[W]{\mathbf{A}\mathbf{x}}$.
    \end{proofing}
    \begin{thm}
        For any linear operators $\mathbf{A}: V_{2} \to V_{3}$ and $\mathbf{B}: V_{1} \to V_{2}$,
        \begin{equation*}
            \norm{\mathbf{A}\mathbf{B}} \leq \norm{\mathbf{A}}\norm{\mathbf{B}}.
        \end{equation*}
    \end{thm}
    \begin{proofing}
        By Theorem \ref{Chapter 5 (Theorem) Consistent Matrix Norm},
        \begin{equation*}
            \norm{\mathbf{A}\mathbf{B}} = \sup_{\pnorm[V_{1}]{\mathbf{x}} = 1} \pnorm[V_{3}]{\mathbf{A}\mathbf{B}\mathbf{x}} = \norm{\mathbf{A}}\sup_{\pnorm[V_{1}]{\mathbf{x}} = 1} \pnorm[V_{2}]{\mathbf{B}\mathbf{x}} = \norm{\mathbf{A}}\norm{\mathbf{B}}.
        \end{equation*}
    \end{proofing}
    \begin{eg}
        Consider $V = \mathbb{R}^{n}$ and $W = \mathbb{R}^{m}$. We endow them with the $2$-norm. For any $\mathbf{A} \in \mathbb{R}^{m \times n}$, we call the corresponding operator norm the \textbf{$2$-norm}, defined by:
        \begin{equation*}
            \pnorm[2]{\mathbf{A}} = \sup_{\pnorm[2]{\mathbf{x}} = 1} \pnorm[2]{\mathbf{A}\mathbf{x}} = \sqrt{\sup_{\pnorm[2]{\mathbf{x}} = 1} \pnorm[2]{\mathbf{A}\mathbf{x}}^{2}} = \sqrt{\sup_{\mathbf{x}^{T}\mathbf{x} = 1} \mathbf{x}^{T}\mathbf{A}^{T}\mathbf{A}\mathbf{x}} = \sqrt{\text{Max eigenvalue of } \mathbf{A}^{T}\mathbf{A}}.
        \end{equation*}
    \end{eg}
    \begin{eg}
        Consider $\mathbf{A} \in \mathbb{R}^{m \times n}$ as a linear operator from $\mathbb{R}^{n}$ to $\mathbb{R}^{m}$. Let $\mathbb{R}^{n}$ and $\mathbb{R}^{m}$ be endowed with $1$-norms. We call the resulting operator norm the \textbf{$1$-norm}, defined by:
        \begin{equation*}
        \pnorm[1]{\mathbf{A}} = \sup_{\pnorm[1]{\mathbf{x}} = 1} \pnorm[1]{\mathbf{A}\mathbf{x}}.
        \end{equation*}
    \end{eg}
    \begin{thm}
        Let $\mathbf{A} = (\mathbf{a}_{1}, \cdots, \mathbf{a}_{n}) \in \mathbb{R}^{m \times n}$. Then:
        \begin{equation*}
            \pnorm[1]{\mathbf{A}} = \max_{1 \leq i \leq n} \pnorm[1]{\mathbf{a}_{i}}.
        \end{equation*}
    \end{thm}
    \begin{proofing}
        For any $\mathbf{x} \in \mathbb{R}^{n}$ satisfying $\pnorm[1]{\mathbf{x}} = 1$,
        \begin{equation*}
            \pnorm[1]{\mathbf{A}\mathbf{x}} = \pnorm[1]{\sum_{i=1}^{n}x_{i}\mathbf{a}_{i}} \leq \sum_{i=1}^{n}\pnorm[1]{x_{i}\mathbf{a}_{i}} = \sum_{i=1}^{n}\abs{x_{i}}\pnorm[1]{\mathbf{a}_{i}} \leq \sum_{j=1}^{n}\abs{x_{j}} \max_{1 \leq i \leq n}\pnorm[1]{\mathbf{a}_{i}} = \pnorm[1]{\mathbf{x}}\max_{1 \leq i \leq n}\pnorm[1]{\mathbf{a}_{i}} = \max_{1 \leq i \leq n}\pnorm[1]{\mathbf{a}_{i}}.
        \end{equation*}
        By taking the supremum over $\mathbf{x}$, we get $\pnorm[1]{\mathbf{A}} \leq \max_{1 \leq i \leq n} \pnorm[1]{\mathbf{a}_{i}}$.
        
        Let $i_{m} = \argmax_{1 \leq i \leq n} \pnorm[1]{\mathbf{a}_{i}}$. Then:
        \begin{equation*}
            \max_{1 \leq i \leq n}\pnorm[1]{\mathbf{a}_{i}} = \pnorm[1]{\mathbf{a}_{i_{m}}} = \pnorm[1]{\mathbf{A}\mathbf{e}_{i_{m}}} \leq \sup_{\pnorm[1]{\mathbf{x}}} \pnorm[1]{\mathbf{A}\mathbf{x}} = \pnorm[1]{\mathbf{A}}.
        \end{equation*}
        Therefore, $\pnorm[1]{\mathbf{A}} = \max_{1 \leq i \leq n} \pnorm[1]{\mathbf{a}_{i}}$.
    \end{proofing}
    \newpage

    \begin{eg}
        Consider $\mathbf{A} \in \mathbb{R}^{m \times n}$ as a linear operator from $\mathbb{R}^{n}$ to $\mathbb{R}^{m}$. Let $\mathbb{R}^{n}$ and $\mathbb{R}^{m}$ be endowed with infinity norms. We call the resulting operator norm the \textbf{$\infty$-norm}, defined by:
        \begin{equation*}
            \pnorm[\infty]{\mathbf{A}} = \sup_{\pnorm[\infty]{\mathbf{x}} = 1} \pnorm[\infty]{\mathbf{A}\mathbf{x}}.
        \end{equation*}
    \end{eg}
    \begin{thm}
        Let $\mathbf{A} = (\mathbf{a}^{(1)}, \cdots, \mathbf{a}^{(m)})^{T} \in \mathbb{R}^{m \times n}$. Then:
        \begin{equation*}
            \pnorm[\infty]{\mathbf{A}} = \max_{1 \leq i \leq m} \spnorm[1]{\mathbf{a}^{(i)}}.
        \end{equation*}
    \end{thm}
    \begin{proofing}
        For any $\mathbf{x} \in \mathbb{R}^{n}$ satisfying $\pnorm[\infty]{\mathbf{x}}=1$,
        \begin{align*}
            \pnorm[\infty]{\mathbf{A}\mathbf{x}} &= \max_{1 \leq i \leq m} \abs{\inprod{\mathbf{a}^{(i)}}{\mathbf{x}}}\\
            &= \max_{1 \leq i \leq m} \abs{\sum_{j=1}^{n} a_{j}^{(i)}x_{j}}\\
            &\leq \max_{1 \leq i \leq m} \sum_{j=1}^{n} \abs{a_{j}^{(i)}}\abs{x_{j}}\\
            &\leq \max_{1 \leq j \leq n} \abs{x_{j}} \max_{1 \leq i \leq m} \sum_{j=1}^{n}\abs{a_{j}^{(i)}} = \pnorm[\infty]{\mathbf{x}} \max_{1 \leq i \leq m} \spnorm[1]{\mathbf{a}^{(i)}} = \max_{1 \leq i \leq m} \spnorm[1]{\mathbf{a}^{(i)}}.
        \end{align*}
        By taking the supremum over $\mathbf{x}$, we get $\pnorm[\infty]{\mathbf{A}} \leq \max_{1 \leq i \leq m} \spnorm[1]{\mathbf{a}^{(i)}}$.
        
        Without loss of generality, assume that:
        \begin{equation*}
            \spnorm[1]{\mathbf{a}^{(1)}} = \max_{1 \leq i \leq m} \spnorm[1]{\mathbf{a}^{(i)}}.
        \end{equation*}
        We construct $\mathbf{x}$ such that $x_{i} = \sgn(a_{i}^{(1)})$, which has $\pnorm[\infty]{\mathbf{x}} = 1$. Then $\abs{\inprod{\mathbf{a}^{(1)}}{\mathbf{x}}} = \spnorm[1]{\mathbf{a}^{(1)}}$.
        
        Therefore, $\pnorm[\infty]{\mathbf{A}} = \max_{1 \leq i \leq m} \spnorm[1]{\mathbf{a}^{(i)}}$.
    \end{proofing}
    \begin{eg}
        Let $T: \mathcal{C}[-1,1] \to \mathcal{C}[0,2]$ be the translation operator defined by:
        \begin{equation*}
            (T f)(t) = f(t-1), \qquad \text{for any } t \in [0,2], \qquad \text{for } f \in \mathcal{C}[-1,1].
        \end{equation*}
        We have found that $T$ is linear. The norm on $\mathcal{C}[a,b]$ is:
        \begin{equation*}
            \pnorm[\infty]{f} = \max_{t \in [a,b]} \abs{f(t)}.
        \end{equation*}
        Then the norm of $T$ is:
        \begin{equation*}
            \norm{T} = \sup_{\pnorm[\infty]{f} = 1} \pnorm[\infty]{T f} = \sup_{\max_{t \in [-1,1]}\abs{f(t)} = 1} \max_{t \in [0,2]} \abs{f(t-1)} = 1.
        \end{equation*}
    \end{eg}
    \begin{eg}
        Let:
        \begin{align*}
            V &= \{f: f \text{ and } f' \text{ are continuous on } [a,b]\}, & W &= \{f: f \text{ is continuous on } [a,b]\}
        \end{align*}
        both equipped with the norm $\pnorm[\infty]{\cdot}$. Consider the differentiation operator $D: V \to W$ defined by:
        \begin{equation*}
            D f = f', \qquad \text{for } f \in V.
        \end{equation*}
        For $t \in [0,1]$, consider $f_{k}(t) = \sin(2\pi kt)$, where $k \in \mathbb{N}$. Then $f_{k}'(t) = 2\pi k \cos(2\pi kt)$. We have $f_{k} \in V$ and:
        \begin{equation*}
            \pnorm[\infty]{D f_{k}} = 2\pi k\pnorm[\infty]{\cos(2\pi kt)} = 2\pi k.
        \end{equation*}
        Hence:
        \begin{equation*}
            \norm{D} = \sup_{\pnorm[\infty]{f} = 1} \pnorm[\infty]{D f} \geq \lim_{k \to \infty} \pnorm[\infty]{D f_{k}} = \lim_{k \to \infty} 2\pi k = \infty.
        \end{equation*}
        Therefore, $\norm{D} = +\infty$.
    \end{eg}
    \newpage

    We can group all linear operators from one vector space to another into a set.
    \begin{defn}
        Let $V, W$ be two normed vector spaces with $\pnorm[V]{\cdot}$ and $\pnorm[W]{\cdot}$, respectively. The normed vector space of all linear and bounded operators is defined as:
        \begin{equation*}
            \mathscr{L}(V, W) = \{\mathbf{A}: \mathbf{A} \text{ is linear from } V \to W \text{ and } \norm{\mathbf{A}} < +\infty\}.
        \end{equation*}
    \end{defn}
    \begin{rem}
        If both $V$ and $W$ are Banach spaces, then $\mathscr{L}(V, W)$ is also a Banach space.
    \end{rem}
    \begin{eg}
        If $V = \mathbb{R}^{n}$ and $W = \mathbb{R}^{m}$ with any norm, then $\mathscr{L}(\mathbb{R}^{n}, \mathbb{R}^{m}) = \mathbb{R}^{m \times n}$.
    \end{eg}
    \begin{eg}
        \label{Chapter 5 (Example) Inner Product Vector Operator is Bounded}
        Let $\mathbf{a}_{1}, \cdots, \mathbf{a}_{k} \in H$, where $H$ is a Hilbert space. Define $L: H \to \mathbb{R}^{k}$ by:
        \begin{equation*}
            L \mathbf{x} = \begin{pmatrix}
                \inprod{\mathbf{a}_{1}}{\mathbf{x}}\\
                \vdots\\
                \inprod{\mathbf{a}_{k}}{\mathbf{x}}
            \end{pmatrix}, \qquad \text{for } \mathbf{x} \in H.
        \end{equation*}
        We know $L$ is linear, but is it bounded?
        \begin{equation*}
            \norm{L} = \sup_{\norm{\mathbf{x}} = 1} \norm{L \mathbf{x}} = \sup_{\norm{\mathbf{x}} = 1} \sqrt{\sum_{i=1}^{k}(\inprod{\mathbf{a}_{i}}{\mathbf{x}})^{2}} \leq \sup_{\norm{\mathbf{x}} = 1} \sqrt{\sum_{i=1}^{k}\norm{\mathbf{a}_{i}}^{2}\norm{\mathbf{x}}^{2}} = \sqrt{\sum_{i=1}^{k}\norm{\mathbf{a}_{i}}^{2}} < +\infty.
        \end{equation*}
        Therefore, $L \in \mathscr{L}(H, \mathbb{R}^{k})$.
    \end{eg}
    \begin{eg}
        \label{Chapter 5 (Example) Translation Functional Operator is Linear and Bounded}
        Let $T: L^{2}(-1,1) \to L^{2}(0,2)$ be the translation operator defined by:
        \begin{equation*}
            (T f)(t) = f(t-1), \qquad \text{for all } t \in (0,2), \qquad \text{for } f \in L^{2}(-1,1).
        \end{equation*}
        We prove that $T$ is linear and bounded.
        \begin{enumerate}
            \item For any $f, g \in L^{2}(-1,1)$ and $\alpha, \beta \in \mathbb{R}$,
            \begin{equation*}
                T(\alpha f + \beta g)(t) = (\alpha f + \beta g)(t-1) = \alpha f(t-1) + \beta g(t-1) = \alpha (T f)(t) + \beta (T g)(t).
            \end{equation*}
            Therefore, $T$ is linear.
            \item 
            \begin{equation*}
                \norm{T} = \sup_{\pnorm[2]{f} = 1} \pnorm[2]{T f} = \sup_{\int_{-1}^{1}\abs{f(t)}^{2}\,dt = 1} \sqrt{\int_{0}^{2}\abs{f(t-1)}^{2}\,dt} = 1.
            \end{equation*}
            Therefore, $T$ is bounded.
        \end{enumerate}
        Thus, $T \in \mathscr{L}(L^{2}(-1,1), L^{2}(0,2))$.
    \end{eg}
    \newpage

    Consider the Hilbert spaces $H_{1}$ and $H_{2}$. Both have an inner product. If there is a linear operator acting on vectors in $H_{1}$ to $H_{2}$, is there a corresponding linear operator acting on vectors in $H_{2}$ to $H_{1}$?
    \begin{defn}
        Let $H_{1}, H_{2}$ be two Hilbert spaces with $\inprod{\cdot}{\cdot}_{H_{1}}$ and $\inprod{\cdot}{\cdot}_{H_{2}}$, respectively. Let $\mathbf{A} \in \mathscr{L}(H_{1}, H_{2})$. The \textbf{adjoint operator} $\mathbf{A}^{*}: H_{2} \to H_{1}$ is the unique bounded linear operator satisfying:
        \begin{equation*}
            \inprod{\mathbf{A}\mathbf{x}}{\mathbf{y}}_{H_{2}} = \inprod{\mathbf{x}}{\mathbf{A}^{*}\mathbf{y}}_{H_{1}}, \qquad \text{for } \mathbf{x} \in H_{1}, \mathbf{y} \in H_{2}.
        \end{equation*} 
    \end{defn}
    \begin{eg}
        Consider $\mathbf{A} \in \mathbb{R}^{m \times n} = \mathscr{L}(\mathbb{R}^{n}, \mathbb{R}^{m})$. Then for all $\mathbf{x} \in \mathbb{R}^{n}$ and $\mathbf{y} \in \mathbb{R}^{m}$,
        \begin{equation*}
            \inprod{\mathbf{A}\mathbf{x}}{\mathbf{y}} = \mathbf{y}^{T}\mathbf{A}\mathbf{x} = (\mathbf{A}^{T}\mathbf{y})^{T}\mathbf{x} = \inprod{\mathbf{x}}{\mathbf{A}^{T}\mathbf{y}}.
        \end{equation*}
        We know that $\mathbf{A}^{T} \in \mathscr{L}(\mathbb{R}^{m}, \mathbb{R}^{n})$. Therefore, the adjoint of $\mathbf{A}$ is $\mathbf{A}^{T}$.
    \end{eg}
    \begin{rem}
        The adjoint is an extension of the matrix transpose.
    \end{rem}
    \begin{eg}
        Let $H$ be a Hilbert space. Let $f: H \to \mathbb{R}$ be linear and bounded. We have:
        \begin{equation*}
            f(\mathbf{x}) = \inprod{\mathbf{a}}{\mathbf{x}}_{H}, \qquad \text{for } \mathbf{a} \in H.
        \end{equation*}
        For all $\mathbf{x} \in H$ and $y \in \mathbb{R}$,
        \begin{equation*}
            \inprod{f(\mathbf{x})}{y}_{\mathbb{R}} = f(\mathbf{x})y = y\inprod{\mathbf{a}}{\mathbf{x}}_{H} = \inprod{\mathbf{x}}{y\mathbf{a}}_{H}.
        \end{equation*}
        We may check whether $g(y) = y\mathbf{a}$ is linear and bounded for all $y \in \mathbb{R}$.
        \begin{enumerate}
            \item For any $y_{1}, y_{2}, \alpha, \beta \in \mathbb{R}$,
            \begin{align*}
                g(\alpha y_{1} + \beta y_{2}) &= (\alpha y_{1} + \beta y_{2})\mathbf{a}\\
                &= \alpha y_{1}\mathbf{a} + \beta y_{2}\mathbf{a}\\
                &= \alpha g(y_{1}) + \beta g(y_{2}).
            \end{align*}
            \item
            \begin{equation*}
                \norm{g} = \sup_{\abs{y} = 1} \pnorm[H]{g(y)} = \sup_{\abs{y} = 1} \pnorm[H]{y\mathbf{a}} = \pnorm[H]{\mathbf{a}} < \infty.
            \end{equation*}
        \end{enumerate}
        Thus, $g \in \mathscr{L}(\mathbb{R}, H)$ and $f^{*}(y) = y\mathbf{a}$.
    \end{eg}
    \newpage

    \begin{eg}
        Let $\mathbf{a}_{1}, \cdots, \mathbf{a}_{k} \in H$, where $H$ is a Hilbert space. Define $L: H \to \mathbb{R}^{k}$ by:
        \begin{equation*}
            L \mathbf{x} = \begin{pmatrix}
                \inprod{\mathbf{a}_{1}}{\mathbf{x}}\\
                \vdots\\
                \inprod{\mathbf{a}_{k}}{\mathbf{x}}
            \end{pmatrix}, \qquad \text{for } \mathbf{x} \in H.
        \end{equation*}
        From Example \ref{Chapter 5 (Example) Inner Product Vector Operator is Bounded}, $L \in \mathscr{L}(H, \mathbb{R}^{k})$. For all $\mathbf{x} \in H$ and $\mathbf{y} \in \mathbb{R}^{k}$,
        \begin{equation*}
            \inprod{L \mathbf{x}}{\mathbf{y}}_{\mathbb{R}^{k}} = \sum_{i=1}^{k}\inprod{\mathbf{a}_{i}}{\mathbf{x}}_{H} y_{i} = \sum_{i=1}^{k} \inprod{y_{i}\mathbf{a}_{i}}{\mathbf{x}}_{H} = \inprod{\mathbf{x}}{\sum_{i=1}^{k}y_{i}\mathbf{a}_{i}}_{H}.
        \end{equation*}
        Therefore, we can define $L^{*}: \mathbb{R}^{k} \to H$ by:
        \begin{equation*}
            L^{*} \mathbf{y} = \sum_{i=1}^{k}y_{i}\mathbf{a}_{i}, \qquad \text{for } \mathbf{y} \in \mathbb{R}^{k}.
        \end{equation*}
        It remains to show that $L^{*} \in \mathscr{L}(\mathbb{R}^{k}, H)$.
        \begin{enumerate}
            \item For any $\mathbf{u}, \mathbf{v} \in \mathbb{R}^{k}$ and $\alpha, \beta \in \mathbb{R}$,
            \begin{equation*}
                L^{*}(\alpha \mathbf{u} + \beta \mathbf{v}) = \sum_{i=1}^{k}(\alpha u_{i} + \beta v_{i})\mathbf{a}_{i} = \alpha \sum_{i=1}^{k} u_{i}\mathbf{a}_{i} + \beta \sum_{i=1}^{k} v_{i}\mathbf{a}_{i} = \alpha L^{*}(\mathbf{u}) + \beta L^{*}(\mathbf{v}).
            \end{equation*}
            Therefore, $L^{*}$ is linear.
            \item 
            \begin{align*}
                \norm{L^{*}} &= \sup_{\norm{\mathbf{y}} = 1} \norm{L^{*}\mathbf{y}} = \sup_{\norm{\mathbf{y}} = 1} \norm{\sum_{i=1}^{k}y_{i}\mathbf{a}_{i}}\\
                &\leq \sup_{\norm{\mathbf{y}} = 1} \sum_{i=1}^{k}\abs{y_{i}}\norm{\mathbf{a}_{i}} = \sup_{\norm{\mathbf{y}} = 1} \inprod{\begin{pmatrix}
                    \abs{y_{1}}\\
                    \vdots\\
                    \abs{y_{k}}
                \end{pmatrix}}{\begin{pmatrix}
                    \norm{\mathbf{a}_{1}}\\
                    \vdots\\
                    \norm{\mathbf{a}_{k}}
                \end{pmatrix}}\\
                &\leq \sup_{\norm{\mathbf{y}} = 1} \norm{\begin{pmatrix}
                        \abs{y_{1}}\\
                        \vdots\\
                        \abs{y_{k}}
                \end{pmatrix}}\norm{\begin{pmatrix}
                        \norm{\mathbf{a}_{1}}\\
                        \vdots\\
                        \norm{\mathbf{a}_{k}}
                \end{pmatrix}} = \norm{\begin{pmatrix}
                    \norm{\mathbf{a}_{1}}\\
                    \vdots\\
                    \norm{\mathbf{a}_{k}}
                \end{pmatrix}} = \sqrt{\sum_{i=1}^{k}\norm{\mathbf{a}_{i}}^{2}} < \infty.
            \end{align*}
            Therefore, $L^{*} \in \mathscr{L}(\mathbb{R}^{k},H)$ and thus $L^{*}$ is the adjoint of $L$.
        \end{enumerate}
    \end{eg}
    \begin{eg}
        Let $T: L^{2}(-1,1) \to L^{2}(0,2)$ be the translation operator defined by:
        \begin{equation*}
            (T f)(t) = f(t-1), \qquad \text{for all } t \in (0,2), \qquad \text{for } f \in L^{2}(-1,1).
        \end{equation*}
        From Example \ref{Chapter 5 (Example) Translation Functional Operator is Linear and Bounded}, we have $T \in \mathscr{L}(L^{2}(-1,1), L^{2}(0,2))$. Let $f \in L^{2}(-1,1)$ and $g \in L^{2}(0,2)$. We have:
        \begin{equation*}
            \inprod{T f}{g} = \int_{0}^{2}(T f)(t)g(t)\,dt = \int_{0}^{2}f(t-1)g(t)\,dt = \int_{-1}^{1}f(s)g(s+1)\,ds.
        \end{equation*}
        We may define $T^{*} g(t) = g(t+1)$ for any $t \in (-1,1)$ and $g \in L^{2}(0,2)$.
        
        Similarly, $T^{*} \in \mathscr{L}(L^{2}(0,2), L^{2}(-1,1))$, and thus $T^{*} g(t) = g(t+1)$.
    \end{eg}
    \newpage

\section{Linear Approximation and Differentiation of Transformations}
    Recall the definition of differentiation in the previous chapter. We can generalize it to linear transformations. Let $V, W$ be two Banach spaces with norms $\pnorm[V]{\cdot}$ and $\pnorm[W]{\cdot}$, respectively. Let $F: V \to W$ be a map (not necessarily linear). At $\mathbf{x} \in V$, the linear approximation passing through $(\mathbf{x}, F(\mathbf{x}))$ is given by:
    \begin{equation*}
        F(\mathbf{y}) \approx F(\mathbf{x}) + L(\mathbf{y}-\mathbf{x}), \qquad \text{for } \mathbf{y} \in V,
    \end{equation*}
    where $L \in \mathscr{L}(V, W)$. If the error is $o(\pnorm[V]{\mathbf{y}-\mathbf{x}})$, then we call $L$ the differentiation of $F$ at $\mathbf{x}$.
    \begin{defn}
        Let $V, W$ be Banach spaces. Let $F: V \to W$ be a map and $\mathbf{x} \in V$. Then, $F$ is said to be \textbf{(Fr\'echet) differentiable} at $\mathbf{x}$ if there exists $L \in \mathscr{L}(V, W)$ such that:
        \begin{equation*}
            \lim_{\pnorm[V]{\mathbf{y}-\mathbf{x}} \to 0} \frac{\pnorm[W]{F(\mathbf{y}) - (F(\mathbf{x}) + L(\mathbf{y}-\mathbf{x}))}}{\pnorm[V]{\mathbf{y}-\mathbf{x}}} = 0.
        \end{equation*}
        $L$ is called the \textbf{differentiation} of $F$ at $\mathbf{x}$, defined by:
        \begin{equation*}
            D F(\mathbf{x}) = L.
        \end{equation*}
    \end{defn}
    \begin{rem}
        $D F(\mathbf{x})$ is a function, not a vector in $W$.
    \end{rem}
    \begin{rem}
    	We can substitute $\mathbf{y} = \mathbf{x} + \mathbf{h}$. Then the definition becomes:
    	\begin{equation*}
    		\lim_{\pnorm[V]{\mathbf{h}} \to 0} \frac{\pnorm[W]{F(\mathbf{x} + \mathbf{h}) - (F(\mathbf{x}) + L \mathbf{h})}}{\pnorm[V]{\mathbf{y}-\mathbf{x}}} = 0.
    	\end{equation*}
    	$L \mathbf{h}$ is called the \textbf{differential} of $F$ at $\mathbf{x}$, defined by:
    	\begin{equation*}
    		D F(\mathbf{x})(\mathbf{h}) = L \mathbf{h}.
    	\end{equation*}
    \end{rem}
    \begin{rem}
        Let $V, W_{1}, W_{2}$ be Banach spaces, and let $f_{1}: V \to W_{1}$, $f_{2}: V \to W_{2}$. Define:
        \begin{equation*}
            F(\mathbf{x}) = (f_{1}(\mathbf{x}), f_{2}(\mathbf{x})) \in W_{1} \times W_{2}, \qquad \text{for } \mathbf{x} \in V.
        \end{equation*}
        If $f_{1}$ and $f_{2}$ are differentiable, then:
        \begin{equation*}
            D F(\mathbf{x}) = (D f_{1}(\mathbf{x}), D f_{2}(\mathbf{x})).
        \end{equation*}
    \end{rem}
    \begin{eg}
        Let $\mathbf{A} \in \mathscr{L}(V, W)$. Then, for all $\mathbf{x} \in V$,
        \begin{equation*}
            \lim_{\pnorm[V]{\mathbf{y}-\mathbf{x}} \to 0} \frac{\pnorm[W]{\mathbf{A}\mathbf{y}-(\mathbf{A}\mathbf{x} + \mathbf{A}(\mathbf{y}-\mathbf{x}))}}{\pnorm[V]{\mathbf{y}-\mathbf{x}}} = 0.
        \end{equation*}
        Therefore, $D \mathbf{A}\mathbf{x} = \mathbf{A} \in \mathscr{L}(V, W)$.
    \end{eg}
    This example is related to the following theorem.
    \begin{thm}
        \label{Chapter 5 (Theorem) Differentiation of Linear Map}
        Let $V, W$ be Banach spaces. Then, for any linear $F: V \to W$, 
        \begin{equation*}
            D F(\mathbf{x}) = F, \qquad \text{for } \mathbf{x} \in V.
        \end{equation*}
    \end{thm}
    \begin{proofing}
        If $F$ is linear,
        \begin{equation*}
            \lim_{\pnorm[V]{\mathbf{y}-\mathbf{x}} \to 0} \frac{\pnorm[W]{F(\mathbf{y}) - (F(\mathbf{x}) + F(\mathbf{y}-\mathbf{x}))}}{\pnorm[V]{\mathbf{y}-\mathbf{x}}} = \lim_{\pnorm[V]{\mathbf{y}-\mathbf{x}} \to 0} \frac{\pnorm[W]{F(\mathbf{y}) - (F(\mathbf{x}) + F(\mathbf{y})-F(\mathbf{x}))}}{\pnorm[V]{\mathbf{y}-\mathbf{x}}} = 0.
        \end{equation*}
    \end{proofing}
    \newpage

    \begin{eg}
        Let $H$ be a Hilbert space and $F: H \to \mathbb{R}$. Then, the differentiation of $F$ at $\mathbf{x} \in H$ is:
        \begin{equation*}
            D F(\mathbf{x})(\mathbf{y}) = \inprod{\nabla F(\mathbf{x})}{\mathbf{y}}, \qquad \text{for } \mathbf{y} \in H.
        \end{equation*}
    \end{eg}
    \begin{rem}
        The gradient is defined only for $F: H \to \mathbb{R}$.
    \end{rem}
    \begin{eg}
        Let $\mathbf{A} \in \mathscr{L}(H_{1}, H_{2})$, where $H_{1}, H_{2}$ are Hilbert spaces. Let $f: H_{1} \to \mathbb{R}$ be defined by:
        \begin{equation*}
            f(\mathbf{x}) = \frac{1}{2}\pnorm[H_{2}]{\mathbf{A}\mathbf{x}-\mathbf{b}}^{2}, \qquad \text{for } \mathbf{x} \in H_{1},
        \end{equation*}
        where $\mathbf{b} \in H_{2}$. How do we find $\nabla f(\mathbf{x})$?
        \begin{align*}
            f(\mathbf{y}) &= \frac{1}{2}\pnorm[H_{2}]{\mathbf{A}\mathbf{y}-\mathbf{b}}^{2}\\
            &= \frac{1}{2}\pnorm[H_{2}]{(\mathbf{A}\mathbf{x}-\mathbf{b}) + (\mathbf{A}\mathbf{y} - \mathbf{A}\mathbf{x})}^{2}\\
            &= \frac{1}{2}\pnorm[H_{2}]{\mathbf{A}\mathbf{x}-\mathbf{b}}^{2} + \inprod{\mathbf{A}\mathbf{x}-\mathbf{b}}{\mathbf{A}(\mathbf{y}-\mathbf{x})}_{H_{2}} + \frac{1}{2}\pnorm[H_{2}]{\mathbf{A}(\mathbf{y}-\mathbf{x})}^{2}\\
            &= f(\mathbf{x}) + \inprod{\mathbf{A}^{*}(\mathbf{A}\mathbf{x}-\mathbf{b})}{\mathbf{y}-\mathbf{x}} + \frac{1}{2}\pnorm[H_{2}]{\mathbf{A}(\mathbf{y}-\mathbf{x})}^{2}.
        \end{align*}
        We may find that as $\pnorm[H_{1}]{\mathbf{y}-\mathbf{x}} \to 0$,
        \begin{equation*}
            \frac{\abs{f(\mathbf{y})-(f(\mathbf{x}) + \inprod{\mathbf{A}^{*}(\mathbf{A}\mathbf{x}-\mathbf{b})}{\mathbf{y}-\mathbf{x}})}}{\pnorm[H_{1}]{\mathbf{y}-\mathbf{x}}} = \frac{\frac{1}{2}\pnorm[H_{2}]{\mathbf{A}(\mathbf{y}-\mathbf{x})}^{2}}{\pnorm[H_{1}]{\mathbf{y}-\mathbf{x}}} \leq \frac{1}{2}\norm{\mathbf{A}}^{2} \frac{\pnorm[H_{1}]{\mathbf{y}-\mathbf{x}}^{2}}{\pnorm[H_{1}]{\mathbf{y}-\mathbf{x}}} \to 0.
        \end{equation*}
        Therefore, $\nabla f(\mathbf{x}) = \mathbf{A}^{*}(\mathbf{A}\mathbf{x}-\mathbf{b})$ and $D f(\mathbf{x})(\mathbf{y}) = \inprod{\mathbf{A}^{*}(\mathbf{A}\mathbf{x}-\mathbf{b})}{\mathbf{y}}$.
    \end{eg}
    It has some properties.
    \begin{thm}
        Let $V, W$ be Banach spaces. For any $F, G: V \to W$ and $\alpha, \beta \in \mathbb{R}$,
        \begin{equation*}
            D(\alpha F + \beta G)(\mathbf{x}) = \alpha D F(\mathbf{x}) + \beta D G(\mathbf{x}), \qquad \text{for } \mathbf{x} \in V.
        \end{equation*}
    \end{thm}
    \begin{proofing}
        By definition, for all $\mathbf{x} \in V$,
        \begin{align*}
            0 &\leq \lim_{\pnorm[V]{\mathbf{y}-\mathbf{x}} \to 0} \frac{\pnorm[W]{(\alpha F + \beta G)(\mathbf{y})-((\alpha F + \beta G)(\mathbf{x}) + (\alpha D F(\mathbf{x}) + \beta D G(\mathbf{x}))(\mathbf{y}-\mathbf{x}))}}{\pnorm[V]{\mathbf{y}-\mathbf{x}}}\\
            &\leq \abs{\alpha} \lim_{\pnorm[V]{\mathbf{y}-\mathbf{x}} \to 0} \frac{\pnorm[W]{F(\mathbf{y})-(F(\mathbf{x}) + D F(\mathbf{x})(\mathbf{y}-\mathbf{x}))}}{\pnorm[V]{\mathbf{y}-\mathbf{x}}} + \abs{\beta} \lim_{\pnorm[V]{\mathbf{y}-\mathbf{x}} \to 0} \frac{\pnorm[W]{G(\mathbf{y})-(G(\mathbf{x}) + D G(\mathbf{x})(\mathbf{y}-\mathbf{x}))}}{\pnorm[V]{\mathbf{y}-\mathbf{x}}}\\
            &\leq \abs{\alpha}(0) + \abs{\beta}(0) = 0.
        \end{align*}
        Therefore, $D(\alpha F + \beta G)(\mathbf{x}) = \alpha D F(\mathbf{x}) + \beta D G(\mathbf{x})$.
    \end{proofing}
    \newpage

    The differentiation has an important property that is fundamental to many other properties.
    \begin{thm}\named{Chain Rule}
        Let $V_{1}, V_{2}, V_{3}$ be Banach spaces. Let $F: V_{1} \to V_{2}$ and $G: V_{2} \to V_{3}$. Then, the differentiation of $G \circ F: V_{1} \to V_{3}$ at $\mathbf{x} \in V_{1}$ is given by:
        \begin{equation*}
            D(G \circ F)(\mathbf{x}) = D G(F(\mathbf{x})) \circ D F(\mathbf{x}),
        \end{equation*}
        if $F$ is differentiable at $\mathbf{x}$ and $G$ is differentiable at $F(\mathbf{x})$.
    \end{thm}
    \begin{proofing}
        Since $F$ is differentiable at $\mathbf{x}$ and $G$ is differentiable at $F(\mathbf{x})$, we have:
        \begin{align*}
            \tag{1}
            \lim_{\pnorm[V_{1}]{\mathbf{y}-\mathbf{x}} \to 0} \frac{\pnorm[V_{2}]{F(\mathbf{y}) - (F(\mathbf{x}) + D F(\mathbf{x})(\mathbf{y}-\mathbf{x}))}}{\pnorm[V_{1}]{\mathbf{y}-\mathbf{x}}} &= 0,\\
            \tag{2}
            \lim_{\pnorm[V_{2}]{F(\mathbf{y})-F(\mathbf{x})} \to 0} \frac{\pnorm[V_{3}]{G(F(\mathbf{y})) - (G(F(\mathbf{x})) + DG(F(\mathbf{x}))(F(\mathbf{y})-F(\mathbf{x})))}}{\pnorm[V_{2}]{F(\mathbf{y})-F(\mathbf{x})}} &= 0.
        \end{align*}
        By (1), there exists $\delta>0$ such that if $\pnorm[V_{1}]{\mathbf{y}-\mathbf{x}} < \delta$,
        \begin{equation*}
            \pnorm[V_{2}]{F(\mathbf{y}) - (F(\mathbf{x}) + D F(\mathbf{x})(\mathbf{y}-\mathbf{x}))} < \pnorm[V_{1}]{\mathbf{y}-\mathbf{x}}.
        \end{equation*}
        Therefore, since $D F(\mathbf{x})$ is bounded, $\norm{D F(\mathbf{x})} = M < \infty$,
        \begin{align*}
            \pnorm[V_{2}]{F(\mathbf{y}) - F(\mathbf{x})} &= \pnorm[V_{2}]{F(\mathbf{y}) - F(\mathbf{x}) - D F(\mathbf{x})(\mathbf{y} - \mathbf{x}) + D F(\mathbf{x})(\mathbf{y} - \mathbf{x})}\\
            &\leq \pnorm[V_{2}]{F(\mathbf{y}) - (F(\mathbf{x}) + D F(\mathbf{x})(\mathbf{y} - \mathbf{x}))} + \pnorm[V_{2}]{D F(\mathbf{x})(\mathbf{y} - \mathbf{x})}\\
            &\leq \pnorm[V_{1}]{\mathbf{y}-\mathbf{x}} + M \pnorm[V_{1}]{\mathbf{y}-\mathbf{x}},\\
            \frac{\pnorm[V_{2}]{F(\mathbf{y}) - F(\mathbf{x})}}{\pnorm[V_{1}]{\mathbf{y}-\mathbf{x}}} &\leq 1 + M.
        \end{align*}
        From this, we also have that $\pnorm[V_{1}]{\mathbf{y}-\mathbf{x}} \to 0$ implies that $\pnorm[V_{2}]{F(\mathbf{y}) - F(\mathbf{x})} \to 0$. Therefore, since $D G(F(\mathbf{x}))$ is bounded,
        \begin{align*}
            0 &\leq \lim_{\pnorm[V_{1}]{\mathbf{y}-\mathbf{x}} \to 0} \frac{\pnorm[V_{3}]{G(F(\mathbf{y})) - (G(F(\mathbf{x})) + DG(F(\mathbf{x}))(D F(\mathbf{x})(\mathbf{y}-\mathbf{x})))}}{\pnorm[V_{1}]{\mathbf{y}-\mathbf{x}}}\\
            &\leq \lim_{\pnorm[V_{1}]{\mathbf{y}-\mathbf{x}} \to 0} \frac{\pnorm[V_{3}]{G(F(\mathbf{y})) - (G(F(\mathbf{x})) + DG(F(\mathbf{x}))(F(\mathbf{y})-F(\mathbf{x})))}}{\pnorm[V_{2}]{F(\mathbf{y})-F(\mathbf{x})}}\left(\frac{\pnorm[V_{2}]{F(\mathbf{y})-F(\mathbf{x})}}{\pnorm[V_{1}]{\mathbf{y}-\mathbf{x}}}\right)\\
            & \qquad + \lim_{\pnorm[V_{1}]{\mathbf{y}-\mathbf{x}} \to 0} \frac{\pnorm[V_{3}]{DG(F(\mathbf{x}))(F(\mathbf{y})-F(\mathbf{x})) - DG(F(\mathbf{x}))(D F(\mathbf{x})(\mathbf{y}-\mathbf{x}))}}{\pnorm[V_{1}]{\mathbf{y}-\mathbf{x}}}\\
            &\leq (1+M)\lim_{\pnorm[V_{2}]{F(\mathbf{y})-F(\mathbf{x})} \to 0} \frac{\pnorm[V_{3}]{G(F(\mathbf{y})) - (G(F(\mathbf{x})) + DG(F(\mathbf{x}))(F(\mathbf{y})-F(\mathbf{x})))}}{\pnorm[V_{2}]{F(\mathbf{y})-F(\mathbf{x})}}\\
            & \qquad + \norm{D G(F(\mathbf{x}))} \lim_{\pnorm[V_{1}]{\mathbf{y}-\mathbf{x}} \to 0} \frac{\pnorm[V_{2}]{F(\mathbf{y}) - (F(\mathbf{x}) + D F(\mathbf{x})(\mathbf{y}-\mathbf{x}))}}{\pnorm[V_{1}]{\mathbf{y}-\mathbf{x}}} = 0.
        \end{align*}
        Therefore, by the Squeeze Theorem,
        \begin{equation*}
            \lim_{\pnorm[V_{1}]{\mathbf{y}-\mathbf{x}} \to 0} \frac{\pnorm[V_{3}]{G(F(\mathbf{y})) - (G(F(\mathbf{x})) + DG(F(\mathbf{x}))(D F(\mathbf{x})(\mathbf{y}-\mathbf{x})))}}{\pnorm[V_{1}]{\mathbf{y}-\mathbf{x}}} = 0,
        \end{equation*}
        and thus $D(G \circ F)(\mathbf{x}) = D G(F(\mathbf{x})) \circ D F(\mathbf{x})$.
    \end{proofing}
    \newpage

    We can use the chain rule to derive many properties of differentiation.
    \begin{thm}\named{Theorem \ref{Chapter 4 (Theorem) Properties of Frechet differentiation}}
        Let $H$ be a Hilbert space. Let $F: H \to \mathbb{R}$ and $G: \mathbb{R} \to \mathbb{R}$. Then, the gradient of $G \circ F: H \to \mathbb{R}$ at $\mathbf{x} \in H$ is given by:
        \begin{equation*}
            \nabla(G \circ F)(\mathbf{x}) = G'(F(\mathbf{x})) \cdot \nabla F(\mathbf{x}),
        \end{equation*}
        if $F$ is differentiable at $\mathbf{x}$ and $G$ is differentiable at $F(\mathbf{x})$.
    \end{thm}
    \begin{proofing}[Alternative Proof.]
        For $\mathbf{y} \in H$ and $\alpha \in \mathbb{R}$,
        \begin{align*}
            D G(F(\mathbf{x}))(\alpha) &= G'(F(\mathbf{x})) \alpha, & D F(\mathbf{x})(\mathbf{y}) &= \inprod{\nabla F(\mathbf{x})}{\mathbf{y}}.
        \end{align*}
        Therefore, by the chain rule,
        \begin{align*}
            D(G \circ F)(\mathbf{x})(\mathbf{y}) &= D G(F(\mathbf{x}))(D F(\mathbf{x})(\mathbf{y}))\\
            &= G'(F(\mathbf{x})) \inprod{\nabla F(\mathbf{x})}{\mathbf{y}}\\
            &= \inprod{G'(F(\mathbf{x})) \cdot \nabla F(\mathbf{x})}{\mathbf{y}}.
        \end{align*}
        Therefore, $\nabla(G \circ F)(\mathbf{x}) = G'(F(\mathbf{x})) \cdot \nabla F(\mathbf{x})$.
    \end{proofing}
    \begin{thm}
        Let $H_{1}, H_{2}$ be Hilbert spaces. Let $F: H_{1} \to H_{2}$ and $G: H_{2} \to \mathbb{R}$. Then, the gradient of $G \circ F: H_{1} \to \mathbb{R}$ at $\mathbf{x} \in H_{1}$ is given by:
        \begin{equation*}
            \nabla(G \circ F)(\mathbf{x}) = (D F(\mathbf{x}))^{*} \nabla G(F(\mathbf{x})),
        \end{equation*}
        if $F$ is differentiable at $\mathbf{x}$ and $G$ is differentiable at $F(\mathbf{x})$.
    \end{thm}
    \begin{proofing}
        For $\mathbf{y} \in H_{1}$, by the chain rule,
        \begin{align*}
            D(G \circ F)(\mathbf{x})(\mathbf{y}) &= D G(F(\mathbf{x}))(D F(\mathbf{x})(\mathbf{y}))\\
            &= \inprod{\nabla G(F(\mathbf{x}))}{D F(\mathbf{x})(\mathbf{y})}\\
            &= \inprod{(D F(\mathbf{x}))^{*} \nabla G(F(\mathbf{x}))}{\mathbf{y}}.
        \end{align*}
        Therefore, $\nabla(G \circ F)(\mathbf{x}) = (D F(\mathbf{x}))^{*} \nabla G(F(\mathbf{x}))$.
    \end{proofing}
    \begin{rem}
        If $G$ is linear and bounded, then there exists $\mathbf{a} \in H_{2}$ such that:
        \begin{align*}
            G(\mathbf{x}) &= \inprod{\mathbf{a}}{\mathbf{x}}, & \nabla G(\mathbf{x}) &= \mathbf{a}, \qquad \text{for } \mathbf{x} \in H_{2}.
        \end{align*}
        Therefore, $\nabla(G \circ F)(\mathbf{x}) = (D F(\mathbf{x}))^{*}\mathbf{a}$.
    \end{rem}
    \begin{rem}
        If $F$ is linear and bounded ($F \in \mathscr{L}(H_{1}, H_{2})$), then:
        \begin{equation*}
            D F(\mathbf{x}) = F, \qquad \text{for } \mathbf{x} \in H_{1}.
        \end{equation*}
        Therefore, $\nabla(G \circ F)(\mathbf{x}) = F^{*}\nabla G(F(\mathbf{x}))$.
    \end{rem}
    \newpage

    We can also derive the product rule for differentiation.
    \begin{thm}
        Let $H$ be a Hilbert space, and let $f_{1}, f_{2}: H \to \mathbb{R}$. If $f_{1}$ and $f_{2}$ are differentiable at $\mathbf{x} \in H$, then:
        \begin{equation*}
            \nabla(f_{1}f_{2})(\mathbf{x}) = f_{2}(\mathbf{x}) \cdot \nabla f_{1}(\mathbf{x}) + f_{1}(\mathbf{x}) \cdot \nabla f_{2}(\mathbf{x}).
        \end{equation*}
    \end{thm}
    \begin{proofing}
        Define $F: H \to \mathbb{R}^{2}$ and $G: \mathbb{R}^{2} \to \mathbb{R}$ by:
        \begin{align*}
            F(\mathbf{x}) &= \begin{pmatrix}
                f_{1}(\mathbf{x})\\
                f_{2}(\mathbf{x})
            \end{pmatrix}, \qquad \text{for } \mathbf{x} \in H, & G\left(\begin{pmatrix}
                \alpha\\
                \beta
            \end{pmatrix}\right) = \alpha\beta, \qquad \text{for } \alpha, \beta \in \mathbb{R}.
        \end{align*}
        Then, $f(\mathbf{x}) = G(F(\mathbf{x})) = (G \circ F)(\mathbf{x})$. For $D G$, for any $\alpha, \beta \in \mathbb{R}$,
        \begin{equation*}
            D G\left(\begin{pmatrix}
                \alpha\\
                \beta
            \end{pmatrix}\right)(\mathbf{y}) = \inprod{\nabla G\left(\begin{pmatrix}
                \alpha\\
                \beta
            \end{pmatrix}\right)}{\mathbf{y}} = \inprod{\begin{pmatrix}
                \beta\\
                \alpha
            \end{pmatrix}}{\mathbf{y}}.
        \end{equation*}
        For $D F$, since $f_{1}$ and $f_{2}$ are Fr\'echet differentiable at $\mathbf{x} \in H$, for any $\mathbf{z} \in H$,
        \begin{align*}
            f_{1}(\mathbf{z}) &= f_{1}(\mathbf{x}) + \inprod{\nabla f_{1}(\mathbf{x})}{\mathbf{z}-\mathbf{x}} + o(\norm{\mathbf{z}-\mathbf{x}}) = f_{1}(\mathbf{x}) + \inprod{\nabla f_{1}(\mathbf{x})}{\mathbf{z}-\mathbf{x}} + \varepsilon_{1},\\
            f_{2}(\mathbf{z}) &= f_{2}(\mathbf{x}) + \inprod{\nabla f_{2}(\mathbf{x})}{\mathbf{z}-\mathbf{x}} + o(\norm{\mathbf{z}-\mathbf{x}}) = f_{2}(\mathbf{x}) + \inprod{\nabla f_{2}(\mathbf{x})}{\mathbf{z}-\mathbf{x}} + \varepsilon_{2},\\
            F(\mathbf{z}) &= F(\mathbf{x}) + \begin{pmatrix}
                \inprod{\nabla f_{1}(\mathbf{x})}{\mathbf{z}-\mathbf{x}}\\
                \inprod{\nabla f_{2}(\mathbf{x})}{\mathbf{z}-\mathbf{x}}
            \end{pmatrix} + \begin{pmatrix}
                \varepsilon_{1}\\
                \varepsilon_{2}
            \end{pmatrix} = F(\mathbf{x}) + \begin{pmatrix}
                \inprod{\nabla f_{1}(\mathbf{x})}{\mathbf{z}-\mathbf{x}}\\
                \inprod{\nabla f_{2}(\mathbf{x})}{\mathbf{z}-\mathbf{x}}
            \end{pmatrix} + \boldsymbol{\varepsilon}.
        \end{align*}
        Since $\pnorm[2]{\boldsymbol{\varepsilon}} \leq \pnorm[1]{\boldsymbol{\varepsilon}} = \abs{\varepsilon_{1}} + \abs{\varepsilon_{2}} \sim o(\norm{\mathbf{z}-\mathbf{x}})$, for $\mathbf{y} \in H$,
        \begin{equation*}
            D F(\mathbf{x})(\mathbf{y}) = \begin{pmatrix}
                \inprod{\nabla f_{1}(\mathbf{x})}{\mathbf{y}}\\
                \inprod{\nabla f_{2}(\mathbf{x})}{\mathbf{y}}
            \end{pmatrix}.
        \end{equation*}
        By the chain rule, for $\mathbf{y} \in H$,
        \begin{align*}
            D(f_{1}f_{2})(\mathbf{x})(\mathbf{y}) &= D G(F(\mathbf{x})) \circ D F(\mathbf{x}) = \inprod{\begin{pmatrix}
                f_{2}(\mathbf{x})\\
                f_{1}(\mathbf{x})
            \end{pmatrix}}{\begin{pmatrix}
                \inprod{\nabla f_{1}(\mathbf{x})}{\mathbf{y}}\\
                \inprod{\nabla f_{2}(\mathbf{x})}{\mathbf{y}}
            \end{pmatrix}} = \inprod{f_{2}(\mathbf{x}) \cdot \nabla f_{1}(\mathbf{x}) + f_{1}(\mathbf{x}) \cdot \nabla f_{2}(\mathbf{x})}{\mathbf{y}},\\
            \nabla(f_{1}f_{2})(\mathbf{x}) &= f_{2}(\mathbf{x}) \cdot \nabla f_{1}(\mathbf{x}) + f_{1}(\mathbf{x}) \cdot \nabla f_{2}(\mathbf{x}).
        \end{align*}
    \end{proofing}
    \begin{thm}
        \label{Chapter 5 (Theorem) Jacobian Matrix}
        Let $F: \mathbb{R}^{n} \to \mathbb{R}^{m}$, where $\mathbb{R}^{n}$ and $\mathbb{R}^{m}$ are equipped with the standard inner product. Then:
        \begin{equation*}
            D F(\mathbf{x}) = \left(\pdv{(F(\mathbf{x}))_{i}}{x_{j}}\right)_{i=1,j=1}^{m,n} \in \mathbb{R}^{m \times n}.
        \end{equation*}
    \end{thm}
    \begin{proofing}
        Denote $(F(\mathbf{x}))_{i} = f_{i}(\mathbf{x}) \in \mathbb{R}$. By differentiability,
        \begin{equation*}
            f_{i}(\mathbf{y}) = f_{i}(\mathbf{x}) + \inprod{\nabla f_{i}(\mathbf{x})}{\mathbf{y}-\mathbf{x}} + \varepsilon_{i}, \qquad \text{for } \mathbf{y} \in \mathbb{R}^{n},
        \end{equation*}
        where $\abs{\varepsilon_{i}} = o(\pnorm[2]{\mathbf{y}-\mathbf{x}})$. Therefore,
        \begin{equation*}
            F(\mathbf{y}) = F(\mathbf{x}) + \begin{pmatrix}
                \inprod{\nabla f_{1}(\mathbf{x})}{\mathbf{y}-\mathbf{x}}\\
                \vdots\\
                \inprod{\nabla f_{m}(\mathbf{x})}{\mathbf{y}-\mathbf{x}}
            \end{pmatrix} + \begin{pmatrix}
                \varepsilon_{1}\\
                \vdots\\
                \varepsilon_{m}
            \end{pmatrix} = F(\mathbf{x}) + \begin{pmatrix}
                \inprod{\nabla f_{1}(\mathbf{x})}{\mathbf{y}-\mathbf{x}}\\
                \vdots\\
                \inprod{\nabla f_{m}(\mathbf{x})}{\mathbf{y}-\mathbf{x}}
            \end{pmatrix} + \boldsymbol{\varepsilon}.
        \end{equation*}
        Since $\pnorm[2]{\boldsymbol{\varepsilon}} \leq \pnorm[1]{\boldsymbol{\varepsilon}} \sim o(\pnorm[2]{\mathbf{y}-\mathbf{x}})$,
        \begin{align*}
            D F(\mathbf{x})(\mathbf{y}) &= \begin{pmatrix}
                \inprod{\nabla f_{1}(\mathbf{x})}{\mathbf{y}}\\
                \vdots\\
                \inprod{\nabla f_{m}(\mathbf{x})}{\mathbf{y}}
            \end{pmatrix} = \begin{pmatrix}
                (\nabla f_{1}(\mathbf{x}))^{T}\mathbf{y}\\
                \vdots\\
                (\nabla f_{m}(\mathbf{x}))^{T}\mathbf{y}
            \end{pmatrix} = \begin{pmatrix}
                (\nabla f_{1}(\mathbf{x}))^{T}\\
                \vdots\\
                (\nabla f_{m}(\mathbf{x}))^{T}
            \end{pmatrix}\mathbf{y}, & D F(\mathbf{x}) &= \begin{pmatrix}
                (\nabla f_{1}(\mathbf{x}))^{T}\\
                \vdots\\
                (\nabla f_{m}(\mathbf{x}))^{T}
            \end{pmatrix}.
        \end{align*}
    \end{proofing}
    \begin{rem}
        Differentiation is an extension of the Jacobian matrix.
    \end{rem}
    \newpage

    Can we extend the multiplication property to Banach spaces?
    \begin{thm}\named{Number Multiplication Rule}
        Let $V$ be a Banach space, and let $f_{1}, f_{2}: V \to \mathbb{R}$. If $f_{1}$ and $f_{2}$ are differentiable at $\mathbf{x} \in V$, then:
        \begin{equation*}
            D(f_{1}f_{2})(\mathbf{x})(\mathbf{y}) = f_{2}(\mathbf{x}) \cdot D f_{1}(\mathbf{x})(\mathbf{y}) + f_{1}(\mathbf{x}) \cdot D f_{2}(\mathbf{x})(\mathbf{y}), \qquad \text{for } \mathbf{y} \in V.
        \end{equation*}
    \end{thm}
    \begin{proofing}
        Since $f_{1}$ and $f_{2}$ are differentiable at $\mathbf{x}$, for any $\mathbf{h} \in V$,
        \begin{align*}
            f_{1}(\mathbf{x}+\mathbf{h}) &= f_{1}(\mathbf{x}) + D f_{1}(\mathbf{x})(\mathbf{h}) + \varepsilon_{1},\\
            f_{2}(\mathbf{x}+\mathbf{h}) &= f_{2}(\mathbf{x}) + D f_{2}(\mathbf{x})(\mathbf{h}) + \varepsilon_{2},
        \end{align*}
        where $\abs{\varepsilon_{i}} = o(\norm{\mathbf{h}})$. Expanding the product,
        \begin{align*}
            (f_{1}f_{2})(\mathbf{x}+\mathbf{h}) &= (f_{1}f_{2})(\mathbf{x}) + (f_{1}(\mathbf{x}) \cdot D f_{2}(\mathbf{x}) + f_{2}(\mathbf{x}) \cdot D f_{1}(\mathbf{x}))(\mathbf{h}) + (D f_{1}(\mathbf{x}) \cdot D f_{2}(\mathbf{x}))(\mathbf{h})\\
            & \qquad + \varepsilon_{1}(f_{2}(\mathbf{x}) + D f_{2}(\mathbf{x})(\mathbf{h})) + \varepsilon_{2}(f_{1}(\mathbf{x}) + D f_{1}(\mathbf{x})(\mathbf{h})) + \varepsilon_{1}\varepsilon_{2}.
        \end{align*}
        Let $R(\mathbf{h}) = (D f_{1}(\mathbf{x}) D f_{2}(\mathbf{x}))(\mathbf{h}) + \varepsilon_{1}(f_{2}(\mathbf{x}) + D f_{2}(\mathbf{x})(\mathbf{h})) + \varepsilon_{2}(f_{1}(\mathbf{x}) + D f_{1}(\mathbf{x})(\mathbf{h})) + \varepsilon_{1}\varepsilon_{2}$. As $\norm{\mathbf{h}} \to 0$,
        \begin{align*}
            \frac{\abs{(D f_{1}(\mathbf{x}) D f_{2}(\mathbf{x}))(\mathbf{h})}}{\norm{\mathbf{h}}} &\leq \frac{\norm{D f_{1}(\mathbf{x})}\norm{D f_{2}(\mathbf{x})}\norm{\mathbf{h}}^{2}}{\norm{\mathbf{h}}} = \norm{D f_{1}(\mathbf{x})}\norm{D f_{2}(\mathbf{x})}\norm{\mathbf{h}} \to 0,\\
            \tag{$\abs{\varepsilon_{1}} = o(\norm{\mathbf{h}})$}
            \frac{\abs{\varepsilon_{1}(f_{2}(\mathbf{x}) + D f_{2}(\mathbf{x})(\mathbf{h}))}}{\norm{\mathbf{h}}} &\leq \frac{\abs{\varepsilon_{1}}}{\norm{\mathbf{h}}}(\abs{f_{2}(\mathbf{x})}+\norm{D f_{2}(\mathbf{x})}\norm{\mathbf{h}}) \to 0,\\
            \tag{$\abs{\varepsilon_{2}} = o(\norm{\mathbf{h}})$}
            \frac{\abs{\varepsilon_{2}(f_{1}(\mathbf{x}) + D f_{1}(\mathbf{x})(\mathbf{h}))}}{\norm{\mathbf{h}}} &\leq \frac{\abs{\varepsilon_{2}}}{\norm{\mathbf{h}}}(\abs{f_{1}(\mathbf{x})}+\norm{D f_{1}(\mathbf{x})}\norm{\mathbf{h}}) \to 0,\\
            \tag{$\abs{\varepsilon_{1}\varepsilon_{2}} = o(\norm{\mathbf{h}}^{2})$}
            \abs{\varepsilon_{1}\varepsilon_{2}} &= o(\norm{\mathbf{h}}).
        \end{align*}
        Therefore, $\abs{R(\mathbf{h})} = o(\norm{\mathbf{h}})$. We have:
        \begin{equation*}
            (f_{1}f_{2})(\mathbf{x}+\mathbf{h}) = (f_{1}f_{2})(\mathbf{x}) + (f_{1}(\mathbf{x}) \cdot D f_{2}(\mathbf{x}) + f_{2}(\mathbf{x}) \cdot D f_{1}(\mathbf{x}))(\mathbf{h}) + o(\norm{\mathbf{h}}).
        \end{equation*}
        By definition, $D(f_{1}f_{2})(\mathbf{x})(\mathbf{y}) = f_{2}(\mathbf{x}) \cdot D f_{1}(\mathbf{x})(\mathbf{y}) + f_{1}(\mathbf{x}) \cdot D f_{2}(\mathbf{x})(\mathbf{y})$.
    \end{proofing}
    \newpage

    \begin{thm}\named{Scalar Multiplication Rule}
        Let $V$ be a Banach space. Let $f: V \to \mathbb{R}$ and $F: V \to V$. If $f$ and $F$ are differentiable at $\mathbf{x} \in V$, then:
        \begin{equation*}
            D(fF)(\mathbf{x})(\mathbf{y}) = f(\mathbf{x}) \cdot D F(\mathbf{x})(\mathbf{y}) + D f(\mathbf{x})(\mathbf{y}) F(\mathbf{x}), \qquad \text{for } \mathbf{y} \in V.
        \end{equation*}
    \end{thm}
    \begin{proofing}
        Define $G: V \to \mathbb{R} \times V$ and $\widetilde{F}: \mathbb{R} \times V \to V$ by:
        \begin{align*}
            G(\mathbf{x}) &= (f(\mathbf{x}), F(\mathbf{x})), & \text{for } & \mathbf{x} \in V,\\
            \widetilde{F}(\lambda, \mathbf{v}) &= \lambda\mathbf{v}, & \text{for } & \lambda \in \mathbb{R}, \mathbf{v} \in V,\\
            fF &= \widetilde{F} \circ G.
        \end{align*}
        We check the differentiability of $\widetilde{F}$. For $h \in \mathbb{R}$ and $\mathbf{k} \in V$, since $\widetilde{F}$ is bilinear,
        \begin{align*}
            \widetilde{F}(\lambda + h, \mathbf{v} + \mathbf{k}) &= \lambda\mathbf{v} + \lambda\mathbf{k} + h\mathbf{v} + h\mathbf{k}\\
            &= \widetilde{F}(\lambda, \mathbf{v}) + \lambda\mathbf{k} + h\mathbf{v}.
        \end{align*}
        We have that $\pnorm[V]{h\mathbf{v}} = \abs{h}\pnorm[V]{\mathbf{v}}$ and as $(h, \mathbf{v}) \to (0, \mathbf{0})$,
        \begin{align*}
            &\frac{\abs{h}\pnorm[V]{\mathbf{v}}}{\sqrt{\abs{h}^{2}+\pnorm[V]{\mathbf{v}}^{2}}} \leq \frac{\abs{h}\pnorm[V]{\mathbf{v}}}{\abs{h}} = \pnorm[V]{\mathbf{v}} \to 0,\\
            &\frac{\abs{h}\pnorm[V]{\mathbf{v}}}{\sqrt{\abs{h}^{2}+\pnorm[V]{\mathbf{v}}^{2}}} \leq \frac{\abs{h}\pnorm[V]{\mathbf{v}}}{\pnorm[V]{\mathbf{v}}} = \abs{h} \to 0,\\
            \implies& \frac{\abs{h}\pnorm[V]{\mathbf{v}}}{\sqrt{\abs{h}^{2}+\pnorm[V]{\mathbf{v}}^{2}}} \leq \min(\abs{h}, \pnorm[V]{\mathbf{v}}) \to 0.
        \end{align*}
        Thus, $\pnorm[V]{h\mathbf{v}} = o(\sqrt{h^{2} + \pnorm[V]{\mathbf{v}}^{2}})$ and:
        \begin{equation*}
            D \widetilde{F}(\lambda, \mathbf{v})(h,\mathbf{k}) = \lambda\mathbf{k} + h\mathbf{v}.
        \end{equation*}
        By the chain rule,
        \begin{align*}
            D(fF)(\mathbf{x})(\mathbf{y}) &= D \widetilde{F}(G(\mathbf{x}))(D G(\mathbf{x})(\mathbf{y}))\\
            &= D \widetilde{F}(f(\mathbf{x}), F(\mathbf{x}))(D f(\mathbf{x})(\mathbf{y}), D F(\mathbf{x})(\mathbf{y}))\\
            &= f(\mathbf{x}) \cdot D F(\mathbf{x})(\mathbf{y}) + D f(\mathbf{x})(\mathbf{y}) F(\mathbf{x}).
        \end{align*}
    \end{proofing}
    \begin{rem}
        You can also prove the number multiplication rule with a similar proof.
    \end{rem}
    \newpage

    \begin{thm}\named{Matrix Multiplication Rule}
        Let $V$ be a Banach space. Let $F: V \to \mathbb{R}^{m \times n}$ and $G: V \to \mathbb{R}^{n \times p}$. If $F$ and $G$ are differentiable at $\mathbf{x} \in V$, then:
        \begin{equation*}
            D(FG)(\mathbf{x})(\mathbf{y}) = D F(\mathbf{x})(\mathbf{y}) G(\mathbf{x}) + F(\mathbf{x}) D G(\mathbf{x})(\mathbf{y}), \qquad \text{for } \mathbf{y} \in V.
        \end{equation*}
    \end{thm}
    \begin{proofing}
        Define $T: V \to \mathbb{R}^{m \times n} \times \mathbb{R}^{n \times p}$ and $M: \mathbb{R}^{m \times n} \times \mathbb{R}^{n \times p} \to \mathbb{R}^{m \times p}$ by:
        \begin{align*}
            T(\mathbf{x}) &= (F(\mathbf{x}), G(\mathbf{x})), & \text{for } & \mathbf{x} \in V,\\
            M(\mathbf{A}, \mathbf{B}) &= \mathbf{A}\mathbf{B}, & \text{for } & \mathbf{A} \in \mathbb{R}^{m \times n}, \mathbf{B} \in \mathbb{R}^{n \times p},\\
            FG &= M \circ T.
        \end{align*}
        We check the differentiability of $M$. For $\mathbf{H} \in \mathbb{R}^{m \times n}$ and $\mathbf{K} \in \mathbb{R}^{n \times p}$, since $M$ is bilinear,
        \begin{align*}
            M(\mathbf{A}+\mathbf{H},\mathbf{B}+\mathbf{K}) &= \mathbf{A}\mathbf{B} + \mathbf{H}\mathbf{B} + \mathbf{A}\mathbf{K} + \mathbf{H}\mathbf{K}\\
            &= M(\mathbf{A}, \mathbf{B}) + \mathbf{H}\mathbf{B} + \mathbf{A}\mathbf{K} + \mathbf{H}\mathbf{K}.
        \end{align*}
        We have that $\norm{\mathbf{H}\mathbf{K}} \leq \norm{\mathbf{H}}\norm{\mathbf{K}}$ and as $(\mathbf{H}, \mathbf{K}) \to (\mathbf{O}, \mathbf{O})$,
        \begin{align*}
            &\frac{\norm{\mathbf{H}}\norm{\mathbf{K}}}{\sqrt{\norm{\mathbf{H}}^{2}+\norm{\mathbf{K}}^{2}}} \leq \frac{\norm{\mathbf{H}}\norm{\mathbf{K}}}{\norm{\mathbf{H}}} = \norm{\mathbf{K}} \to 0,\\
            &\frac{\norm{\mathbf{H}}\norm{\mathbf{K}}}{\sqrt{\norm{\mathbf{H}}^{2}+\norm{\mathbf{K}}^{2}}} \leq \frac{\norm{\mathbf{H}}\norm{\mathbf{K}}}{\norm{\mathbf{K}}} = \norm{\mathbf{H}} \to 0,\\
            \implies & \frac{\norm{\mathbf{H}}\norm{\mathbf{K}}}{\sqrt{\norm{\mathbf{H}}^{2}+\norm{\mathbf{K}}^{2}}} \leq \min(\norm{\mathbf{H}}, \norm{\mathbf{K}}) \to 0.
        \end{align*}
        Thus, $\norm{\mathbf{H}\mathbf{K}} = o(\sqrt{\norm{\mathbf{H}}^{2}+\norm{\mathbf{K}}^{2}})$ and:
        \begin{equation*}
            D M(\mathbf{A}, \mathbf{B})(\mathbf{H}, \mathbf{K}) = \mathbf{H}\mathbf{B} + \mathbf{A}\mathbf{K}.
        \end{equation*}
        By the chain rule,
        \begin{align*}
            D(FG)(\mathbf{x})(\mathbf{y}) &= D M(T(\mathbf{x}))(D T(\mathbf{x})(\mathbf{y}))\\
            &= D M(F(\mathbf{x}), G(\mathbf{x}))(D F(\mathbf{x})(\mathbf{y}), D G(\mathbf{x})(\mathbf{y}))\\
            &= D F(\mathbf{x})(\mathbf{y}) G(\mathbf{x}) + F(\mathbf{x}) D G(\mathbf{x})(\mathbf{y}).
        \end{align*}
    \end{proofing}
    \begin{rem}
        The matrix multiplication rule can also be applied to $\mathbb{R}$ and $\mathbb{R}^{n}$.
    \end{rem}
    \newpage

    \begin{thm}
        Let $H_{1},H_{2}$ be Hilbert spaces. Let $F, G: H_{1} \to H_{2}$ and define:
        \begin{equation*}
            \inprod{F}{G}(\mathbf{x}) = \inprod{F(\mathbf{x})}{G(\mathbf{x})}_{H_{2}}, \qquad \text{for } \mathbf{x} \in H_{1}.
        \end{equation*}
        If $F$ and $G$ are differentiable at $\mathbf{x} \in H_{1}$, then:
        \begin{equation*}
            \nabla \inprod{F}{G}(\mathbf{x})(\mathbf{y}) = (D F(\mathbf{x}))^{*} G(\mathbf{x}) + (D G(\mathbf{x}))^{*} F(\mathbf{x}).
        \end{equation*}
    \end{thm}
    \begin{proofing}
        Define $T: H_{1} \to H_{2} \times H_{2}$ and $M: H_{2} \times H_{2} \to \mathbb{R}$ by:
        \begin{align*}
            T(\mathbf{x}) &= (F(\mathbf{x}), G(\mathbf{x})), & \text{for } & \mathbf{x} \in H_{1},\\
            M(\mathbf{u}, \mathbf{v}) &= \inprod{\mathbf{u}}{\mathbf{v}}_{H_{2}}, & \text{for } & \mathbf{u}, \mathbf{v} \in H_{2},\\
            \inprod{F}{G} &= M \circ T.
        \end{align*}
        We check the differentiability of $M$. For $\mathbf{h}, \mathbf{k} \in H_{2}$, since $M$ is bilinear,
        \begin{equation*}
            M(\mathbf{u}+\mathbf{h}, \mathbf{v}+\mathbf{k}) = M(\mathbf{u}, \mathbf{v}) + \inprod{\mathbf{u}}{\mathbf{k}}_{H_{2}} + \inprod{\mathbf{h}}{\mathbf{v}}_{H_{2}} + \inprod{\mathbf{h}}{\mathbf{k}}_{H_{2}}.
        \end{equation*}
        By the Cauchy-Schwarz inequality, $\abs{\inprod{\mathbf{h}}{\mathbf{k}}}_{H_{2}} \leq \pnorm[H_{2}]{\mathbf{h}}\pnorm[H_{2}]{\mathbf{k}}$. As $(\mathbf{h}, \mathbf{k}) \to (\mathbf{0}, \mathbf{0})$,
        \begin{align*}
            &\frac{\pnorm[H_{2}]{\mathbf{h}}\pnorm[H_{2}]{\mathbf{k}}}{\sqrt{\pnorm[H_{2}]{\mathbf{h}}^{2}+\pnorm[H_{2}]{\mathbf{k}}^{2}}} \leq \frac{\pnorm[H_{2}]{\mathbf{h}}\pnorm[H_{2}]{\mathbf{k}}}{\pnorm[H_{2}]{\mathbf{h}}} = \pnorm[H_{2}]{\mathbf{k}} \to 0,\\
            &\frac{\pnorm[H_{2}]{\mathbf{h}}\pnorm[H_{2}]{\mathbf{k}}}{\sqrt{\pnorm[H_{2}]{\mathbf{h}}^{2}+\pnorm[H_{2}]{\mathbf{k}}^{2}}} \leq \pnorm[H_{2}]{\mathbf{h}} \to 0,\\
            \implies & \frac{\pnorm[H_{2}]{\mathbf{h}}\pnorm[H_{2}]{\mathbf{k}}}{\sqrt{\pnorm[H_{2}]{\mathbf{h}}^{2}+\pnorm[H_{2}]{\mathbf{k}}}} \leq \min(\pnorm[H_{2}]{\mathbf{h}}, \pnorm[H_{2}]{\mathbf{k}}) \to 0.
        \end{align*}
        As $(\mathbf{h}, \mathbf{k}) \to (\mathbf{0}, \mathbf{0})$, $\min(\pnorm[H_{2}]{\mathbf{h}}, \pnorm[H_{2}]{\mathbf{k}}) \to 0$. Thus, $\abs{\inprod{\mathbf{h}}{\mathbf{k}}}_{H_{2}} = o(\sqrt{\pnorm[H_{2}]{\mathbf{h}}^{2}+\pnorm[H_{2}]{\mathbf{k}}})$ and:
        \begin{equation*}
            D M(\mathbf{u}, \mathbf{v})(\mathbf{h}, \mathbf{k}) = \inprod{\mathbf{u}}{\mathbf{k}}_{H_{2}} + \inprod{\mathbf{h}}{\mathbf{v}}_{H_{2}}.
        \end{equation*}
        By the chain rule,
        \begin{align*}
            D \inprod{F}{G}(\mathbf{x})(\mathbf{y}) &=  D M(F(\mathbf{x}), G(\mathbf{x}))(D F(\mathbf{x})(\mathbf{y}), D G(\mathbf{x})(\mathbf{y}))\\
            &= \inprod{F(\mathbf{x})}{D G(\mathbf{x})(\mathbf{y})}_{H_{2}} + \inprod{D F(\mathbf{x})(\mathbf{y})}{G(\mathbf{x})}_{H_{2}}\\
            &= \inprod{(D G(\mathbf{x}))^{*} F(\mathbf{x}) + (D F(\mathbf{x}))^{*} G(\mathbf{x})}{\mathbf{y}}_{H_{1}}.
        \end{align*}
        Therefore, $\nabla \inprod{F}{G}(\mathbf{x}) = (D G(\mathbf{x}))^{*} F(\mathbf{x}) + (D F(\mathbf{x}))^{*} G(\mathbf{x})$.
    \end{proofing}
    \begin{rem}
        There are other bilinear mappings that satisfy a similar differentiation rule, e.g., convolution.
    \end{rem}
    \newpage
    
\section{Hessian of functions}
    Similar to standard derivatives, what are the second-order derivatives of the Fr\'echet differentiation?

    Let $H$ be a Hilbert space, and let $f: H \to \mathbb{R}$. We can consider $\nabla f$ as a mapping $\nabla f: H \to H$.
    \begin{defn}
        Let $H$ be a Hilbert space, and $f: H \to \mathbb{R}$. The \textbf{Hessian} of $f$ is defined as:
        \begin{equation*}
            \nabla^{2} f(\mathbf{x}) = D(\nabla f)(\mathbf{x}), \qquad \text{for } \mathbf{x} \in H.
        \end{equation*}
    \end{defn}
    \begin{rem}
        $\nabla^{2} f \in \mathscr{L}(H, H)$.
    \end{rem}
    \begin{rem}
        By Theorem \ref{Chapter 4 (Theorem) Elementwise Gradient} and \ref{Chapter 5 (Theorem) Jacobian Matrix}, for $f: \mathbb{R}^{n} \to \mathbb{R}$,
        \begin{equation*}
            \nabla^{2} f(\mathbf{x}) = D(\nabla f)(\mathbf{x}) = \left(\pdv{f}{x_{i},x_{j}}\right)_{i=1,j=1}^{n,n} \in \mathbb{R}^{n \times n}.
        \end{equation*}
    \end{rem}
    \begin{eg}
        Let $\mathbf{A} \in \mathscr{L}(H_{1}, H_{2})$, where $H_{1}, H_{2}$ are Hilbert spaces. Let $f: H_{1} \to \mathbb{R}$ be defined as:
        \begin{equation*}
            f(\mathbf{x}) = \frac{1}{2}\pnorm[H_{2}]{\mathbf{A}\mathbf{x}-\mathbf{b}}^{2}, \qquad \text{for } \mathbf{x} \in H_{1},
        \end{equation*}
        where $\mathbf{b} \in H_{2}$. We have found that $\nabla f(\mathbf{x}) = \mathbf{A}^{*}(\mathbf{A}\mathbf{x} - \mathbf{b}) = \mathbf{A}^{*}\mathbf{A}\mathbf{x} - \mathbf{A}^{*}\mathbf{b}$. Then,
        \begin{equation*}
            \nabla^{2} f(\mathbf{x}) = D(\nabla f)(\mathbf{x}) = \mathbf{A}^{*}\mathbf{A}.
        \end{equation*}
    \end{eg}
    \begin{eg}
        Let $\mathbf{A} \in \mathscr{L}(H_{1}, H_{2})$, where $H_{1}, H_{2}$ are Hilbert spaces. Let $f: H_{1} \to \mathbb{R}$ and $F: H_{2} \to \mathbb{R}$ be defined as:
        \begin{equation*}
            f(\mathbf{x}) = F(\mathbf{A}\mathbf{x}), \text{for } \mathbf{x} \in H_{1}.
        \end{equation*}
        By the chain rule,
        \begin{equation*}
            \nabla f(\mathbf{x}) = \mathbf{A}^{*}\nabla F(\mathbf{A}\mathbf{x}).
        \end{equation*}
        We have that for $\mathbf{y} \in H_{1}$,
        \begin{align*}
            D \mathbf{A}^{*}(\nabla F(\mathbf{A}\mathbf{x})) &= \mathbf{A}^{*},\\
            D(\nabla F)(\mathbf{A}\mathbf{x}) &= \nabla^{2} F(\mathbf{A}\mathbf{x}),\\
            D \mathbf{A}(\mathbf{x})(\mathbf{y}) &= \mathbf{A}\mathbf{y},\\
            \nabla^{2} f(\mathbf{x})(\mathbf{y}) = D(\nabla f)(\mathbf{x})(\mathbf{y}) &= D(\mathbf{A}^{*}\nabla F(\mathbf{A}\mathbf{x}))(\mathbf{y})\\
            &= D \mathbf{A}^{*}(\nabla F(\mathbf{A}\mathbf{x}))(D(\nabla F)(\mathbf{A}\mathbf{x})(D \mathbf{A}(\mathbf{x})(\mathbf{y})))\\
            &= \mathbf{A}^{*} \nabla^{2} F(\mathbf{A}\mathbf{x})\mathbf{A}\mathbf{y}.
        \end{align*}
    \end{eg}
    \begin{eg}
        Let $f: \mathbb{R}^{n} \to \mathbb{R}$ be defined as:
        \begin{equation*}
            f(\mathbf{x}) = \sum_{i=1}^{m} f_{i}(\inprod{\mathbf{a}_{i}}{\mathbf{x}}),
        \end{equation*}
        where $\mathbf{a}_{i} \in \mathbb{R}^{n}$, $\mathbf{x} \in \mathbb{R}^{n}$, and $f_{i}: \mathbb{R} \to \mathbb{R}$. How do we find $\nabla^{2} f(\mathbf{x})$? Let:
        \begin{align*}
            \mathbf{A} &= \begin{pmatrix}
                \mathbf{a}_{1}^{T}\\
                \vdots\\
                \mathbf{a}_{m}^{T}
            \end{pmatrix}, & F(\mathbf{y}) &= \sum_{i=1}^{m} f_{i}(y_{i}), \qquad \text{for } \mathbf{y} \in \mathbb{R}^{m}.
        \end{align*}
        Then we have $f(\mathbf{x}) = F(\mathbf{A}\mathbf{x})$. Therefore,
        \begin{align*}
            \nabla^{2} f(\mathbf{x}) &= \mathbf{A}^{T}\nabla F(\mathbf{A}\mathbf{x}) = \sum_{i=1}^{m}f_{i}'(\mathbf{a}_{i}^{T}\mathbf{x})\mathbf{a}_{i}, & \nabla^{2} f(\mathbf{x}) &= \mathbf{A}^{T} \nabla^{2} F(\mathbf{A}\mathbf{x})\mathbf{A} = \sum_{i=1}^{m} f_{i}''(\mathbf{a}_{i}^{T}\mathbf{x})\mathbf{a}_{i}\mathbf{a}_{i}^{T}.
        \end{align*}
    \end{eg}
    \newpage

    \begin{eg}
        Let $H$ be a Hilbert space, and $f(\mathbf{x}) = \norm{\mathbf{x}}$, where $\mathbf{x} \in H$. What is $\nabla^{2} f(\mathbf{x})$? We know that when $\mathbf{x} \neq \mathbf{0}$,
        \begin{equation*}
            \nabla f(\mathbf{x}) = \frac{\mathbf{x}}{\norm{\mathbf{x}}}.
        \end{equation*}
        Let $g: V \to \mathbb{R}$ and $I: V \to V$ be defined as:
        \begin{align*}
            g(\mathbf{x}) &= \frac{1}{\norm{\mathbf{x}}}, & I(\mathbf{x}) &= \mathbf{x}, & \nabla f &= gI.
        \end{align*}
        Let $g_{1}(t) = \frac{1}{\sqrt{t}}$ and $g_{2}(\mathbf{x}) = \norm{\mathbf{x}}^{2}$. Then $g = g_{1} \circ g_{2}$. We have:
        \begin{align*}
            g_{1}'(t) &= -\frac{1}{2}t^{-\frac{3}{2}}, & \nabla g_{2}(\mathbf{x}) &= 2\mathbf{x}.
        \end{align*}
        Therefore, by the chain rule, since $g_{1}$ is continuous for $t>0$ and $\mathbf{x} \neq \mathbf{0}$,
        \begin{equation*}
            \nabla g(\mathbf{x}) = g_{1}'(g_{2}(\mathbf{x})) \cdot \nabla g_{2}(\mathbf{x}) = -\frac{\mathbf{x}}{\norm{\mathbf{x}}^{3}}.
        \end{equation*}
        By the scalar multiplication rule,
        \begin{align*}
            \nabla^{2} f(\mathbf{x})(\mathbf{y}) &= D(\nabla f)(\mathbf{x})(\mathbf{y})\\
            &= D g(\mathbf{x})(\mathbf{y}) \cdot I(\mathbf{x}) + g(\mathbf{x}) \cdot D I(\mathbf{x})(\mathbf{y})\\
            &= \inprod{-\frac{1}{\norm{\mathbf{x}}^{3}}\mathbf{x}}{\mathbf{y}}\mathbf{x} + \frac{\mathbf{y}}{\norm{\mathbf{x}}}.
        \end{align*}
        If $H = \mathbb{R}^{n}$, then:
        \begin{align*}
            \nabla^{2} f(\mathbf{x})(\mathbf{y}) &= -\frac{\mathbf{x}\mathbf{x}^{T}\mathbf{y}}{\norm{\mathbf{x}}^{3}} + \frac{\mathbf{y}}{\norm{\mathbf{x}}}, & \nabla^{2} f(\mathbf{x}) &= -\frac{1}{\norm{\mathbf{x}}^{3}}\mathbf{x}\mathbf{x}^{T} + \frac{1}{\norm{\mathbf{x}}}\mathbf{I}.
        \end{align*}
    \end{eg}
    \newpage

\section{Function Expansion}
    Let $f: H \to \mathbb{R}$ be a differentiable function, where $H$ is a Hilbert space. From the definition of the gradient at $\mathbf{x}$,
    \begin{equation*}
        f(\mathbf{y}) = f(\mathbf{x}) + \inprod{\nabla f(\mathbf{x})}{\mathbf{y}-\mathbf{x}} + o(\norm{\mathbf{y}-\mathbf{x}}).
    \end{equation*}
    We aim to derive the expansion up to the second-order derivative. Consider:
    \begin{equation*}
        g(t) = f(\mathbf{x} + t\mathbf{u}),
    \end{equation*}
    where $\mathbf{x}, \mathbf{u} \in H$ are given, and $t \in \mathbb{R}$. What is the directional derivative?
    \begin{thm}\named{Theorem \ref{Chapter 4 (Theorem) Derivative on not t=0}}
        \label{Chapter 5 (Theorem) Derivative on not t=0}
        Let $H$ be a Hilbert space. Let $f: H \to \mathbb{R}$ and $\mathbf{x} \in H$. For any $\mathbf{u} \in H$,
        \begin{equation*}
            \odv*{f(\mathbf{x} + t\mathbf{u})}{t} = \inprod{\nabla f(\mathbf{x} + t\mathbf{u})}{\mathbf{u}},
        \end{equation*}
        if $f$ is differentiable at $\mathbf{x} + t\mathbf{u}$.
    \end{thm}
    \begin{proofing}[Alternative Proof]
        Let $G: \mathbb{R} \to H$ be defined as:
        \begin{align*}
            G(t) &= \mathbf{x} + t\mathbf{u}, & f(\mathbf{x} + t\mathbf{u}) &= f(G(t)).
        \end{align*}
        Since $D G(t)(s) = s\mathbf{u}$ for $s \in \mathbb{R}$ and $D f(\mathbf{z})(\mathbf{y}) = \inprod{\nabla f(\mathbf{z})}{\mathbf{y}}$ for $\mathbf{y} \in H$, by the chain rule,
        \begin{equation*}
            \odv*{f(\mathbf{x} + t\mathbf{u})}{t}(s) = D f(G(t))(D G(t)(s)) = \inprod{\nabla f(\mathbf{x} + t\mathbf{u})}{s\mathbf{u}} = s\inprod{\nabla f(\mathbf{x} + t\mathbf{u})}{\mathbf{u}}.
        \end{equation*}
        Therefore, $\odv*{f(\mathbf{x} + t\mathbf{u})}{t} = \inprod{\nabla f(\mathbf{x} + t\mathbf{u})}{\mathbf{u}}$.
    \end{proofing}
    \begin{cor}
        \label{Chapter 5 (Corollary) Double Derivative on not t=0}
        Let $H$ be a Hilbert space, and let $f: H \to \mathbb{R}$. Let $f$ and $\nabla f$ be differentiable at $\mathbf{x} \in H$. For any $\mathbf{u} \in H$,
        \begin{equation*}
            \left.\odv*[order=2]{f(\mathbf{x} + t\mathbf{u})}{t}\right|_{t=0} = \inprod{\nabla^{2} f(\mathbf{x})(\mathbf{u})}{\mathbf{u}}.
        \end{equation*}
    \end{cor}
    \begin{proofing}
        By Theorem \ref{Chapter 5 (Theorem) Derivative on not t=0},
        \begin{equation*}
            \odv*[order=2]{f(\mathbf{x} + t\mathbf{u})}{t} = \odv*{\inprod{\nabla f(\mathbf{x} + t\mathbf{u})}{\mathbf{u}}}{t} = \inprod{\odv*{\nabla f(\mathbf{x} + t\mathbf{u})}{t}}{\mathbf{u}} = \inprod{\nabla^{2} f(\mathbf{x} + t\mathbf{u})(\mathbf{u})}{\mathbf{u}}.
        \end{equation*}
        Setting $t=0$,
        \begin{equation*}
            \left.\odv*[order=2]{f(\mathbf{x} + t\mathbf{u})}{t}\right|_{t=0} = \inprod{\nabla^{2} f(\mathbf{x})(\mathbf{u})}{\mathbf{u}}.
        \end{equation*}
    \end{proofing}
    \newpage

    \begin{thm}
        Let $H$ be a Hilbert space, and let $f: H \to \mathbb{R}$. Let $f$ and $\nabla f$ be differentiable at $\mathbf{x} \in H$. For any $\mathbf{u}, \mathbf{v} \in H$,
        \begin{equation*}
            \left.\pdv*{f(\mathbf{x} + s\mathbf{u} + t\mathbf{v})}{s,t}\right|_{s=t=0} = \inprod{\nabla^{2} f(\mathbf{x})(\mathbf{v})}{\mathbf{u}} = \inprod{\nabla^{2} f(\mathbf{x})(\mathbf{u})}{\mathbf{v}}.
        \end{equation*}
    \end{thm}
    \begin{proofing}
        By Theorem \ref{Chapter 5 (Theorem) Derivative on not t=0},
        \begin{equation*}
            \pdv*{f(\mathbf{x} + s\mathbf{u} + t\mathbf{v})}{s,t} = \pdv*{\inprod{\nabla f(\mathbf{x} + s\mathbf{u} + t\mathbf{v})}{\mathbf{u}}}{t} = \inprod{\pdv*{\nabla f(\mathbf{x} + s\mathbf{u} + t\mathbf{v})}{t}}{\mathbf{u}} = \inprod{\nabla^{2} f(\mathbf{x} + s\mathbf{u} + t\mathbf{v})(\mathbf{v})}{\mathbf{u}}.
        \end{equation*}
        Similarly, 
        \begin{equation*}
            \pdv*{f(\mathbf{x} + s\mathbf{u} + t\mathbf{v})}{s,t} = \pdv*{\inprod{\nabla f(\mathbf{x} + s\mathbf{u} + t\mathbf{v})}{\mathbf{v}}}{s} = \inprod{\pdv*{\nabla f(\mathbf{x} + s\mathbf{u} + t\mathbf{v})}{s}}{\mathbf{v}} = \inprod{\nabla^{2} f(\mathbf{x} + s\mathbf{u} + t\mathbf{v})(\mathbf{u})}{\mathbf{v}}.
        \end{equation*}
        Setting $s=t=0$,
        \begin{equation*}
            \left.\pdv*{f(\mathbf{x} + s\mathbf{u} + t\mathbf{v})}{s,t}\right|_{s=t=0} = \inprod{\nabla^{2} f(\mathbf{x})(\mathbf{v})}{\mathbf{u}} = \inprod{\nabla^{2} f(\mathbf{x})(\mathbf{u})}{\mathbf{v}}.
        \end{equation*}
    \end{proofing}
    \begin{rem}
        This shows that $(\nabla^{2} f(\mathbf{x}))^{*} = \nabla^{2} f(\mathbf{x})$ (self-adjoint).
    \end{rem}

    We can present the second-order expansion of $f: H \to \mathbb{R}$.
    \begin{thm}
        Let $H$ be a Hilbert space, and let $f: H \to \mathbb{R}$. Assume that $f$ and $\nabla f$ are differentiable at $\mathbf{x} \in H$. The \textbf{second-order Taylor expansion} of $f$ is:
        \begin{equation*}
            f(\mathbf{y}) = f(\mathbf{x}) + \inprod{\nabla f(\mathbf{x})}{\mathbf{y}-\mathbf{x}} + \frac{1}{2}\inprod{\nabla^{2}f(\mathbf{x})(\mathbf{y}-\mathbf{x})}{\mathbf{y}-\mathbf{x}} + o(\norm{\mathbf{y}-\mathbf{x}}^{2}), \qquad \text{for } \mathbf{y} \in H.
        \end{equation*}
    \end{thm}
    \begin{proofing}
        Let $g(t) = f(\mathbf{x} + t\mathbf{u})$, where $\mathbf{x}, \mathbf{u} \in H$, $t \in \mathbb{R}$. By the Taylor expansion on $g(t)$,
        \begin{align*}
            g(t) = g(t_{0}) + g'(t_{0})(t-t_{0}) + \frac{1}{2}g''(t_{0})(t-t_{0})^{2} + o(\abs{t-t_{0}}^{2}).
        \end{align*}
        By Theorem \ref{Chapter 5 (Theorem) Derivative on not t=0} and Corollary \ref{Chapter 5 (Corollary) Double Derivative on not t=0},
        \begin{equation*}
            f(\mathbf{x} + t\mathbf{u}) = f(\mathbf{x} + t_{0}\mathbf{u}) + \inprod{\nabla f(\mathbf{x} + t_{0}\mathbf{u})}{\mathbf{u}}(t-t_{0}) + \frac{1}{2}\inprod{\nabla^{2} f(\mathbf{x} + t_{0}\mathbf{u})(\mathbf{u})}{\mathbf{u}}(t-t_{0})^{2} + o(\abs{t-t_{0}})^{2}.
        \end{equation*}
        For all $\mathbf{x} \in H$, we can choose:
        \begin{align*}
            \mathbf{u} &= \frac{\mathbf{y}-\mathbf{x}}{\norm{\mathbf{y}-\mathbf{x}}}, & t &= \norm{\mathbf{y}-\mathbf{x}}, & t_{0} &= 0, & \mathbf{x} + t\mathbf{u} &= \mathbf{x} + \norm{\mathbf{y}-\mathbf{x}}\left(\frac{\mathbf{y}-\mathbf{x}}{\norm{\mathbf{y}-\mathbf{x}}}\right) = \mathbf{y}.
        \end{align*}
        Therefore, the second-order Taylor expansion is:
        \begin{equation*}
            f(\mathbf{y}) = f(\mathbf{x}) + \inprod{\nabla f(\mathbf{x})}{\mathbf{y}-\mathbf{x}} + \frac{1}{2}\inprod{\nabla^{2}f(\mathbf{x})(\mathbf{y}-\mathbf{x})}{\mathbf{y}-\mathbf{x}} + o(\norm{\mathbf{y}-\mathbf{x}}^{2}).
        \end{equation*}
    \end{proofing}
    \newpage

\section{Matrix Differentiation}
    It is straightforward to compute the derivatives of vectors in Euclidean space. However, in many machine learning tasks, matrix differentiation is also required for operations such as backpropagation. Assume we have a Hilbert space $\mathbb{R}^{m \times n}$ with the inner product defined as:
    \begin{equation*}
        \inprod{\mathbf{X}}{\mathbf{Y}} = \sum_{i=1}^{m}\sum_{j=1}^{n}x_{ij}y_{ij} = \Tr(\mathbf{X}^{T}\mathbf{Y}) = \Tr(\mathbf{Y}^{T}\mathbf{X}), \qquad \text{for } \mathbf{X}, \mathbf{Y} \in \mathbb{R}^{m \times n}.
    \end{equation*}
    The induced norm is defined as:
    \begin{equation*}
        \pnorm[F]{\mathbf{X}} = \sqrt{\inprod{\mathbf{X}}{\mathbf{X}}} = \sqrt{\sum_{i=1}^{m}\sum_{j=1}^{n}x_{ij}^{2}}.
    \end{equation*}
    \begin{eg}\named{Linear matrix multiplications}
        Let $\mathbf{A} \in \mathbb{R}^{p \times m}$ and $\mathbf{B} \in \mathbb{R}^{n \times q}$. Consider the mapping $F: \mathbb{R}^{m \times n} \to \mathbb{R}^{p \times q}$ defined as:
        \begin{equation*}
            F(\mathbf{X}) = \mathbf{A}\mathbf{X}\mathbf{B}, \qquad \text{for } \mathbf{X} \in \mathbb{R}^{m \times n}.
        \end{equation*}
        Since $F$ is linear, by Theorem \ref{Chapter 5 (Theorem) Differentiation of Linear Map}, for $\mathbf{X}, \mathbf{Y} \in \mathbb{R}^{m \times n}$,
        \begin{equation*}
            D F(\mathbf{X})(\mathbf{Y}) = \mathbf{A}\mathbf{Y}\mathbf{B}.
        \end{equation*}
    \end{eg}
    \begin{eg}\named{Quadratic matrix multiplications}
        Let $\mathbf{A} \in \mathbb{R}^{n \times n}$ be given. Consider the mapping $F: \mathbb{R}^{n \times n} \to \mathbb{R}^{n \times n}$ defined as:
        \begin{equation*}
            F(\mathbf{X}) = \mathbf{X}\mathbf{A}\mathbf{X}, \qquad \text{for } \mathbf{X} \in \mathbb{R}^{n \times n}.
        \end{equation*}
        By the matrix multiplication rule, for $\mathbf{X}, \mathbf{Y} \in \mathbb{R}^{n \times n}$,
        \begin{align*}
            D F(\mathbf{X})(\mathbf{Y}) &= D(\mathbf{X})(\mathbf{Y}) \mathbf{A}\mathbf{X} + \mathbf{X} D(\mathbf{A}\mathbf{X})(\mathbf{Y})\\
            &= \mathbf{Y}\mathbf{A}\mathbf{X} + \mathbf{X}\mathbf{A}\mathbf{Y}.
        \end{align*}
    \end{eg}
    \begin{eg}\named{Transpose}
        Consider the mapping $F: \mathbb{R}^{m \times n} \to \mathbb{R}^{n \times m}$ defined as:
        \begin{equation*}
            F(\mathbf{X}) = \mathbf{X}^{T}.
        \end{equation*}
        Since $F$ is linear, by Theorem \ref{Chapter 5 (Theorem) Differentiation of Linear Map}, for $\mathbf{X}, \mathbf{Y} \in \mathbb{R}^{m \times n}$,
        \begin{equation*}
            D F(\mathbf{X})(\mathbf{Y}) = \mathbf{Y}^{T}.
        \end{equation*}
    \end{eg}
    \begin{eg}\named{Inversion}
        Consider the mapping $F: \mathbb{R}^{n \times n} \to \mathbb{R}^{n \times n}$ defined as:
        \begin{equation*}
            F(\mathbf{X}) = \mathbf{X}^{-1}, \qquad \text{for } \mathbf{X} \in \mathbb{R}^{n \times n} \text{ and invertible}.
        \end{equation*}
        Note that $\mathbf{X}\mathbf{X}^{-1} = \mathbf{I}$. By the matrix multiplication rule, for invertible $\mathbf{X}, \mathbf{Y} \in \mathbb{R}^{n \times n}$,
        \begin{align*}
            \mathbf{O} = D(\mathbf{I})(\mathbf{Y}) &= D(\mathbf{X})(\mathbf{Y}) \mathbf{X}^{-1} + \mathbf{X} D(\mathbf{X}^{-1})(\mathbf{Y})\\
            \mathbf{O} &= \mathbf{Y}\mathbf{X}^{-1} + \mathbf{X} D(\mathbf{X}^{-1})(\mathbf{Y})\\
            -\mathbf{X}^{-1}\mathbf{Y}\mathbf{X}^{-1} &= D(\mathbf{X}^{-1})(\mathbf{Y}).
        \end{align*}
        Therefore, $D F(\mathbf{X})(\mathbf{Y}) = -\mathbf{X}^{-1}\mathbf{Y}\mathbf{X}^{-1}$ for invertible $\mathbf{X}, \mathbf{Y} \in \mathbb{R}^{n \times n}$.
    \end{eg}
    \begin{eg}\named{Trace}
        Consider the mapping $\Tr: \mathbb{R}^{n \times n} \to \mathbb{R}$ defined as:
        \begin{equation*}
            \Tr(\mathbf{X}) = \sum_{i=1}^{n} \lambda_{i}(\mathbf{X}), \qquad \text{for } \mathbf{X} \in \mathbb{R}^{n \times n},
        \end{equation*}
        where $\lambda_{i}(\mathbf{X})$ is the $i$-th eigenvalue of $\mathbf{X}$. Since $\Tr(\mathbf{X}) = \inprod{\mathbf{X}}{\mathbf{I}}$, it is linear. By Theorem \ref{Chapter 5 (Theorem) Differentiation of Linear Map}, for $\mathbf{X}, \mathbf{Y} \in \mathbb{R}^{n \times n}$,
        \begin{equation*}
            D \Tr(\mathbf{X})(\mathbf{Y}) = \inprod{\mathbf{Y}}{\mathbf{I}}.
        \end{equation*}
        Thus, $\nabla \Tr(\mathbf{X}) = \mathbf{I}$.
    \end{eg}
    \newpage

    \begin{thm}
        \label{Chapter 5 (Theorem) Jacobi's Formula First Lemma}
        For any $\mathbf{Y} \in \mathbb{R}^{n \times n}$,
        \begin{equation*}
            D F(\mathbf{I})(\mathbf{Y}) = \Tr(\mathbf{Y}).
        \end{equation*}
    \end{thm}
    \begin{proofing}
        We may find that:
        \begin{equation*}
            \det(\mathbf{X}) = \prod_{i=1}^{n}\lambda_{i}(\mathbf{X}), \qquad \text{for } \mathbf{X} \in \mathbb{R}^{n \times n},
        \end{equation*}
        where $\lambda_{i}(\mathbf{X})$ is the $i$-th eigenvalue of $\mathbf{X}$. For $\mathbf{Y} \in \mathbb{R}^{n \times n}$,
        \begin{align*}
            \det(\mathbf{I} + \mathbf{Y}) &= \prod_{i=1}^{n}(1 + \lambda_{i}(\mathbf{Y}))\\
            &= \prod_{i=1}^{n}1 + \sum_{i=1}^{n}\lambda_{i}(\mathbf{Y}) + \sum_{i<j}\lambda_{i}(\mathbf{Y})\lambda_{j}(\mathbf{Y}) + \sum_{i<j<k}\lambda_{i}(\mathbf{Y})\lambda_{j}(\mathbf{Y})\lambda_{k}(\mathbf{Y}) + \cdots\\
            &= \det(\mathbf{I}) + \Tr(\mathbf{Y}) + \sum_{i<j}\lambda_{i}(\mathbf{Y})\lambda_{j}(\mathbf{Y}) + \sum_{i<j<k}\lambda_{i}(\mathbf{Y})\lambda_{j}(\mathbf{Y})\lambda_{k}(\mathbf{Y}) + \cdots.
        \end{align*}
        Therefore, by using the fact that $\max_{i}\abs{\lambda_{i}(\mathbf{Y})} \leq \pnorm[2]{\mathbf{Y}} \leq \pnorm[F]{\mathbf{Y}}$,
        \begin{align*}
            0 &\leq \lim_{\pnorm[F]{\mathbf{Y}} \to 0} \frac{\abs{\det(\mathbf{I} + \mathbf{Y}) - (\det(\mathbf{I}) + \Tr(\mathbf{Y}))}}{\pnorm[F]{\mathbf{Y}}}\\
            &\leq \lim_{\pnorm[F]{\mathbf{Y}} \to 0} \frac{\sum_{i<j}\abs{\lambda_{i}(\mathbf{Y})\lambda_{j}(\mathbf{Y})} + \sum_{i<j<k}\abs{\lambda_{i}(\mathbf{Y})\lambda_{j}(\mathbf{Y})\lambda_{k}(\mathbf{Y})} + \cdots}{\pnorm[F]{\mathbf{Y}}}\\
            &\leq \lim_{\pnorm[F]{\mathbf{Y}} \to 0} \frac{\sum_{i<j}\pnorm[F]{\mathbf{Y}}^{2} + \sum_{i<j<k}\pnorm[F]{\mathbf{Y}}^{3} + \cdots}{\pnorm[F]{\mathbf{Y}}} = 0.
        \end{align*}
        Therefore, by the Squeeze Theorem, $D \det(\mathbf{I})(\mathbf{Y}) = \Tr(\mathbf{Y})$.
    \end{proofing}
    \begin{eg}\named{Determinant}
        \label{Chapter 5 (Example) Jacobi's Formula Second Lemma}
        Consider the mapping $\det: \mathbb{R}^{n \times n} \to \mathbb{R}$ be defined as:
        \begin{equation*}
        	\det(\mathbf{X}) = \prod_{i=1}^{n}\lambda_{i}(\mathbf{X}), \qquad \text{for } \mathbf{X} \in \mathbb{R}^{n \times n},
        \end{equation*}
        where $\lambda_{i}(\mathbf{X})$ is the $i$-th eigenvalue of $\mathbf{X}$. For any invertible $\mathbf{X}, \mathbf{Z} \in \mathbb{R}^{n \times n}$,
        \begin{align*}
            \tag{Theorem \ref{Chapter 5 (Theorem) Derivative on not t=0}}
            D \det(\mathbf{X})(\mathbf{Z}) &= \left.\odv*{\det(\mathbf{X} + t\mathbf{Z})}{t}\right|_{t=0}\\
            &= \det(\mathbf{X}) \left.\odv*{\det(\mathbf{I} + t\mathbf{X}^{-1}\mathbf{Z})}{t}\right|_{t=0}\\
            &= \det(\mathbf{X}) D \det(\mathbf{I})(\mathbf{X}^{-1}\mathbf{Z})\\
            \tag{Theorem \ref{Chapter 5 (Theorem) Jacobi's Formula First Lemma}}
            &= \det(\mathbf{X})\Tr(\mathbf{X}^{-1}\mathbf{Z})\\
            &= \inprod{\det(\mathbf{X})(\mathbf{X}^{-1})^{T}}{\mathbf{Z}}.
        \end{align*}
        Therefore, $D \det(\mathbf{X})(\mathbf{Y}) = \det(\mathbf{X})\Tr(\mathbf{X}^{-1}\mathbf{Y})$ and $\nabla \det(\mathbf{X}) = \det(\mathbf{X})(\mathbf{X}^{-1})^{T}$.
    \end{eg}
    \begin{thm}\named{Jacobi's Formula}
        Let $f: \mathbb{R} \to \mathbb{R}$ be defined as:
        \begin{equation*}
            f(t) = \det(\mathbf{A}(t)), \qquad \text{for } t \in \mathbb{R},
        \end{equation*}
        where $\mathbf{A}: \mathbb{R} \to \mathbb{R}^{n \times n}$ is differentiable and invertible. We have:
        \begin{equation*}
            \odv*{f(t)}{t} = \det(\mathbf{A}(t))\Tr\left(\mathbf{A}^{-1}\left(\odv*{\mathbf{A}(t)}{t}\right)\right).
        \end{equation*}
    \end{thm}
    \begin{proofing}
        By the chain rule,
        \begin{equation*}
            \odv*{\det(\mathbf{A}(t))}{t} = D \det(\mathbf{A}(t))\left(\odv*{\mathbf{A}(t)}{t}\right) = \det(\mathbf{A}(t))\Tr\left(\mathbf{A}^{-1}\left(\odv*{\mathbf{A}(t)}{t}\right)\right).
        \end{equation*}
    \end{proofing}
    \newpage

    \begin{eg}
        Let $F: \mathbb{R}^{n \times n} \to \mathbb{R}$ be defined by:
        \begin{equation*}
            F(\mathbf{X}) = \ln(\det(\mathbf{X})), \qquad \text{for symmetric positive definite } \mathbf{X} \in \mathbb{R}^{n \times n}.
        \end{equation*}
        By the chain rule, for any symmetric positive definite $\mathbf{X}, \mathbf{Y} \in \mathbb{R}^{n \times n}$,
        \begin{align*}
            D F(\mathbf{X})(\mathbf{Y}) &= D \ln(\det(\mathbf{X}))(D \det(\mathbf{X})(\mathbf{Y}))\\
            \tag{Example \ref{Chapter 5 (Example) Jacobi's Formula Second Lemma}}
            &= \frac{\det(\mathbf{X})\Tr(\mathbf{X}^{-1}\mathbf{Y})}{\det(\mathbf{X})}\\
            &= \Tr(\mathbf{X}^{-1}\mathbf{Y}) = \inprod{(\mathbf{X}^{-1})^{T}}{\mathbf{Y}}.
        \end{align*}
        Therefore, $\nabla F(\mathbf{X}) = (\mathbf{X}^{-1})^{T}$.
    \end{eg}
    \begin{eg}\named{Eigenvalues and eigenvectors}
        Let $\lambda_{i}: \mathbb{R}^{n \times n} \to \mathbb{R}$ and $\mathbf{v}_{i}: \mathbb{R}^{n \times n} \to \mathbb{R}$ be defined by:
        \begin{equation*}
            \mathbf{X}\mathbf{v}_{i}[\mathbf{X}] = (\lambda_{i}\mathbf{v}_{i})[\mathbf{X}], \qquad \text{for symmetric and real } \mathbf{X} \in \mathbb{R}^{n \times n},
        \end{equation*}
        where $\lambda_{i}[\mathbf{X}]$ is the $i$-th eigenvalue of $\mathbf{X}$ with the corresponding unit eigenvector $\mathbf{v}_{i}[\mathbf{X}]$. By the matrix multiplication rule, for any symmetric and real $\mathbf{X}, \mathbf{Y} \in \mathbb{R}^{n \times n}$,
        \begin{align*}
            D(\mathbf{X}\mathbf{v}_{i}[\mathbf{X}])(\mathbf{Y}) &= D(\lambda_{i}\mathbf{v}_{i})[\mathbf{X}](\mathbf{Y})\\
            D(\mathbf{X})(\mathbf{Y})\mathbf{v}_{i}[\mathbf{X}] + \mathbf{X}D \mathbf{v}_{i}[\mathbf{X}](\mathbf{Y}) &= D \lambda_{i}[\mathbf{X}](\mathbf{Y})\mathbf{v}_{i}[\mathbf{X}] + \lambda_{i}[\mathbf{X}]D \mathbf{v}_{i}[\mathbf{X}](\mathbf{Y})\\
            \tag{1}
            \mathbf{Y}\mathbf{v}_{i}[\mathbf{X}] + \mathbf{X}D \mathbf{v}_{i}[\mathbf{X}](\mathbf{Y}) &= D \lambda_{i}[\mathbf{X}](\mathbf{Y})\mathbf{v}_{i}[\mathbf{X}] + \lambda_{i}[\mathbf{X}]D \mathbf{v}_{i}[\mathbf{X}](\mathbf{Y}).
        \end{align*} 
        Since $\mathbf{v}_{i}[\mathbf{X}]$ is a unit vector, by the matrix multiplication rule,
        \begin{align*}
            (\mathbf{v}_{i}^{T}\mathbf{v}_{i})[\mathbf{X}] &= 1, & (D \mathbf{v}_{i}[\mathbf{X}])^{T}\mathbf{v}_{i}[\mathbf{X}] + \mathbf{v}_{i}^{T}[\mathbf{X}]D \mathbf{v}_{i}[\mathbf{X}] &= 2\mathbf{v}_{i}^{T}[\mathbf{X}]D \mathbf{v}_{i}[\mathbf{X}] = 0.
        \end{align*}
        Therefore, $\mathbf{v}_{i}^{T}[\mathbf{X}]D \mathbf{v}_{i}[\mathbf{X}] = 0$. Multiplying $\mathbf{v}_{i}^{T}[\mathbf{X}]$ to (1),
        \begin{align*}
            \tag{$\mathbf{X}$ is symmetric}
            \mathbf{v}_{i}^{T}[\mathbf{X}]\mathbf{Y}\mathbf{v}_{i}[\mathbf{X}] + \mathbf{v}_{i}^{T}[\mathbf{X}]\mathbf{X}D \mathbf{v}_{i}[\mathbf{X}](\mathbf{Y}) &= \mathbf{v}_{i}^{T}[\mathbf{X}]\mathbf{Y}\mathbf{v}_{i}[\mathbf{X}] + (\lambda_{i}\mathbf{v}_{i}^{T})[\mathbf{X}]D \mathbf{v}_{i}[\mathbf{X}](\mathbf{Y})\\
            &= \mathbf{v}_{i}^{T}[\mathbf{X}]\mathbf{Y}\mathbf{v}_{i}[\mathbf{X}],\\
            D \lambda_{i}[\mathbf{X}](\mathbf{Y})(\mathbf{v}_{i}^{T}\mathbf{v}_{i})[\mathbf{X}] + (\lambda_{i}\mathbf{v}_{i}^{T})[\mathbf{X}]D \mathbf{v}_{i}[\mathbf{X}](\mathbf{Y}) &= D \lambda_{i}[\mathbf{X}](\mathbf{Y}).
        \end{align*}
        Thus, we can find that:
        \begin{align*}
            D \lambda_{i}[\mathbf{X}](\mathbf{Y}) &= \mathbf{v}_{i}^{T}[\mathbf{X}]\mathbf{Y}\mathbf{v}_{i}[\mathbf{X}] = \inprod{(\mathbf{v}_{i}\mathbf{v}_{i}^{T})[\mathbf{X}]}{\mathbf{Y}}, & \nabla \lambda_{i}[\mathbf{X}] &= (\mathbf{v}_{i}\mathbf{v}_{i}^{T})[\mathbf{X}].
        \end{align*}
        Moreover, substituting $D \lambda_{i}[\mathbf{X}](\mathbf{Y})$ into (1),
        \begin{align*}
            \mathbf{Y}\mathbf{v}_{i}[\mathbf{X}] + \mathbf{X}D \mathbf{v}_{i}[\mathbf{X}](\mathbf{Y}) &= (\mathbf{v}_{i}\mathbf{v}_{i}^{T})[\mathbf{X}]\mathbf{Y}\mathbf{v}_{i}[\mathbf{X}] + \lambda_{i}[\mathbf{X}]D \mathbf{v}_{i}[\mathbf{X}](\mathbf{Y})\\
            \tag{2}
            (\mathbf{X}-\lambda_{i}[\mathbf{X}]\mathbf{I})D \mathbf{v}_{i}[\mathbf{X}](\mathbf{Y}) &= ((\mathbf{v}_{i}\mathbf{v}_{i}^{T})[\mathbf{X}] - \mathbf{I})\mathbf{Y}\mathbf{v}_{i}[\mathbf{X}].
        \end{align*}
        The matrix $\mathbf{X}-\lambda_{i}[\mathbf{X}]\mathbf{I}$ is not invertible. Consider when $\mathbf{X}$ is non-singular and $\lambda_{i}[\mathbf{X}]$ is a simple (unique) eigenvalue. We have:
        \begin{equation*}
            \mathbf{X} - \lambda_{i}[\mathbf{X}]\mathbf{I} = \sum_{j=1}^{n}(\lambda_{j}\mathbf{v}_{j}\mathbf{v}_{j}^{T})[\mathbf{X}] - \sum_{j=1}^{n}(\lambda_{i}\mathbf{v}_{j}\mathbf{v}_{j}^{T})[\mathbf{X}] = \sum_{j \neq i}((\lambda_{j}-\lambda_{i})\mathbf{v}_{j}\mathbf{v}_{j}^{T})[\mathbf{X}].
        \end{equation*}
        Note that $(\mathbf{v}_{i}\mathbf{v}_{i}^{T})[\mathbf{X}] - \mathbf{I} = -\sum_{j \neq i}(\mathbf{v}_{j}\mathbf{v}_{j}^{T})[\mathbf{X}]$. If we consider each term of the summation by applying to (2),
        \begin{equation*}
            ((\lambda_{j} - \lambda_{i})\mathbf{v}_{j}\mathbf{v}_{j}^{T})[\mathbf{X}]D \mathbf{v}_{i}[\mathbf{X}](\mathbf{Y}) = -(\mathbf{v}_{j}\mathbf{v}_{j}^{T})[\mathbf{X}]\mathbf{Y}\mathbf{v}_{i}[\mathbf{X}].
        \end{equation*}
        Therefore,
        \begin{align*}
            D \mathbf{v}_{i}[\mathbf{X}](\mathbf{Y}) = \sum_{j=1}^{n}(\mathbf{v}_{j}\mathbf{v}_{j}^{T})[\mathbf{X}]D \mathbf{v}_{i}[\mathbf{X}](\mathbf{Y}) = \sum_{j \neq i}(\mathbf{v}_{j}\mathbf{v}_{j}^{T})[\mathbf{X}]D \mathbf{v}_{i}[\mathbf{X}](\mathbf{Y}) &= -\sum_{j \neq i}\frac{(\mathbf{v}_{j}\mathbf{v}_{j}^{T})[\mathbf{X}]\mathbf{Y}\mathbf{v}_{i}[\mathbf{X}]}{(\lambda_{j}-\lambda_{i})[\mathbf{X}]}\\
            &= \sum_{j \neq i}\frac{\mathbf{v}_{j}^{T}[\mathbf{X}]\mathbf{Y}\mathbf{v}_{i}[\mathbf{X}]}{(\lambda_{i}-\lambda_{j})[\mathbf{X}]}\mathbf{v}_{j}.
        \end{align*}
    \end{eg}
    Read Appendix \ref{Case Study I: Newton's Method} (Newton's Method) and \ref{Case Study J: Deep Neural Network Training in Deep Learning} (Deep Neural Network Training in Deep Learning) to see the case studies for Chapter \ref{Chapter 5: Linear Transformations or Linear Operators}.
    
\appendix
\renewcommand{\thechapter}{\Alph{chapter}}
\chapter{Clustering, K-means, K-medians}
	\label{Case Study A: Clustering, K-means, K-medians}
	This case study assumes that you have already read Chapter \ref{Chapter 2: Vector spaces, metrics, limits, and convergence}.
	
    Suppose we are given $N$ vectors in $\mathbb{R}^{n}$:
    \begin{equation*}
        \mathbf{x}_{1}, \cdots, \mathbf{x}_{N} \in \mathbb{R}^{n}.
    \end{equation*}
    We want to group them into $K$ different clusters.
    \begin{rem}
        $\mathbb{R}^{n}$ is used for simplicity. In fact, it can be replaced by any vector space.
    \end{rem}
    Before performing clustering on any vector space, we must formulate the problem mathematically.
    \begin{enumerate}
        \item Representation: Starting with $N$ vectors $\{\mathbf{x}_{i}\}_{i=1}^{N}$ and $K$ clusters $\{G_{j}\}_{j=1}^{K}$, we define the following variables:
        \begin{enumerate}
            \item $\mathbf{x}_{i}\in\mathbb{R}^{n}$: the vectors to be grouped,
            \item $c_{i}\in\{1,\cdots,K\}$: the cluster to which $\mathbf{x}_{i}$ belongs,
            \item $G_{j} = \{i : c_{i} = j\}$: the clusters, which are sets of indices representing the vectors in the group,
            \item $\mathbf{z}_{j}\in\mathbb{R}^{n}$: the representative vector in $G_{j}$, not necessarily one of the vectors in $\{\mathbf{x}_{1}, \cdots, \mathbf{x}_{N}\}$.
        \end{enumerate}
        \item Evaluation: What problem do we want to solve? The vectors in each cluster should be close to each other.
        \begin{enumerate}
            \item The distance between the vectors in a cluster and the corresponding representative vector should be minimized. Therefore, we define an optimization function:
            \begin{equation*}
                d_{j} = \sum_{i \in G_{j}}\norm{\mathbf{x}_{i} - \mathbf{z}_{j}}^{2}.
            \end{equation*}
            Our objective for this cluster is to minimize this optimization function.
            \item Altogether, we get the overall optimization function:
            \begin{equation*}
                d = \sum_{j=1}^{K}d_{j}.
            \end{equation*}
            Then, we solve:
            \begin{equation*}
                \min_{\substack{G_{1}, \cdots, G_{K}\\\mathbf{z}_{1}, \cdots, \mathbf{z}_{K}}}d \Longleftrightarrow \min_{\substack{G_{1}, \cdots, G_{K}\\\mathbf{z}_{1}, \cdots, \mathbf{z}_{K}}}\sum_{j=1}^{K}\sum_{i \in G_{j}}\norm{\mathbf{x}_{i} - \mathbf{z}_{j}}^{2}.
            \end{equation*}
        \end{enumerate}
        \item Optimization: We now have two sets of unknowns $\{G_{1}, \cdots, G_{K}\}$ and $\{\mathbf{z}_{1}, \cdots, \mathbf{z}_{K}\}$. However, both influence each other. How do we tackle this issue? We use alternating minimization.
        \begin{itemize}
            \item[Step 0:] Initialize $\mathbf{z}_{1}, \cdots, \mathbf{z}_{K}$.
            \item[Step 1:] Fix $\mathbf{z}_{1}, \cdots, \mathbf{z}_{K}$ and solve the function with respect to $G_{1}, \cdots, G_{K}$:
            \begin{equation*}
                \min_{G_{1}, \cdots, G_{K}}\sum_{j=1}^{K}\sum_{i \in G_{j}}\norm{\mathbf{x}_{i} - \mathbf{z}_{j}}^{2}.
            \end{equation*}
            \item[Step 2:] Fix $G_{1}, \cdots, G_{K}$ and solve the function with respect to $\mathbf{z}_{1}, \cdots, \mathbf{z}_{K}$:
            \begin{equation*}
                \min_{\mathbf{z}_{1}, \cdots, \mathbf{z}_{K}}\sum_{j=1}^{K}\sum_{i \in G_{j}}\norm{\mathbf{x}_{i} - \mathbf{z}_{j}}^{2}.
            \end{equation*}
            \item[] Repeat Steps 1 and 2 until convergence is achieved.
        \end{itemize}
    \end{enumerate}
    \newpage

    How do we solve the functions in the alternating minimization algorithm? To obtain the clusters, we solve the following function:
    \begin{align*}
        \min_{G_{1}, \cdots, G_{K}}\sum_{j=1}^{K}\sum_{i \in G_{j}}\norm{\mathbf{x}_{i} - \mathbf{z}_{j}}^{2} &\Longleftrightarrow \min_{c_{1}, \cdots, c_{N}}\sum_{i=1}^{N}\norm{\mathbf{x}_{i} - \mathbf{z}_{c_{i}}}^{2}\\
        &\Longleftrightarrow \min_{c_{i} \in \{1, \cdots, K\}}\norm{\mathbf{x}_{i} - \mathbf{z}_{c_{i}}}^{2}\\
        &\Longleftrightarrow c_{i} = \argmin_{j \in \{1, \cdots, K\}}\norm{\mathbf{x}_{i} - \mathbf{z}_{j}}, \qquad \text{for } i = 1, \cdots, N.
    \end{align*}
    In simpler terms, finding clusters that minimize the distance between the vectors and the representatives is the same as assigning each vector to the cluster that minimizes the distance to its representative.

    Therefore, $\mathbf{x}_{i}$ is assigned to the cluster whose representative is closest to $\mathbf{x}_{i}$. The new $G_{j}$ is then created by:
    \begin{equation*}
        G_{j} = \{i : c_{i} = j\}.
    \end{equation*}
    To obtain the representative vectors, we solve the following function:
    \begin{align*}
        \min_{\mathbf{z}_{1}, \cdots, \mathbf{z}_{K}}\sum_{j=1}^{K}\sum_{i \in G_{j}}\norm{\mathbf{x}_{i} - \mathbf{z}_{j}}^{2} &\Longleftrightarrow \min_{\mathbf{z}_{j}}\sum_{i \in G_{j}}\norm{\mathbf{x}_{i} - \mathbf{z}_{j}}^{2}, \qquad \text{for } j = 1, \cdots, K.
    \end{align*}
    We only need to consider each cluster separately to find the corresponding representative vector.

    At this point, we have not specified which norms are used to construct the clusters. In $\mathbb{R}^{n}$, one of the most commonly used norms for clustering is the $2$-norm. For $j = 1, \cdots, K$:    
    \begin{equation*}
        \min_{\mathbf{z}_{j}}\sum_{i \in G_{j}}\pnorm[2]{\mathbf{x}_{i} - \mathbf{z}_{j}}^{2} \Longleftrightarrow \min_{z_{j1}, \cdots, z_{jn}}\sum_{i \in G_{j}}\sum_{k=1}^{n}(x_{ik} - z_{jk})^{2}.
    \end{equation*}
    Taking derivatives for each entry of $\mathbf{z}_{j}$, we have:
    \begin{align*}
        2\sum_{i \in G_{j}}(z_{jk} - x_{ik}) = 0 &\Longleftrightarrow z_{jk} = \frac{1}{\abs{G_{j}}}\sum_{i \in G_{j}}x_{ik}, \qquad \text{for } k = 1, \cdots, n,\\
        &\Longleftrightarrow \mathbf{z}_{j} = \frac{1}{\abs{G_{j}}}\sum_{i \in G_{j}}\mathbf{x}_{i}.
    \end{align*}
    This is called the K-means algorithm.
    \begin{defn}
        The \textbf{K-means algorithm} is a method that assigns $N$ vectors into $K$ clusters, where each vector belongs to the cluster with the nearest mean. The steps of the algorithm are as follows:
        \begin{itemize}
            \item[0:] Initialize $\mathbf{z}_{1}, \cdots, \mathbf{z}_{K}$.
            \item[1:] Fix $\mathbf{z}_{1}, \cdots, \mathbf{z}_{K}$. For each $\mathbf{x}_{i}$, assign it to the cluster whose representative is closest in Euclidean distance:
            \begin{align*}
                c_{i} &= \argmin_{j \in \{1, \cdots, K\}}\pnorm[2]{\mathbf{x}_{i} - \mathbf{z}_{j}}^{2}, \qquad \text{for } i = 1, \cdots, N, & G_{j} &= \{i : c_{i} = j\}, \qquad \text{for } j = 1, \cdots, K.
            \end{align*}
            \item[2:] For each cluster $G_{j}$, calculate the new $\mathbf{z}_{j}$ as the mean of vectors in $G_{j}$:
            \begin{equation*}
                \mathbf{z}_{j} = \frac{1}{\abs{G_{j}}}\sum_{i \in G_{j}}\mathbf{x}_{i}, \qquad \text{for } j = 1, \cdots, K.
            \end{equation*}
            \item[] Repeat Step 1-2 until convergence is achieved.
        \end{itemize}
    \end{defn}
    \newpage

    What happens if we switch from the $2$-norm to the $1$-norm? Derivations are omitted, but we find that it turns into the K-medians algorithm.
    \begin{defn}
        The \textbf{K-medians algorithm} is a method that assigns $N$ vectors into $K$ clusters, where each vector belongs to the cluster with the nearest median. The steps of the algorithm are as follows:
        \begin{itemize}
            \item[0:] Initialize $\mathbf{z}_{1}, \cdots, \mathbf{z}_{K}$.
            \item[1:] Fix $\mathbf{z}_{1}, \cdots, \mathbf{z}_{K}$. For each $\mathbf{x}_{i}$, assign it to the cluster whose representative is closest in Manhattan distance:
            \begin{align*}
                c_{i} &= \argmin_{j \in \{1, \cdots, K\}}\pnorm[1]{\mathbf{x}_{i} - \mathbf{z}_{j}}^{2}, \qquad \text{for } i = 1, \cdots, N, & G_{j} &= \{i : c_{i} = j\}, \qquad \text{for } j = 1, \cdots, K.
            \end{align*}
            \item[2:] For each cluster $G_{j}$, calculate the new $\mathbf{z}_{j}$ as the median of vectors in $G_{j}$:
            \begin{equation*}
                \mathbf{z}_{j} = \median\{\mathbf{x}_{i} : i \in G_{j}\}, \qquad \text{for } j = 1, \cdots, K.
            \end{equation*}
            \item[] Repeat Step 1-2 until convergence is achieved.
        \end{itemize}
    \end{defn}
    \begin{rem}
        The K-means algorithm is more sensitive to outliers, while the K-medians algorithm is more robust to outliers.
    \end{rem}

\chapter{Kernel K-means/Kernel Trick}
    \label{Case Study B: Kernel K-means/Kernel Trick}
    This case study assumes that you have already read Chapter \ref{Chapter 3: Inner products, Hilbert Spaces} and Appendix \ref{Case Study A: Clustering, K-means, K-medians}.

    In the regular K-means algorithm, we assume that the vectors can be separated linearly. However, it fails when the boundary is curved. How do we modify the K-means algorithm to deal with curved data? We can transform the data to a new domain and then apply the K-means algorithm in that transformed domain.
    \begin{enumerate}
        \item Representation: Starting with $N$ vectors $\{\mathbf{x}_{i}\}_{i=1}^{N}$ and $K$ clusters $\{G_{j}\}_{j=1}^{K}$, we define the following variables:
        \begin{enumerate}
            \item $\mathbf{x}_{i}\in\mathbb{R}^{n}$: the vectors to be grouped.
            \item $c_{i}\in\{1,\cdots,K\}$: the cluster to which $\mathbf{x}_{i}$ belongs.
            \item $G_{j}=\{i:c_{i}=j\}$: the clusters, which are sets of indices representing the vectors in the group.
            \item $H$: the feature space that contains the transformed vectors.
            \item $\phi: \mathbb{R}^{n}\to H$: the feature map that transforms the vectors.
            \item $\mathbf{z}_{j}\in H$: the representative vector in $G_{j}$, not necessarily one of the vectors in $\{\phi(\mathbf{x}_{1}),\cdots,\phi(\mathbf{x}_{N})\}$.
        \end{enumerate}
        \item Evaluation: With the feature map, our objective becomes finding: 
        \begin{equation*}
            \min_{\substack{G_{1},\cdots,G_{K}\\\mathbf{z}_{1},\cdots,\mathbf{z}_{K}}}\sum_{j=1}^{K}\sum_{i\in G_{j}}\norm{\phi(\mathbf{x}_{i})-\mathbf{z}_{j}}^{2}
        \end{equation*}
        In this algorithm, we also need to find a good $\phi$ that can transform the data well. If we assume that the norm we use has an inner product, our goal is to ensure that:
        \begin{enumerate}
            \item If $\mathbf{x}_{i_{1}}$ and $\mathbf{x}_{i_{2}}$ are close ($\pnorm[2]{\mathbf{x}_{i_{1}}-\mathbf{x}_{i_{2}}}$ is small), then $\pnorm[2]{\phi(\mathbf{x}_{i_{1}})-\phi(\mathbf{x}_{i_{2}})}$ is small.
            \item If $\mathbf{x}_{i_{1}}$ and $\mathbf{x}_{i_{2}}$ are far apart ($\pnorm[2]{\mathbf{x}_{i_{1}}-\mathbf{x}_{i_{2}}}$ is large), then $\pnorm[2]{\phi(\mathbf{x}_{i_{1}})-\phi(\mathbf{x}_{i_{2}})}$ is large.
        \end{enumerate}
        \item Optimization: Since $\phi$ depends on the shape of $\mathbf{x}_{1},\cdots,\mathbf{x}_{N}$, it is not easy to find a good $\phi$ and $H$ explicitly. 
        
        Therefore, we can utilize the \textbf{Kernel trick} -- define $\phi$ and $H$ implicitly.
        
        Moreover, if we only need to know $G_{1},\cdots,G_{K}$, we can eliminate $\mathbf{z}_{1},\cdots,\mathbf{z}_{K}$ in our algorithm.
    \end{enumerate}

    The K-means algorithm can be modified as follows:
    \begin{itemize}
        \item[0:] Initialize $G_{1}, \cdots, G_{K}$.
        \item[1:] For each $\mathbf{x}_{i}$, assign it to the cluster whose representative is closest in Euclidean distance:
        \begin{align*}
            c_{i} &= \argmin_{j \in \{1, \cdots, K\}}\pnorm[2]{\phi(\mathbf{x}_{i}) - \frac{1}{\abs{G_{j}}}\sum_{k\in G_{j}}\phi(\mathbf{x}_{k})}^{2}, \qquad \text{for } i = 1, \cdots, N, & G_{j} &= \{i : c_{i} = j\}, \qquad \text{for } j = 1, \cdots, K.
        \end{align*}
        \item[] Repeat until convergence is achieved.
    \end{itemize}
    \newpage
    
    We can rearrange the equation in the algorithm:
    \begin{align*}
        \pnorm[2]{\phi(\mathbf{x}_{i}) - \frac{1}{\abs{G_{j}}}\sum_{i\in G_{j}}\phi(\mathbf{x}_{i})}^{2} &= \inprod{\phi(\mathbf{x}_{i}) - \frac{1}{\abs{G_{j}}}\sum_{i\in G_{j}}\phi(\mathbf{x}_{i})}{\phi(\mathbf{x}_{i}) - \frac{1}{\abs{G_{j}}}\sum_{i\in G_{j}}\phi(\mathbf{x}_{i})}\\
        &=\inprod{\phi(\mathbf{x}_{i})}{\phi(\mathbf{x}_{i})}-\frac{2}{\abs{G_{j}}}\inprod{\phi(\mathbf{x}_{i})}{\sum_{i\in G_{j}}\phi(\mathbf{x}_{i})}+\frac{1}{\abs{G_{j}}^{2}}\inprod{\sum_{i\in G_{j}}\phi(\mathbf{x}_{i})}{\sum_{i\in G_{j}}\phi(\mathbf{x}_{i})}\\
        &=\inprod{\phi(\mathbf{x}_{i})}{\phi(\mathbf{x}_{i})}-\frac{2}{\abs{G_{j}}}\sum_{k\in G_{j}}\inprod{\phi(\mathbf{x}_{i})}{\phi(\mathbf{x}_{k})}+\frac{1}{\abs{G_{j}}^{2}}\sum_{k\in G_{j}}\sum_{\ell\in G_{j}}\inprod{\phi(\mathbf{x}_{k})}{\phi(\mathbf{x}_{\ell})}.
    \end{align*}
    We can see that all the inner products are usually in the form of:
    \begin{equation*}
        \inprod{\phi(\cdot)}{\phi(\cdot)}.
    \end{equation*}
    By using the Kernel trick, we can define $\phi$ and $H$ implicitly by defining the kernel function.
    \begin{defn}
        For some $\phi$, the \textbf{kernel function} is a binary operator $\kappa: (\mathbb{R}^{n},\mathbb{R}^{n})\to\mathbb{R}$ such that for $\mathbf{x},\mathbf{y}\in\mathbb{R}^{n}$,
        \begin{equation*}
            \kappa(\mathbf{x},\mathbf{y})=\inprod{\phi(\mathbf{x})}{\phi(\mathbf{y})}.
        \end{equation*}
    \end{defn}
    We can then form a generalized K-means algorithm.
    \begin{defn}
        The \textbf{Kernel K-means algorithm} is a method that assigns $N$ vectors into $K$ clusters, where each vector is transformed and classified into the cluster with the nearest mean. The steps of the algorithm are as follows:
        \begin{itemize}
            \item[0:] Initialize $G_{1},\cdots,G_{K}$ and define a kernel function $\kappa$.
            \item[1:] For each $\mathbf{x}_{i}$, assign it to the cluster whose mean is the closest in Euclidean distance after transformation.
            \begin{align*}
                c_{i}&=\argmin_{j \in \{1, \cdots, K\}} \left(\kappa(\mathbf{x}_{i},\mathbf{x}_{i})-\frac{2}{\abs{G_{j}}}\sum_{k\in G_{j}}\kappa(\mathbf{x}_{i},\mathbf{x}_{k})+\frac{1}{\abs{G_{j}}^{2}}\sum_{k\in G_{j}}\sum_{\ell\in G_{j}}\kappa(\mathbf{x}_{k},\mathbf{x}_{\ell})\right), & &\text{for }i=1,\cdots,N\\
                G_{j}&=\{i:c_{i}=j\}, & &\text{for }j=1,\cdots,K.
            \end{align*}
            \item[] Repeat Step 1 until convergence is achieved.
        \end{itemize}
    \end{defn}
    Now the problem switches to determining which kernel function we can choose. We have some necessary conditions.
    \begin{defn}
        The kernel function $\kappa$ is a \textbf{symmetric kernel} if for all $\mathbf{x},\mathbf{y}\in\mathbb{R}^{n}$,
        \begin{equation*}
            \kappa(\mathbf{x},\mathbf{y})=\kappa(\mathbf{y},\mathbf{x}).
        \end{equation*}
    \end{defn}
    With a set of vectors $\mathbf{y}_{1},\cdots,\mathbf{y}_{m}\in\mathbb{R}^{n}$, we can define a matrix by:
    \begin{equation*}
        \boldsymbol{\kappa}=[\kappa(\mathbf{y}_{i},\mathbf{y}_{j})]_{i,j}.
    \end{equation*}
    Assume that we have a new vector that is the linear combination of the $m$ transformed vectors. For any $\mathbf{z}\in\mathbb{R}^{m}$,
    \begin{equation*}
        0\leq\inprod{\sum_{i=1}^{m}z_{i}\phi(\mathbf{y}_{i})}{\sum_{i=1}^{m}z_{i}\phi(\mathbf{y}_{i})}=\sum_{i=1}^{m}\sum_{j=1}^{m}z_{i}z_{j}\inprod{\phi(\mathbf{y}_{i})}{\phi(\mathbf{y}_{j})}=\sum_{i=1}^{m}\sum_{j=1}^{m}z_{i}z_{j}\kappa(\mathbf{y}_{i},\mathbf{y}_{j})=\mathbf{z}^{T}\boldsymbol{\kappa}\mathbf{z}.
    \end{equation*}
    \begin{defn}
        The kernel function $\kappa$ is symmetric positive semi-definite (SPSD) if:
        \begin{enumerate}
            \item $\kappa(\mathbf{x},\mathbf{y})=\kappa(\mathbf{y},\mathbf{x})$ for all $\mathbf{x},\mathbf{y}\in\mathbb{R}^{n}$.
            \item For any $m>0$ and $\mathbf{y}_{1},\cdots,\mathbf{y}_{m}\in\mathbb{R}^{n}$, the matrix:
            \begin{equation*}
                \boldsymbol{\kappa}=[\kappa(\mathbf{y}_{i},\mathbf{y}_{j})]_{i,j}
            \end{equation*}
            is symmetric positive semi-definite.
        \end{enumerate}
    \end{defn}
    \newpage

    Therefore, by the following theorem, we have a way to determine which mapping can be used as a kernel function.
    \begin{thm}\named{Mercer's Theorem}
        If the kernel function $\kappa$ is continuous, symmetric, and positive semi-definite, then there exists a Hilbert space $H$ and a mapping such that for $\mathbf{x},\mathbf{y}\in\mathbb{R}^{n}$:
        \begin{equation*}
            \kappa(\mathbf{x},\mathbf{y})=\inprod{\phi(\mathbf{x})}{\phi(\mathbf{y})}.
        \end{equation*}
    \end{thm}
    \begin{eg}
        The traditional kernel, which involves no transformation, is defined by:
        \begin{align*}
            \kappa(\mathbf{x},\mathbf{y})&=\mathbf{x}^{T}\mathbf{y}, & \phi(\mathbf{x})&=\mathbf{x}, & &\text{for } \mathbf{x},\mathbf{y}\in\mathbb{R}^{n}.
        \end{align*}
        Applying this kernel function gives the normal K-means algorithm.
    \end{eg}
    \begin{rem}
        Most of the time, finding the feature map $\phi$ is extremely difficult.
    \end{rem}
    \begin{eg}
        The \textbf{polynomial kernel} for $\alpha\in\mathbb{Z}$ and $c\in\mathbb{R}$ is defined by:
        \begin{equation*}
            \kappa(\mathbf{x},\mathbf{y})=(\mathbf{x}^{T}\mathbf{y}+c)^{\alpha}, \qquad \text{for }\mathbf{x},\mathbf{y}\in \mathbb{R}^{n}.
        \end{equation*}
        If $\alpha = 2$, then the feature map is an extremely large vector defined by:
        \begin{equation*}
            \phi(\mathbf{x})=\begin{pmatrix}
                x_{1}^{2}\\
                \vdots\\
                x_{n}^{2}\\
                \sqrt{2}x_{1}x_{2}\\
                \vdots\\
                \sqrt{2}x_{n-1}x_{n}\\
                \sqrt{2}cx_{1}\\
                \vdots\\
                \sqrt{2}cx_{n}\\
                c
            \end{pmatrix}, \qquad \text{for }\mathbf{x}\in\mathbb{R}^{n}.
        \end{equation*}
        A more general term would require the multinomial theorem.
    \end{eg}
    \begin{eg}
        The \textbf{Gaussian kernel} for $\sigma>0$ is defined by:
        \begin{equation*}
            \kappa(\mathbf{x},\mathbf{y})=e^{-\frac{1}{\sigma^{2}}\pnorm[2]{\mathbf{x}-\mathbf{y}}^{2}}=\exp\left(-\frac{1}{\sigma^{2}}\pnorm[2]{\mathbf{x}-\mathbf{y}}^{2}\right), \qquad \text{for }\mathbf{x},\mathbf{y}\in \mathbb{R}^{n}.
        \end{equation*}
        Obtaining its feature map would require using the Taylor expansion.
    \end{eg}
    \begin{eg}
        Consider that we use the Gaussian kernel to perform the Kernel K-means algorithm. For the same vectors:
        \begin{equation*}
            \kappa(\mathbf{x}_{i},\mathbf{x}_{i})=\exp\left(-\frac{1}{\sigma^{2}}\pnorm[2]{\mathbf{x}_{i}-\mathbf{x}_{i}}^{2}\right)=e^{0}=1, \qquad \text{for }i=1,\cdots,N.
        \end{equation*}
        For different vectors, we can normalize the distances between two vectors to lie between $0$ and $1$ via the transformation:
        \begin{equation*}
            \kappa(\mathbf{x}_{i},\mathbf{x}_{j})\begin{cases}
                \approx 1, &\text{if }\pnorm[2]{\mathbf{x}_{i}-\mathbf{x}_{j}}\text{ is small}\\
                \approx 0, &\text{if }\pnorm[2]{\mathbf{x}_{i}-\mathbf{x}_{j}}\text{ is large}.
            \end{cases}
        \end{equation*}
        Does the kernel fulfill the goal for what we aimed for with the feature map?
        \begin{align*}
            \pnorm[2]{\phi(\mathbf{x}_{i_{1}})-\phi(\mathbf{x}_{i_{2}})}^{2}&=\pnorm[2]{\phi(\mathbf{x}_{i_{1}})}^{2}-2\inprod{\phi(\mathbf{x}_{i_{1}})}{\phi(\mathbf{x}_{i_{2}})}+\pnorm[2]{\phi(\mathbf{x}_{i_{2}})}^{2}\\
            &=\kappa(\mathbf{x}_{i_{1}},\mathbf{x}_{i_{1}})-2\kappa(\mathbf{x}_{i_{1}},\mathbf{x}_{i_{2}})+\kappa(\mathbf{x}_{i_{2}},\mathbf{x}_{i_{2}}).
        \end{align*}
        Therefore, the distance in transformed data is given by:
        \begin{equation*}
            \pnorm[2]{\phi(\mathbf{x}_{i_{1}})-\phi(\mathbf{x}_{i_{2}})}^{2}\begin{cases}
                \approx 0, &\text{if }\pnorm[2]{\mathbf{x}_{i}-\mathbf{x}_{j}}\text{ is small}\\
                \approx 2, &\text{if }\pnorm[2]{\mathbf{x}_{i}-\mathbf{x}_{j}}\text{ is large}.
            \end{cases}
        \end{equation*}
    \end{eg}

\chapter{Metric Learning}
    \label{Case Study C: Metric Learning}
    This case study assumes that you have already read Chapter \ref{Chapter 3: Inner products, Hilbert Spaces}.

    Suppose we are given $N$ vectors in $\mathbb{R}^{n}$:
    \begin{equation*}
        \mathbf{x}_{1},\cdots,\mathbf{x}_{N}\in\mathbb{R}^{n},
    \end{equation*}
    and a set of vector pairs $S$ and $D$ such that:
    \begin{equation*}
        (\mathbf{x}_{i},\mathbf{x}_{j})\in\begin{cases}
            S, &\text{if }\mathbf{x}_{i}\text{ and }\mathbf{x}_{j}\text{ are similar},\\
            D, &\text{if }\mathbf{x}_{i}\text{ and }\mathbf{x}_{j}\text{ are dissimilar (different)}.
        \end{cases}
    \end{equation*}
    How do we find a metric $\norm{\cdot}$ such that:
    \begin{align*}
        \norm{\mathbf{x}_{i}-\mathbf{x}_{j}}&\text{ is small}, & \text{for }&(\mathbf{x}_{i},\mathbf{x}_{j})\in S,\\
        \norm{\mathbf{x}_{i}-\mathbf{x}_{j}}&\text{ is large}, & \text{for }&(\mathbf{x}_{i},\mathbf{x}_{j})\in D.
    \end{align*} 
    \begin{enumerate}
        \item Representation: How do we represent the norm? We can try using the $p$-norms with $p\geq 1$:
        \begin{equation*}
            \pnorm[p]{\mathbf{x}}=\left(\sum_{i=1}^{n}\abs{x_{i}}^{p}\right)^{\frac{1}{p}}.
        \end{equation*}
        However, the norm set is too small for us to explore. We can use the more generalized norm induced by the weighted inner product with a symmetric positive definite matrix $\mathbf{A}\in\mathbb{R}^{n\times n}$: 
        \begin{equation*}
            \pnorm[\mathbf{A}]{\mathbf{x}}=\sqrt{\mathbf{x}^{T}\mathbf{A}\mathbf{x}}.
        \end{equation*}
        The set of all SPD matrices is large enough, though it is not closed. Its closure is the set of all SPSD matrices. However, for any SPSD matrices $\mathbf{A}$ that are not SPD matrices:
        \begin{align*}
            \pnorm[\mathbf{A}]{\mathbf{x}}&=\sqrt{\mathbf{x}^{T}\mathbf{A}\mathbf{x}}=0, & 
            \mathbf{x}^{T}\mathbf{Ax}&\centernot{\Longrightarrow}\mathbf{x}=\mathbf{0}.
        \end{align*}
        This violates the positive-definiteness property of a norm. It is still a semi-norm.
        \begin{defn}
            Let $V$ be a vector space. A \textbf{semi-norm} on $V$ is a function $\norm{\cdot}:V\to\mathbb{R}$ such that for $\mathbf{x},\mathbf{y}\in V$:
            \begin{itemize}
                \item[1.] $\norm{\mathbf{x}}\geq 0$,
                \item[2.] $\norm{\alpha\mathbf{x}}=\abs{\alpha}\norm{\mathbf{x}}$ for $\alpha\in\mathbb{R}$,
                \item[3.] $\norm{\mathbf{x}+\mathbf{y}}\leq\norm{\mathbf{x}}+\norm{\mathbf{y}}$.
            \end{itemize}
        \end{defn}
        Since we only want to identify similarity, non-negativity is sufficient for our task.
        \item Evaluation: Which SPSD matrices are the best?
        
        Distance should be small for pairs in $S$, while distance should be large for pairs in $D$. Since the distance can only approach $0$, but extends to $\infty$, we can start large and minimize the distance for the pairs in $S$.
        \begin{equation*}
            \min_{\mathbf{A}\in\mathbb{R}^{n\times n}\text{ is SPSD}}\sum_{(\mathbf{x}_{i},\mathbf{x}_{j})\in S}\pnorm[\mathbf{A}]{\mathbf{x}_{i}-\mathbf{x}_{j}}^{2}\text{ such that }\sum_{(\mathbf{x}_{i},\mathbf{x}_{j})\in D}\pnorm[\mathbf{A}]{\mathbf{x}_{i}-\mathbf{x}_{j}}^{2}\geq 1.
        \end{equation*}
        \item Optimization: We do not perform it here.
    \end{enumerate}

\chapter{Linear Regression}
    \label{Case Study D: Linear Regression}
    This case study assumes that you have already read Chapter \ref{Chapter 4: Linear Functions and Differentiation}.

    Suppose that we are given $N$ input-output pairs:
    \begin{equation*}
        (\mathbf{x}_{1}, y_{1}), \cdots, (\mathbf{x}_{N}, y_{N})
    \end{equation*}
    where $\mathbf{x}_{i} \in \mathbb{R}^{n}$ and $y_{i} \in \mathbb{R}$ for $i=1,\cdots,N$. We aim to predict the corresponding output of any given new $\mathbf{x}$.
    \begin{enumerate}
        \item Representations: Starting with $N$ input-output pairs $\{(\mathbf{x}_{i}, y_{i})\}_{i=1}^{N}$, we define the following variables:
        \begin{enumerate}
            \item $\mathbf{x}_{i} \in \mathbb{R}^{n}$: the input vectors (independent variable).
            \item $y_{i} \in \mathbb{R}$: the corresponding output of $\mathbf{x}_{i}$ (dependent variable).
            \item $f:\mathbb{R}^{n} \to \mathbb{R}$: the function that predicts the output.
        \end{enumerate}
        \item Evaluation: We aim to find the function $f$ such that:
        \begin{equation*}
            f(\mathbf{x}_{i}) = y_{i}, \qquad\text{for } i = 1, \cdots, N.
        \end{equation*}
        \begin{enumerate}
            \item The set of all functions that map from $\mathbb{R}^{n}$ to $\mathbb{R}$ is too large because:
            \begin{enumerate}
                \item We do not have enough data to determine $f$ uniquely.
                \item There are too many functions that are not suitable for prediction.
            \end{enumerate}
            Therefore, we find $f$ in a subset $\Phi$ of all functions. 
            \item The choice of $\Phi$ is fundamental since too small a subset would result in weak approximation power. We may choose a large enough subset:
            \begin{equation*}
                \Phi = \{\text{all affine functions } f:\mathbb{R}^{n} \to \mathbb{R}\}.
            \end{equation*}
            Our objective becomes finding a linear model $f$.
            \item By Theorem \ref{Chapter 4 (Theorem) Properties of affine functions}(2), we can rewrite our function as:
            \begin{equation*}
                f(\mathbf{x}) = \inprod{\mathbf{a}}{\mathbf{x}} + b, \qquad\text{for } \mathbf{x} \in \mathbb{R}^{n}
            \end{equation*}
            where $\mathbf{a} \in \mathbb{R}^{n}$ and $b \in \mathbb{R}$. Therefore, our problem is to solve:
            \begin{equation*}
                \min_{\substack{\mathbf{a} \in \mathbb{R}^{n}\\ b \in \mathbb{R}}}\sum_{i=1}^{N}(\inprod{\mathbf{a}}{\mathbf{x}_{i}} + b - y_{i})^{2} \Longleftrightarrow \min_{\substack{\mathbf{a} \in \mathbb{R}^{n}\\ b \in \mathbb{R}}}\sum_{i=1}^{N}(\mathbf{x}_{i}^{T}\mathbf{a} + b - y_{i})^{2}
            \end{equation*}
            \item We write that:
            \begin{align*}
                \mathbf{X} &= \begin{pmatrix}
                    \mathbf{x}_{1}^{T} & 1\\
                    \vdots & \vdots\\
                    \mathbf{x}_{N}^{T} & 1
                \end{pmatrix} \in \mathbb{R}^{N \times (n+1)} & \boldsymbol{\beta} &= \begin{pmatrix}
                    \mathbf{a}\\
                    b
                \end{pmatrix} \in \mathbb{R}^{n+1} & \mathbf{y} &= \begin{pmatrix}
                    y_{1}\\
                    \vdots\\
                    y_{N}
                \end{pmatrix} \in \mathbb{R}^{N}
            \end{align*}
            Therefore, our problem now is to solve:
            \begin{equation*}
                \min_{\substack{\mathbf{a} \in \mathbb{R}^{n}\\ b \in \mathbb{R}}}\sum_{i=1}^{N}(\mathbf{x}_{i}^{T}\mathbf{a} + b - y_{i})^{2} \Longleftrightarrow \min_{\boldsymbol{\beta} \in \mathbb{R}^{n+1}} \pnorm[2]{\mathbf{X}\boldsymbol{\beta}-\mathbf{y}}^{2}
            \end{equation*}
        \end{enumerate}
        \newpage
        
        \item Optimization: For a limited amount of data, can we always find a unique solution? In order to solve the following equation,
        \begin{equation*}
            \mathbf{X}\boldsymbol{\beta} = \mathbf{y}
        \end{equation*}
        which has $N$ equations and $n+1$ unknowns, we would need $N \geq n+1$ for a unique solution. 
        
        However, in practice, we may also have cases when $N << n$. For example, in image recognition. Therefore, we need to perform some regularizations to further shrink the set of candidate functions. We will discuss this later. The method on how to find the solution numerically can be found in Appendix \ref{Case Study H: Gradient Descent}.
    \end{enumerate}
    The method is called the least squares.
    \begin{defn}
        The \textbf{(linear) least squares regression} is a regression method that estimates the model $f$ that best fits the given $N$ input-output pairs, such that:
        \begin{equation*}
            f(\mathbf{x}_{i}) = \mathbf{x}_{i}^{T}\hat{\boldsymbol{\beta}} \approx y_{i}, \qquad \text{for } i = 1,\cdots,N
        \end{equation*}
        where $\hat{\boldsymbol{\beta}}$ is the solution of:
        \begin{equation*}
            \min_{\boldsymbol{\beta} \in \mathbb{R}^{n+1}} \pnorm[2]{\mathbf{X}\boldsymbol{\beta}-\mathbf{y}}^{2}
        \end{equation*}
    \end{defn}
    \begin{thm}
        If $\mathbf{X}^{T}\mathbf{X}$ is invertible, then least squares regression has a unique solution for $\boldsymbol{\beta}$:
        \begin{equation*}
            \hat{\boldsymbol{\beta}} = (\mathbf{X}^{T}\mathbf{X})^{-1}\mathbf{X}^{T}\mathbf{y}
        \end{equation*}
    \end{thm}
    \begin{proofing}
        Let $X_{ij}$ be the $(i,j)$-th entry of $\mathbf{X}$ such that $X_{i(n+1)}=1$. We can rewrite the minimization problem:
        \begin{equation*}
            \min_{\boldsymbol{\beta} \in \mathbb{R}^{n+1}} \pnorm[2]{\mathbf{X}\boldsymbol{\beta}-\mathbf{y}}^{2} \Longleftrightarrow \min_{\beta_{1},\cdots,\beta_{n+1}\in\mathbb{R}} \sum_{i=1}^{N}(X_{i1}\beta_{1}+\cdots+X_{i(n+1)}\beta_{n+1}-y_{i})^{2}
        \end{equation*}
        Taking the derivatives of $\beta_{j}$ for $j=1,\cdots,n+1$, we have:
        \begin{equation*}
            2\sum_{i=1}^{N}X_{ij}(X_{i1}\beta_{1}+\cdots+X_{i(n+1)}\beta_{n+1}-y_{i}) = 0 \Longleftrightarrow \sum_{i=1}^{N}X_{ij}(X_{i1}\beta_{1}+\cdots+X_{i(n+1)}\beta_{n+1}) = \sum_{i=1}^{N}X_{ij}y_{i}
        \end{equation*}
        By substituting:
        \begin{align*}
            \mathbf{X}^{T}\mathbf{X} &= \begin{pmatrix}
                \sum_{i=1}^{N}X_{i1}^{2} & \sum_{i=1}^{N}X_{i1}X_{i2} & \hdots & \sum_{i=1}^{N}X_{i1}X_{i(n+1)}\\
                \sum_{i=1}^{N}X_{i2}X_{i1} & \sum_{i=1}^{N}X_{i2}^{2} & \hdots & \sum_{i=1}^{N}X_{i2}X_{i(n+1)}\\
                \vdots & \vdots & \ddots & \vdots\\
                \sum_{i=1}^{N}X_{i(n+1)}X_{i1} & \sum_{i=1}^{N}X_{i(n+1)}X_{i2} & \hdots & \sum_{i=1}^{N}X_{i(n+1)}^{2}
            \end{pmatrix}
        \end{align*}
        We can find that:
        \begin{equation*}
            \mathbf{X}^{T}\mathbf{X}\boldsymbol{\beta} = \mathbf{X}^{T}\mathbf{y}
        \end{equation*}
        Therefore, $\hat{\boldsymbol{\beta}} = (\mathbf{X}^{T}\mathbf{X})^{-1}\mathbf{X}^{T}\mathbf{y}$ if $\mathbf{X}^{T}\mathbf{X}$ is invertible.
    \end{proofing}
    In cases where the number of data points is significantly smaller than the dimension of the data, we have to use regularizations, which means that we have to use a subset of affine functions. The subset would be obtained by:
    \begin{equation*}
        \Phi_{S} = \left\{f: f(\mathbf{x}) = \inprod{\mathbf{a}}{\mathbf{x}} + b\text{ and }\boldsymbol{\beta}=\begin{pmatrix}
            \mathbf{a}\\
            b
        \end{pmatrix}\in S\right\}
    \end{equation*}
    for a given set $S$. Therefore, our problem now turns into solving:
    \begin{equation*}
        \min_{\boldsymbol{\beta} \in S}\pnorm[2]{\mathbf{X}\boldsymbol{\beta}-\mathbf{y}}^{2}
    \end{equation*}
    \newpage

    We can obtain two popular choices of regularizations.
    \begin{enumerate}
        \item Ridge regression: We choose
        \begin{equation*}
            S = \left\{\boldsymbol{\beta} = \begin{pmatrix}
                \mathbf{a}\\
                b
            \end{pmatrix}\in\mathbb{R}^{n+1} : \pnorm[2]{\mathbf{a}} \leq c\right\}
        \end{equation*}
        for some $c>0$. Therefore, the minimization problem is to solve:
        \begin{equation*}
            \min_{\boldsymbol{\beta} \in S}\pnorm[2]{\mathbf{X}\boldsymbol{\beta}-\mathbf{y}}^{2} \Longleftrightarrow \min_{\boldsymbol{\beta}=\begin{pmatrix}
                    \mathbf{a}\\
                    b
            \end{pmatrix} \in \mathbb{R}^{n+1}} \left(\pnorm[2]{\mathbf{X}\boldsymbol{\beta}-\mathbf{y}}^{2} + \lambda\pnorm[2]{\mathbf{a}}^{2}\right)
        \end{equation*}
        where $\lambda>0$ is a constant depending on $c$ and other factors. The ridge regression is defined as follows:
        \begin{defn}
            The \textbf{ridge regression} is a regression method that estimates the model $f$ that best fits the given $N$ input-output pairs, such that:
            \begin{equation*}
                f(\mathbf{x}_{i}) = \mathbf{x}_{i}^{T}\hat{\boldsymbol{\beta}} \approx y_{i}, \qquad \text{for } i = 1,\cdots,N
            \end{equation*}
            where $\hat{\boldsymbol{\beta}}$ is the solution of:
            \begin{equation*}
                \min_{\boldsymbol{\beta}=\begin{pmatrix}
                    \mathbf{a}\\
                    b
                \end{pmatrix} \in \mathbb{R}^{n+1}} \left(\pnorm[2]{\mathbf{X}\boldsymbol{\beta}-\mathbf{y}}^{2} + \lambda\pnorm[2]{\mathbf{a}}^{2}\right), \qquad \text{for } \lambda>0
            \end{equation*}
        \end{defn}
        \begin{thm}
            Ridge regression has a unique solution for $\boldsymbol{\beta}$:
            \begin{equation*}
                \hat{\boldsymbol{\beta}}=(\mathbf{X}^{T}\mathbf{X}+\lambda\mathbf{I})^{-1}\mathbf{X}^{T}\mathbf{y}
            \end{equation*}
        \end{thm}
        This minimization problem can be separated into three parts:
        \begin{enumerate}
            \item Data-fitting term: $\pnorm[2]{\mathbf{X}\boldsymbol{\beta}-\mathbf{y}}^{2}$.
            \item Regularization term: $\pnorm[2]{\mathbf{a}}^{2}$.
            \item Regularization parameter: $\lambda$.
            
            A larger $\lambda$ means a smaller $\Phi_{S}$, allowing for looser fitting.\\ 
            A smaller $\lambda$ means a larger $\Phi_{S}$, allowing for better fitting.
        \end{enumerate}
        \item LASSO regression: We choose
        \begin{equation*}
            S = \left\{\boldsymbol{\beta} = \begin{pmatrix}
                \mathbf{a}\\
                b
            \end{pmatrix}\in\mathbb{R}^{n+1} : \pnorm[1]{\mathbf{a}} \leq c\right\}
        \end{equation*}
        for some $c>0$. Therefore, the minimization problem is to solve:
        \begin{equation*}
            \min_{\boldsymbol{\beta} \in S}\pnorm[2]{\mathbf{X}\boldsymbol{\beta}-\mathbf{y}}^{2} \Longleftrightarrow \min_{\boldsymbol{\beta}=\begin{pmatrix}
                    \mathbf{a}\\
                    b
                \end{pmatrix} \in \mathbb{R}^{n+1}} \left(\pnorm[2]{\mathbf{X}\boldsymbol{\beta}-\mathbf{y}}^{2} + \lambda\pnorm[1]{\mathbf{a}}^{2}\right)
        \end{equation*}
        where $\lambda>0$ is a constant depending on $c$ and other factors. The LASSO regression is defined as follows:
        \begin{defn}
            The \textbf{LASSO regression} is a regression method that estimates the model $f$ that best fits the given $N$ input-output pairs, such that:
            \begin{equation*}
                f(\mathbf{x}_{i}) = \mathbf{x}_{i}^{T}\hat{\boldsymbol{\beta}} \approx y_{i}, \qquad \text{for } i = 1,\cdots,N
            \end{equation*}
            where $\hat{\boldsymbol{\beta}}$ is the solution of:
            \begin{equation*}
                \min_{\boldsymbol{\beta}=\begin{pmatrix}
                        \mathbf{a}\\
                        b
                    \end{pmatrix} \in \mathbb{R}^{n+1}} \left(\pnorm[2]{\mathbf{X}\boldsymbol{\beta}-\mathbf{y}}^{2} + \lambda\pnorm[1]{\mathbf{a}}^{2}\right), \qquad \text{for } \lambda>0
            \end{equation*}
        \end{defn}
        Similarly, this minimization problem can be separated into three parts:
        \begin{enumerate}
            \item Data-fitting term: $\pnorm[2]{\mathbf{X}\boldsymbol{\beta}-\mathbf{y}}^{2}$.
            \item Regularization term: $\pnorm[1]{\mathbf{a}}^{2}$.
            \item Regularization parameter: $\lambda$.
        \end{enumerate}
    \end{enumerate}
    \newpage

    What are the differences between these two types of regularizations? 

    In general, LASSO gives a sparse vector $\mathbf{a}$ (more zeros in $\mathbf{a}$). Before we dive in, we first define the support of a vector.
    \begin{defn}
        Let $V$ be a vector space over $\mathbb{R}$ and let $\mathbf{x} \in V$. The \textbf{support} of $\mathbf{x}$ is given by:
        \begin{equation*}
            \supp(\mathbf{x}) = \{i : x_{i} \neq 0\}.
        \end{equation*}
    \end{defn}
    The model now predicts the output by:
    \begin{equation*}
        f(\mathbf{x}) = \inprod{\mathbf{a}}{\mathbf{x}} + b = \sum_{i=1}^{n}a_{i}x_{i} + b = \sum_{i \in \supp(\mathbf{a})}a_{i}x_{i} + b
    \end{equation*}
    Only $x_{i}$ where $i \in \supp(\mathbf{a})$ contributes to the prediction. If $\mathbf{a}$ is sparse, then:
    \begin{equation*}
        \abs{\supp(\mathbf{a})} << n
    \end{equation*}
    This allows for more interpretable predictions.

\chapter{Kernel Ridge Regression}
    \label{Case Study E: Kernel Ridge Regression}
    This case study assumes that you have already read Chapter \ref{Chapter 4: Linear Functions and Differentiation} and Appendices \ref{Case Study B: Kernel K-means/Kernel Trick} and \ref{Case Study D: Linear Regression}.

    Similarly, in regular linear regression, we assume that a linear model is sufficient to predict a desirable output. In practice, however, points in datasets often exhibit non-linear relationships. Therefore, similar to the Kernel K-means algorithm, we can also apply the kernel method in regression.

    \begin{enumerate}
        \item Representations: Starting with $N$ input-output pairs $\{(\mathbf{x}_{i}, y_{i})\}_{i=1}^{N}$, we define the following variables:
        \begin{enumerate}
            \item $\mathbf{x}_{i} \in \mathbb{R}^{n}$: the input vectors (independent variables).
            \item $y_{i} \in \mathbb{R}$: the corresponding outputs of $\mathbf{x}_{i}$ (dependent variables).
            \item $H$: the feature space that contains the transformed input vectors.
            \item $\phi:\mathbb{R}^{n} \to H$: the feature map that transforms the input vectors.
            \item $f:H \to \mathbb{R}$: the function that predicts the outputs.
        \end{enumerate}
        \item Evaluation: With the feature map, our objective becomes finding an affine function $f$ such that:
        \begin{equation*}
            f(\phi(\mathbf{x}_{i})) = y_{i}, \qquad \text{for } i=1,\cdots,N.
        \end{equation*}
        How do we find a good $H$, $\phi$, and $f$? Assume that $f:H \to \mathbb{R}$ is affine and bounded. For any $\mathbf{x} \in \mathbb{R}^{n}$, we may define a new $\widetilde{\phi}$ and $\widetilde{H}=(H,\mathbb{R})$: 
        \begin{equation*}
            \widetilde{\phi}(\mathbf{x}) = \begin{pmatrix}
                \phi(\mathbf{x})\\
                1
            \end{pmatrix} \in \widetilde{H}.
        \end{equation*}
        Then for any $\mathbf{x} \in \mathbb{R}^{n}$,
        \begin{align*}
            f(\phi(\mathbf{x})) &= \inprod{\phi(\mathbf{x})}{\mathbf{a}} + b\\
            &= \inprod{\begin{pmatrix}
                \phi(\mathbf{x})\\
                1
            \end{pmatrix}}{\begin{pmatrix}
                \mathbf{a}\\
                b
            \end{pmatrix}}\\
            &= \inprod{\widetilde{\phi}(\mathbf{x})}{\widetilde{\mathbf{a}}}, \qquad \text{where } \widetilde{\mathbf{a}} = \begin{pmatrix}
                \mathbf{a}\\
                b
            \end{pmatrix} \in \widetilde{H},
        \end{align*}
        where $\mathbf{a} \in H$ and $b \in \mathbb{R}$. Therefore, $f(\phi(\mathbf{x}))$ is a linear and bounded function on $\widetilde{H}$.
        
        Thus, we only need to consider linear and bounded functions on $H$, which means that we want to find $\mathbf{a} \in H$ such that:
        \begin{equation*}
            \inprod{\mathbf{a}}{\phi(\mathbf{x}_{i})} \approx y_{i}, \qquad \text{for } i=1,\cdots,N.
        \end{equation*}
        Therefore, our problem is to solve:
        \begin{equation*}
            \min_{\mathbf{a} \in \mathbb{R}^{n}} \sum_{i=1}^{N}(\inprod{\mathbf{a}}{\phi(\mathbf{x}_{i})}-y_{i})^{2}.
        \end{equation*}
        We still need to find explicit $\phi$ and $H$.
        \item Optimization: 
        \begin{enumerate}
            \item The feature space $H$ is infinitely dimensional in most cases. It is impossible to find $\mathbf{a}$ with $N$ data points only. Therefore, we need to perform regularization directly.
            \item We can utilize the Kernel trickdefine the kernel function $\kappa$ implicitly.
        \end{enumerate}
    \end{enumerate}
    \newpage

    How do we perform regularization? Recall that we use a subset of affine functions when performing regularization for regular ridge regression. The subset would now be:
    \begin{equation*}
        \Phi_{S} = \left\{f:f(\mathbf{x})=\inprod{\mathbf{a}}{\phi(\mathbf{x})} \text{ and } \mathbf{a} \in S\right\}.
    \end{equation*}
    Assume that $H$ has a norm $\pnorm[H]{\cdot}$. We choose
    \begin{equation*}
        S = \left\{\mathbf{a} \in \mathbb{R}^{n} : \pnorm[H]{\mathbf{a}} \leq c\right\},
    \end{equation*}
    for some $c>0$. Therefore, the minimization problem is to solve:
    \begin{equation*}
        \tag{KR}
        \min_{\mathbf{a} \in H} \left(\sum_{i=1}^{N}(\inprod{\mathbf{a}}{\phi(\mathbf{x}_{i})}-y_{i})^{2} + \lambda\pnorm[H]{\mathbf{a}}^{2}\right),
    \end{equation*}
    where $\lambda>0$ is a constant depending on $c$ and other factors.

    How do we find the vector $\mathbf{a}$? The following theorem provides the solution.
    \begin{thm}\named{Representer Theorem}
        The solution of (KR) must be in the form:
        \begin{equation*}
            \mathbf{a} = \sum_{i=1}^{N} c_{i}\phi(\mathbf{x}_{i}), \qquad \text{where } \mathbf{c} = \begin{pmatrix}
                c_{1}\\
                \vdots\\
                c_{N}
            \end{pmatrix} \in \mathbb{R}^{N}.
        \end{equation*}
    \end{thm}
    \begin{proofing}
        For any $\mathbf{a} \in H$, we claim that $\mathbf{a}$ can be decomposed as:
        \begin{align*}
            \mathbf{a} &= \mathbf{a}_{S} + \sum_{i=1}^{N}c_{i}\phi(\mathbf{x}_{i}), & \qquad \text{where } \mathbf{c} = \begin{pmatrix}
                c_{1}\\
                \vdots\\
                c_{N}
            \end{pmatrix} \in \mathbb{R}^{N}, \inprod{\mathbf{a}_{S}}{\phi(\mathbf{x}_{i})} = 0, \qquad \text{for } i=1,\cdots,N.
        \end{align*}
        We first prove the case when $N=1$. Let $S = \{\mathbf{v} \in H: \inprod{\mathbf{v}}{\phi(\mathbf{x}_{1})} = 0\}$, which is a hyperplane. We may write $\mathbf{a}$ as:
        \begin{equation*}
            \mathbf{a} = P_{S}\mathbf{a} + (\mathbf{a}-P_{S}\mathbf{a}).
        \end{equation*}
        By Theorem \ref{Chapter 4 (Theorem) Criteria for Projection Vector}, setting $\mathbf{x}=\mathbf{0}$, we have:
        \begin{equation*}
            \inprod{\mathbf{a}-P_{S}\mathbf{a}}{P_{S}\mathbf{a}} = 0.
        \end{equation*}
        Moreover, by the explicit formula for $P_{S}\mathbf{a}$,
        \begin{equation*}
            \mathbf{a}-P_{S}\mathbf{a} = \frac{\inprod{\phi(\mathbf{x}_{1})}{\mathbf{a}}}{\pnorm[H]{\phi(\mathbf{x}_{1})}^{2}} \phi(\mathbf{x}_{1}).
        \end{equation*}
        Therefore, by setting:
        \begin{align*}
            c_{i} &= \frac{\inprod{\phi(\mathbf{x}_{1})}{\mathbf{a}}}{\pnorm[H]{\phi(\mathbf{x}_{1})}^{2}}, & \mathbf{a}_{S} = P_{S}\mathbf{a},
        \end{align*}
        we obtain:
        \begin{equation*}
            \mathbf{a} = \mathbf{a}_{S} + c_{1}\phi(\mathbf{x}_{1}), \qquad \text{where } \inprod{\mathbf{a}_{S}}{\phi(\mathbf{x}_{1})} = 0.
        \end{equation*}
        For general $N$, we can use projection onto the intersection of the hyperplanes:
        \begin{equation*}
            S = \bigcap_{i=1}^{N} \{\mathbf{v} \in H:\inprod{\mathbf{v}}{\phi(\mathbf{x}_{i})} = 0\}.
        \end{equation*}
        This concludes the claim. The proof continues on the next page.
        \newpage
        
        We may rearrange the objective function:
        \begin{align*}
            \sum_{i=1}^{N}(\inprod{\mathbf{a}}{\phi(\mathbf{x}_{i})}-y_{i})^{2} + \lambda\pnorm[H]{\mathbf{a}}^{2} &= \sum_{i=1}^{N}\left(\inprod{\mathbf{a}_{S} + \sum_{j=1}^{N}c_{i}\phi(\mathbf{x}_{j})}{\phi(\mathbf{x}_{i})}-y_{i}\right)^{2} + \lambda\pnorm[H]{\mathbf{a}_{S} + \sum_{i=1}^{N}c_{i}\phi(\mathbf{x}_{i})}^{2}\\
            &= \sum_{i=1}^{N}\left(\sum_{j=1}^{N}c_{j}\inprod{\phi(\mathbf{x}_{j})}{\phi(\mathbf{x}_{i})}-y_{i}\right)^{2} + \lambda\pnorm[H]{\sum_{i=1}^{N}c_{i}\phi(\mathbf{x}_{i})}^{2} + \lambda\pnorm[H]{\mathbf{a}_{S}}^{2}\\
            &= \sum_{i=1}^{N}\left(\sum_{j=1}^{N}c_{j}\inprod{\phi(\mathbf{x}_{j})}{\phi(\mathbf{x}_{i})}-y_{i}\right)^{2} + \lambda \sum_{i=1}^{N}\sum_{j=1}^{N}c_{i}c_{j}\inprod{\phi(\mathbf{x}_{i})}{\phi(\mathbf{x}_{j})} + \lambda\pnorm[H]{\mathbf{a}_{S}}^{2}.
        \end{align*}
        We can define the kernel function and kernel matrix as follows:
        \begin{align*}
            \kappa(\mathbf{x},\mathbf{y}) &= \inprod{\phi(\mathbf{x})}{\phi(\mathbf{y})}, \qquad \text{for } \mathbf{x},\mathbf{y} \in \mathbb{R}^{n}, & \boldsymbol{\kappa} &= [\kappa(\mathbf{x}_{i},\mathbf{x}_{j})]_{i,j}.
        \end{align*}
        Therefore, we may rewrite the objective function as:
        \begin{equation*}
            \tag{P1}
            \text{(KR)} \Longleftrightarrow \min_{\substack{\mathbf{c} \in \mathbb{R}^{N}\\\mathbf{a}_{S} \in H}}(\spnorm[2]{\boldsymbol{\kappa}^{T}\mathbf{c}-\mathbf{y}}^{2} + \lambda \mathbf{c}^{T}\boldsymbol{\kappa}\mathbf{c} + \lambda\pnorm[H]{\mathbf{a}_{S}}^{2}), \qquad \text{such that } \inprod{\mathbf{a}_{S}}{\phi(\mathbf{x}_{i})}=0 \text{ for } i=1,\cdots,N.
        \end{equation*}
        For now, we drop the constraints in (P1):
        \begin{equation*}
            \tag{P2}
            \min_{\substack{\mathbf{c} \in \mathbb{R}^{N}\\\mathbf{a}_{S} \in H}}(\spnorm[2]{\boldsymbol{\kappa}^{T}\mathbf{c}-\mathbf{y}}^{2} + \lambda \mathbf{c}^{T}\boldsymbol{\kappa}\mathbf{c} + \lambda\pnorm[H]{\mathbf{a}_{S}}^{2}) \Longleftrightarrow \min_{\mathbf{c} \in \mathbb{R}^{n}}(\spnorm[2]{\boldsymbol{\kappa}^{T}\mathbf{c}-\mathbf{y}}^{2} + \lambda \mathbf{c}^{T}\boldsymbol{\kappa}\mathbf{c}) \text{ and } \min_{\mathbf{a}_{S} \in H} \lambda \pnorm[H]{\mathbf{a}_{S}}^{2}.
        \end{equation*}
        We set the solution of (P2) to be:
        \begin{align*}
            \mathbf{c}^{*} &= \argmin_{\mathbf{c} \in \mathbb{R}^{N}} (\spnorm[2]{\boldsymbol{\kappa}^{T}\mathbf{c}-\mathbf{y}}^{2} + \lambda \mathbf{c}^{T}\boldsymbol{\kappa}\mathbf{c}), & \mathbf{a}_{S}^{*} &= \mathbf{0}.
        \end{align*}
        We show that $(\mathbf{c}^{*},\mathbf{a}_{S}^{*})$ is also a solution of (P1).
        
        Since (P2) only removed the constraints from (P1), we find that:
        \begin{equation*}
            \text{(P1)} \geq \text{(P2)}.
        \end{equation*}
        Since $\mathbf{a}_{S}^{*} = \mathbf{0}$, we find that:
        \begin{equation*}
            \inprod{\mathbf{a}_{S}^{*}}{\phi(\mathbf{x}_{i})} = 0, \qquad \text{for } i=1,\cdots,N.
        \end{equation*}
        Therefore, $(\mathbf{c}^{*},\mathbf{a}_{S}^{*})$ satisfies the constraints in (P1), and we have:
        \begin{equation*}
            \text{(P1)} \leq \text{(P2)}.
        \end{equation*}
        Thus, $(\mathbf{c}^{*},\mathbf{a}_{S}^{*})$ is a solution of (P1), and the solution of (KR) is given by:
        \begin{equation*}
            \mathbf{a} = \mathbf{a}_{S}^{*} + \sum_{i=1}^{N}c_{i}^{*}\phi(\mathbf{x}_{i}) = \sum_{i=1}^{N}c_{i}^{*}\phi(\mathbf{x}_{i}), \qquad \text{where } \mathbf{c}^{*} = \argmin_{\mathbf{c} \in \mathbb{R}^{N}} (\spnorm[2]{\boldsymbol{\kappa}^{T}\mathbf{c}-\mathbf{y}}^{2} + \lambda \mathbf{c}^{T}\boldsymbol{\kappa}\mathbf{c}).
        \end{equation*}
    \end{proofing}
    \newpage

    Similar to the K-Means Algorithm, we may define Kernel ridge regression as follows:
    \begin{defn}
        \textbf{Kernel ridge regression} is a regression method that estimates the model $f$ that best fits the given $N$ input-output pairs, where each input vector is transformed such that:
        \begin{equation*}
            f(\phi(\mathbf{x}_{i})) \approx y_{i}, \qquad \text{for } i=1,\cdots,N.
        \end{equation*}
        The steps of the algorithm are as follows:
        \begin{itemize}
            \item[1:] Choose a kernel function $\kappa$.
            \item[2:] Form a kernel matrix $\boldsymbol{\kappa} = [\kappa(\mathbf{x}_{i},\mathbf{x}_{j})]_{i,j} \in \mathbb{R}^{N \times N}$.
            \item[3:] Solve $\mathbf{c}^{*}$ from:
            \begin{equation*}
                \mathbf{c}^{*} = \argmin_{\mathbf{c} \in \mathbb{R}^{N}} (\spnorm[2]{\boldsymbol{\kappa}^{T}\mathbf{c}-\mathbf{y}}^{2} + \lambda \mathbf{c}^{T}\boldsymbol{\kappa}\mathbf{c}).
            \end{equation*}
            \item[4:] Obtain the regression function:
            \begin{equation*}
                f(\phi(\mathbf{x})) = \sum_{i=1}^{N} c_{i}^{*}\kappa(\mathbf{x}_{i},\mathbf{x}), \qquad \text{for } \mathbf{x} \in \mathbb{R}^{n}.
            \end{equation*}
        \end{itemize}
    \end{defn}

\chapter{Linear Classification}
    \label{Case Study F: Linear Classification}
    This case study assumes that you have already read Chapter \ref{Chapter 4: Linear Functions and Differentiation} and Appendices \ref{Case Study B: Kernel K-means/Kernel Trick} and \ref{Case Study E: Kernel Ridge Regression}.

    Suppose that we are given $N$ input-output pairs:
    \begin{equation*}
        (\mathbf{x}_{1},y_{1}), \cdots, (\mathbf{x}_{N},y_{N})
    \end{equation*}
    where $\mathbf{x}_{i} \in \mathbb{R}^{n}$ and $y_{i} \in \{-1,1\}$ for $i=1,\cdots,N$. We aim to classify any given new $\mathbf{x}$ as either $-1$ or $1$.
    \begin{enumerate}
        \item Representation: Starting with $N$ input-output pairs $\{(\mathbf{x}_{i},y_{i})\}_{i=1}^{N}$, we define the following variables:
        \begin{enumerate}
            \item $\mathbf{x}_{i} \in \mathbb{R}^{n}$: the input vectors.
            \item $y_{i} \in \{-1,1\}$: the corresponding class of $\mathbf{x}_{i}$.
            \item $f:\mathbb{R}^{n} \to \mathbb{R}$: the function that allows for classification based on its output for $\mathbf{x}_{i}$.
        \end{enumerate}
        \item Evaluation: We use the function $f$ to define a separation between two classes. 
        \begin{enumerate}
            \item Given a new $\mathbf{x} \in \mathbb{R}^{n}$, the predicted label would be:
            \begin{equation*}
                y = \sgn(f(\mathbf{x}))
            \end{equation*}
            Here, we use $\{\mathbf{x}:f(\mathbf{x})=0\}$ to separate the two classes. To enhance confidence, we should find a function such that:
            \begin{equation*}
                \begin{cases}
                    f(\mathbf{x}_{i}) \geq 1, &\text{if } y_{i}=1,\\
                    f(\mathbf{x}_{i}) \leq -1, &\text{if } y_{i}=-1
                \end{cases}, \qquad \text{for } i=1,\cdots,N.
            \end{equation*}
            \item What function class should $f$ belong to? Similar to linear regression, we can use the set of all affine functions. We need to find $\mathbf{a} \in \mathbb{R}^{n}$ and $b \in \mathbb{R}$ such that:
            \begin{equation*}
                \begin{cases}
                    \inprod{\mathbf{a}}{\mathbf{x}_{i}}+b \geq 1, &\text{if } y_{i}=1,\\
                    \inprod{\mathbf{a}}{\mathbf{x}_{i}}+b \leq -1, &\text{if } y_{i}=-1
                \end{cases}, \qquad \text{for } i=1,\cdots,N.
            \end{equation*}
            This means we are using hyperplanes to separate points in different classes.
            \item Which hyperplane is the best among all the possible ones? The larger the margin, the more stable the classification.
            \begin{defn}
                The \textbf{Support Vector Machine (SVM)} is an algorithm that finds the best hyperplane that maximizes the margin between the two classes. Let:
                \begin{align*}
                    S_{+} &= \{\mathbf{x}:\inprod{\mathbf{a}}{\mathbf{x}} + b = 1\}, & S_{-} &= \{\mathbf{x}:\inprod{\mathbf{a}}{\mathbf{x}} + b = -1\}.
                \end{align*}
                The margin is defined by:
                \begin{equation*}
                    \pnorm[2]{\mathbf{x}_{+} - \mathbf{x}_{-}}, \qquad \text{where } \begin{cases}
                        \mathbf{x}_{+} = P_{S_{+}}\mathbf{x}_{-},\\
                        \mathbf{x}_{-} = P_{S_{-}}\mathbf{x}_{+}.
                    \end{cases}
                \end{equation*}
            \end{defn}
            By the projection formula, since $\mathbf{x}_{-} \in S_{-}$,
            \begin{equation*}
                \mathbf{x}_{+} = \mathbf{x}_{-} - \left(\frac{\inprod{\mathbf{a}}{\mathbf{x}_{-}} - (1-b)}{\pnorm[2]{\mathbf{a}}^{2}}\right)\mathbf{a} = \mathbf{x}_{-} - \left(\frac{(-1-b) - (1-b)}{\pnorm[2]{\mathbf{a}}^{2}}\right)\mathbf{a} = \mathbf{x}_{-} + \frac{2}{\pnorm[2]{\mathbf{a}}^{2}}\mathbf{a}.
            \end{equation*}
            Therefore, the margin is $\pnorm[2]{\mathbf{x}_{+} - \mathbf{x}_{-}} = \frac{2}{\pnorm[2]{\mathbf{a}}}$.
        \end{enumerate}
    \end{enumerate}
    \newpage

    We can prevent data points from falling into the margin. Therefore, our problem is to solve:
    \begin{equation*}
        \max_{\substack{\mathbf{a} \in \mathbb{R}^{n}\\ b \in \mathbb{R}}}\frac{2}{\pnorm[2]{\mathbf{a}}} \text{ such that } \begin{cases}
            \inprod{\mathbf{a}}{\mathbf{x}_{i}} + b \geq 1, &\text{for } y_{i} = 1,\\
            \inprod{\mathbf{a}}{\mathbf{x}_{i}} + b \leq -1, &\text{for } y_{i} = -1
        \end{cases}, \qquad \text{for } i=1,\cdots,N.
    \end{equation*}
    The hard-margin SVM is defined as follows:
    \begin{defn}
        The \textbf{hard-margin Support Vector Machine} is an algorithm that finds the best hyperplane $S$ that maximizes the margin between the two classes, requiring that all data points be linearly separable and not allowed to be on the other side. The hyperplane is in the form of:
        \begin{equation*}
            S = \{\mathbf{x}:\inprod{\mathbf{a}}{\mathbf{x}}+b = 0\},
        \end{equation*}
        where $(\mathbf{a},b)$ is the solution of:
        \begin{equation*}
            \tag{SVM-1}
            \min_{\substack{\mathbf{a} \in \mathbb{R}^{n}\\ b \in \mathbb{R}}} \pnorm[2]{\mathbf{a}}^{2} \text{ such that } y_{i}(\inprod{\mathbf{a}}{\mathbf{x}_{i}}+b) \geq 1, \qquad \text{for } i=1,\cdots,N.
        \end{equation*}
    \end{defn}
    \begin{figure}[h]
        \centering
        \begin{tikzpicture}
            \draw[thick, dotted] (-0.3, 1.5) node[anchor=south] {$S$} -- (0.3, -1.5);
            \draw[thick] (-0.8, 1.5) node[anchor=south] {$S_{+}$} -- (-0.2, -1.5);
            \draw[thick] (0.2, 1.5) node[anchor=south] {$S_{-}$} -- (0.8, -1.5);
            \draw[red] plot [only marks, mark=*, mark size=0.5, domain=-1.5:-0.8, samples=80] (\x, {rnd*3-1.5});
            \draw[red] plot [only marks, mark=*, mark size=0.5, domain=-0.8:-0.25, samples=20] (\x, {min(rnd*-5*(\x+0.25)-1.5,rnd*3-1.5)});
            \draw[blue] plot [only marks, mark=*, mark size=0.5, domain=0.8:1.5, samples=80] (\x, {rnd*3-1.5});
            \draw[blue] plot [only marks, mark=*, mark size=0.5, domain=0.25:0.8, samples=20] (\x, {rnd*-5*(\x-0.25))+1.5});
        \end{tikzpicture}
        \caption{Hard-margin SVM}
    \end{figure}
    \begin{rem}
        (SVM-1) is not robust to noise in training data.
    \end{rem}
    \begin{rem}
        In some cases, the hyperplane cannot separate the two classes in the presence of high noise.
    \end{rem}
    \begin{figure}[h]
        \centering
        \begin{tikzpicture}
            \draw[red] plot [only marks, mark=*, mark size=0.5, domain=-1.5:0.2, samples=100] (\x, {rnd*3-1.5});
            \draw[blue] plot [only marks, mark=*, mark size=0.5, domain=-0.2:1.5, samples=100] (\x, {rnd*3-1.5});
        \end{tikzpicture}
        \caption{No solution to (SVM-1)}
    \end{figure}
    Is there a way to improve SVM? We need to first reformulate (SVM-1). We define a function $h:\mathbb{R} \to \mathbb{R}\cup\{+\infty\}$ by:
    \begin{equation*}
        h(t) = \begin{cases}
            0, &\text{if } t \geq 0,\\
            +\infty, &\text{if } t < 0.
        \end{cases}
    \end{equation*}
    Then we can rewrite (SVM-1) as:
    \begin{equation*}
        \text{(SVM-1)} \Longleftrightarrow \min_{\substack{\mathbf{a} \in \mathbb{R}^{n}\\ b \in \mathbb{R}}} \left(\sum_{i=1}^{N} h(y_{i}(\inprod{\mathbf{a}}{\mathbf{x}_{i}}+b)-1) + \lambda\pnorm[2]{\mathbf{a}}^{2}\right),
    \end{equation*}
    where $\lambda>0$. This still fulfills the constraint that no data points must sit on the margin.
    \newpage

    In order to consider the case when some data points cannot be separated with a hard margin, we can soften the function $h$ by defining it as:
    \begin{equation*}
        h(t) = \max(0, -t) = \begin{cases}
            0, &\text{if } t \geq 0,\\
            \abs{t}, &\text{if } t < 0.
        \end{cases}
    \end{equation*}
    This function is called the hinge loss function. The soft-margin SVM is defined as follows:
    \begin{defn}
        The \textbf{soft-margin Support Vector Machine} is an algorithm that finds the best hyperplane $S$ that maximizes the margin between the two classes, allowing some data points to be misclassified or fall within the margin. The hyperplane is in the form of:
        \begin{equation*}
            S = \{\mathbf{x}:\inprod{\mathbf{a}}{\mathbf{x}} + b = 0\},
        \end{equation*}
        where $(\mathbf{a},b)$ is the solution of:
        \begin{equation*}
            \tag{SVM-2}
            \min_{\substack{\mathbf{a} \in \mathbb{R}^{n}\\ b \in \mathbb{R}}} \left(\sum_{i=1}^{N}\max(0,1-y_{i}(\inprod{\mathbf{a}}{\mathbf{x}_{i}}+b)) + \lambda\pnorm[2]{\mathbf{a}}\right),
        \end{equation*}
        where $\lambda > 0$.
    \end{defn}
    \begin{rem}
        We can replace $h$ with some other functions. For example,
        \begin{equation*}
            h(t) = \log(1+e^{-t}),
        \end{equation*}
        which is a smooth function that approximates hinge loss, called logistic loss.
    \end{rem}
    So far, we have only introduced classifiers that are linear. Similar to what we did in regression, we can apply the Kernel trick to create a non-linear classifier. Consider a feature map $\phi:\mathbb{R}^{n} \to H$. The hyperplane will become:
    \begin{equation*}
        S = \{\mathbf{x}:\inprod{\mathbf{a}}{\phi(\mathbf{x})} = 0\}.
    \end{equation*}
    The formulation of SVM becomes:
    \begin{align*}
        \sum_{i=1}^{N} h(y_{i}\inprod{\mathbf{a}}{\phi(\mathbf{x}_{i})}-1) + \lambda\pnorm[H]{\mathbf{a}}^{2} &= \sum_{i=1}^{N} h\left(y_{i}\inprod{\mathbf{a}_{S} + \sum_{j=1}^{N}c_{j}\phi(\mathbf{x}_{j})}{\phi(\mathbf{x}_{i})}-1\right) + \lambda\pnorm[H]{\mathbf{a}_{S} + \sum_{j=1}^{N}c_{j}\phi(\mathbf{x}_{j})}^{2}\\
        &= \sum_{i=1}^{N} h\left(y_{i}\sum_{j=1}^{N}c_{j}\inprod{\phi(\mathbf{x}_{j})}{\phi(\mathbf{x}_{i})}-1\right) + \lambda\pnorm[H]{\mathbf{a}_{S}}^{2} + \lambda\sum_{i=1}^{N}\sum_{j=1}^{N}c_{i}c_{j}\inprod{\phi(\mathbf{x}_{i})}{\phi(\mathbf{x}_{j})}.
    \end{align*}
    We introduce the kernel function $\kappa(\mathbf{x},\mathbf{y}) = \inprod{\phi(\mathbf{x})}{\phi(\mathbf{y})}$ for any $\mathbf{x},\mathbf{y} \in \mathbb{R}^{n}$ and the kernel matrix $\boldsymbol{\kappa} = \{\kappa(\mathbf{x}_{i},\mathbf{x}_{j})\}_{i,j=1}^{N} \in \mathbb{R}^{N \times N}$. Therefore, we would be solving:
    \begin{equation*}
        \tag{K-SVM}
        \min_{\substack{\mathbf{c} \in \mathbb{R}^{N}\\ \mathbf{a}_{S} \in H}} (h(y_{i}[\boldsymbol{\kappa}^{T}\mathbf{c}]_{i}-1) + \lambda \mathbf{c}^{T}\boldsymbol{\kappa}\mathbf{c} + \lambda\pnorm[H]{\mathbf{a}_{S}}^{2}), \qquad \text{such that } \inprod{\mathbf{a}_{S}}{\phi(\mathbf{x}_{i})} = 0 \text{ for } i=1,\cdots,N.
    \end{equation*}
    Similar to Appendix \ref{Case Study E: Kernel Ridge Regression}, by the Representer Theorem, the solution of (K-SVM) is:
    \begin{align*}
        \mathbf{a} &= \sum_{i=1}^{N} c_{i}^{*}\phi(\mathbf{x}_{i}), & \mathbf{c}^{*} &= \argmin_{\mathbf{c} \in \mathbb{R}^{N}} \left(\sum_{i=1}^{N}h(y_{i}[\kappa^{T}\mathbf{c}]_{i}-1) + \lambda\mathbf{c}^{T}\boldsymbol{\kappa}\mathbf{c}\right), & \mathbf{a}_{S} &= \mathbf{0}.
    \end{align*}
    \newpage

    Therefore, we may define the Kernel SVM as follows:
    \begin{defn}
        The \textbf{Kernel Support Vector Machine} is an algorithm that finds the best hyperplane that maximizes the margin between the two classes in the feature space, where the input vectors are transformed. The hyperplane is in the form of:
        \begin{equation*}
            S = \{\mathbf{x}:\inprod{\mathbf{a}}{\phi(\mathbf{x})} = 0\},
        \end{equation*}
        where $\mathbf{a}$ can be found by the following algorithm:
        \begin{itemize}
            \item[1:] Choose a kernel function $\kappa$.
            \item[2:] Form a kernel matrix $\boldsymbol{\kappa} = [\kappa(\mathbf{x}_{i},\mathbf{x}_{j})]_{i,j} \in \mathbb{R}^{N \times N}$.
            \item[3:] Solve $\mathbf{c}^{*}$ from:
            \begin{equation*}
                \argmin_{\mathbf{c} \in \mathbb{R}^{N}} \left(\sum_{i=1}^{N}h(y_{i}[\kappa^{T}\mathbf{c}]_{i}-1) + \lambda\mathbf{c}^{T}\boldsymbol{\kappa}\mathbf{c}\right).
            \end{equation*}
            \item[4:] Obtain the classifier function:
            \begin{equation*}
                \inprod{\mathbf{a}}{\phi(\mathbf{x})} = \sum_{i=1}^{N}c_{i}^{*}\kappa(\mathbf{x}_{i},\mathbf{x}), \qquad \text{for } \mathbf{x} \in \mathbb{R}^{n}.
            \end{equation*}
            \item[5:] Predict the label of any input:
            \begin{equation*}
                y = \sgn\left(\sum_{i=1}^{N}c_{i}^{*}\kappa(\mathbf{x}_{i},\mathbf{x})\right), \qquad \text{for } \mathbf{x} \in \mathbb{R}^{n}.
            \end{equation*}
        \end{itemize}
    \end{defn}

\chapter{Solvability and Optimality}
    \label{Case Study G: Solvability and Optimality}
    This case study assumes that you have already read Chapter \ref{Chapter 4: Linear Functions and Differentiation}.

    Let $H$ be a Hilbert space. Suppose that we have a function $f: H \to \mathbb{R}$. We aim to find the solution of:
    \begin{equation*}
        \tag{OPT}
        \min_{\mathbf{x} \in H} f(\mathbf{x}).
    \end{equation*}
    \begin{enumerate}
        \item Representations: We define the following variables:
        \begin{enumerate}
            \item $\mathbf{x}^{*} \in H$: the solution of (OPT).
            \item $f: H \to \mathbb{R}$: the function that we want to minimize.
        \end{enumerate}
        \item Evaluation: We would mostly focus on this part. More specifically, we want to ask:
        \begin{enumerate}
            \item What optimality and solvability conditions does the solution need to satisfy?
            \item What conditions does the function need to satisfy to guarantee a solution?
        \end{enumerate}
        \item Optimization: This will be discussed in Appendix \ref{Case Study H: Gradient Descent}.
    \end{enumerate}
    From the problem, we can immediately say $\mathbf{x}$ is a solution of (OPT) if:
    \begin{equation*}
        \tag{0th optimality condition}
        \mathbf{x}^{*} = \argmin_{\mathbf{x} \in H} f(\mathbf{x}) \iff f(\mathbf{x}^{*}) \leq f(\mathbf{x}), \qquad \text{for } \mathbf{x} \in H.
    \end{equation*}
    \begin{defn}
        If $\mathbf{x}^{*} \in H$ satisfies the 0th optimality condition, then $\mathbf{x}^{*}$ is called a \textbf{global minimizer} of $f$.
    \end{defn}
    \begin{rem}
        The existence of $\mathbf{x}^{*}$ is not guaranteed automatically.
    \end{rem}
    \begin{eg}
        Let $f: \mathbb{R} \to \mathbb{R}$ be defined by:
        \begin{equation*}
            f(x) = x(x-1)(x+1), \qquad \text{for } x \in \mathbb{R}.
        \end{equation*}
        When $x \to -\infty$, $f(x) \to -\infty$. There is no solution to (OPT).
    \end{eg}
    \begin{eg}
        Let $f: H \to \mathbb{R}$ be defined by:
        \begin{equation*}
            f(\mathbf{x}) = \inprod{\mathbf{a}}{\mathbf{x}}, \qquad \text{for } \mathbf{x} \in H,
        \end{equation*}
        for some non-zero $\mathbf{a} \in H$. Then, setting $\mathbf{x} = c\mathbf{a}$ for some $c \in \mathbb{R}$, we have:
        \begin{equation*}
            f(c\mathbf{a}) = \inprod{\mathbf{a}}{c\mathbf{a}} = c\norm{\mathbf{a}}^{2}.
        \end{equation*}
        When $c \to -\infty$, $f(\mathbf{x}) \to -\infty$. Therefore, there is no solution to (OPT).
    \end{eg}
    \newpage

    \begin{eg}
        Let $f: \mathbb{R} \to \mathbb{R}$ be defined by:
        \begin{equation*}
            f(x) = \frac{1}{x}, \qquad \text{for } x > 0.
        \end{equation*}
        Since $f(x)$ is a decreasing function for $x > 0$, for any solution $x^{*}$, we can always find a more optimal solution larger than $x^{*}$. Therefore, there is no solution to (OPT).
    \end{eg}
    It seems that only the 0th-order optimality condition does not guarantee a solution. Let us investigate what additional conditions need to be satisfied.

    \begin{thm}
        \label{Case Study G (Theorem) Solution has Gradient=0}
        Let $f: H \to \mathbb{R}$ be differentiable at $\mathbf{x}^{*} \in H$. Then,
        \begin{equation*}
            \mathbf{x}^{*} = \argmin_{\mathbf{x} \in H} f(\mathbf{x}) \implies \nabla f(\mathbf{x}^{*}) = \mathbf{0}.
        \end{equation*}
    \end{thm}
    \begin{proofing}
        By the Taylor expansion, for any $\mathbf{x} \in H$,
        \begin{equation*}
            f(\mathbf{x}) = f(\mathbf{x}^{*}) + \inprod{\nabla f(\mathbf{x}^{*})}{\mathbf{x}-\mathbf{x}^{*}} + o(\norm{\mathbf{x}-\mathbf{x}^{*}}).
        \end{equation*}
        Suppose that $\nabla f(\mathbf{x}^{*}) \neq \mathbf{0}$. We choose $\widetilde{\mathbf{x}} = \mathbf{x}^{*} - t \nabla f(\mathbf{x}^{*})$ with $t > 0$. We have:
        \begin{equation*}
            f(\widetilde{\mathbf{x}}) = f(\mathbf{x}^{*}) + \inprod{\nabla f(\mathbf{x}^{*})}{-t \nabla f(\mathbf{x}^{*})} + o(t \norm{\nabla f(\mathbf{x}^{*})}) = f(\mathbf{x}^{*}) - t \norm{\nabla f(\mathbf{x}^{*})}^{2} + o(t \norm{\nabla f(\mathbf{x}^{*})}).
        \end{equation*}
        Since we have:
        \begin{equation*}
            \lim_{t \to 0} \frac{o(t \norm{\nabla f(\mathbf{x}^{*})})}{t \norm{\nabla f(\mathbf{x}^{*})}} = 0,
        \end{equation*}
        for all $c > 0$, there exists $t > 0$ such that:
        \begin{equation*}
            ct \norm{\nabla f(\mathbf{x}^{*})} > o(t \norm{\nabla f(\mathbf{x}^{*})}).
        \end{equation*}
        We choose $c = \norm{\nabla f(\mathbf{x}^{*})} > 0$. Then,
        \begin{equation*}
            t \norm{\nabla f(\mathbf{x}^{*})}^{2} > o(t \norm{\nabla f(\mathbf{x}^{*})}).
        \end{equation*}
        Therefore, $f(\widetilde{\mathbf{x}}) < f(\mathbf{x}^{*})$, which contradicts the 0th optimality condition. Thus, $\nabla f(\mathbf{x}^{*}) = \mathbf{0}$.
    \end{proofing}
    \begin{rem}
        The reverse is not true in general.
    \end{rem}
    If we just look at $\nabla f(\mathbf{x}^{*}) = 0$, what can it be?
    \begin{defn}
        For any $\mathbf{x}^{*} \in H$, if there exists $\varepsilon > 0$ such that for all $\mathbf{x} \in H$ such that $\norm{\mathbf{x}-\mathbf{x}^{*}} \leq \varepsilon$,
        \begin{equation*}
            f(\mathbf{x}^{*}) \leq f(\mathbf{x}),
        \end{equation*}
        then $\mathbf{x}^{*}$ is called a \textbf{local minimizer}.
    \end{defn}
    \begin{defn}
        For any $\mathbf{x}^{*} \in H$, if there exist $\mathbf{u}, \mathbf{v} \in H$ and $\varepsilon > 0$ such that for all $t \in \mathbb{R}$ with $\abs{t} \leq \varepsilon$,
        \begin{equation*}
            f(\mathbf{x}^{*}) \geq f(\mathbf{x}^{*} + t \mathbf{u}) \text{ and } f(\mathbf{x}^{*}) \leq f(\mathbf{x}^{*} + t \mathbf{v}),
        \end{equation*}  
        then $\mathbf{x}^{*}$ is called a \textbf{saddle point}.
    \end{defn}
    \begin{rem}
        Saddle points only exist if $\dim(H) \geq 2$.
    \end{rem}
    Global minimizers and maximizers, local minimizers and maximizers, and saddle points all satisfy $\nabla f(\mathbf{x}^{*}) = \mathbf{0}$. If we want only the global minimizer to satisfy this condition, what restrictions should we impose on the function?
    \newpage

    \begin{defn}
        Let $V$ be a vector space. A function $f: V \to \mathbb{R}$ is called \textbf{convex} if, for any $\mathbf{x}, \mathbf{y} \in V$ and $\alpha \in [0,1]$,
        \begin{equation*}
            f(\alpha \mathbf{x} + (1-\alpha)\mathbf{y}) \leq \alpha f(\mathbf{x}) + (1-\alpha)f(\mathbf{y}).
        \end{equation*}
    \end{defn}
    \begin{figure}[h]
        \centering
        \begin{subfigure}[h]{.4\textwidth}
            \centering
            \begin{tikzpicture}
                \draw plot [domain=-1.75:1.75] (\x, {\x*\x});
                \draw[blue] (-1.5, 2.25) -- (1, 1);
                \draw[blue] (-1, 1) -- (1.3, 1.69);
                \draw[blue] (-0.4, 0.16) -- (1.6, 2.56);
            \end{tikzpicture}
            \caption{Convex}
        \end{subfigure}
        \begin{subfigure}[h]{.4\textwidth}
            \centering
            \begin{tikzpicture}
                \draw plot [domain=-1.6:1.55] (\x, {\x*(\x+0.3)*(\x+1.3)*(\x-1.3)});
                \draw[red] (-1.5, 1.008) -- (1, -0.897);
                \draw[blue] (-1.4, 0.4158) -- (1.4, 0.6426);
            \end{tikzpicture}
            \caption{Non-convex}
        \end{subfigure}
    \end{figure}
    \begin{eg}
        Let $V$ be a vector space with norm $\norm{\cdot}$, and let $f: V \to \mathbb{R}$ be defined by:
        \begin{equation*}
            f(\mathbf{x}) = \norm{\mathbf{x}}, \qquad \text{for } \mathbf{x} \in V.
        \end{equation*}
        For all $\mathbf{x}, \mathbf{y} \in V$ and $\alpha \in [0,1]$, by the triangle inequality,
        \begin{equation*}
            f(\alpha \mathbf{x} + (1-\alpha) \mathbf{y}) = \norm{\alpha \mathbf{x} + (1-\alpha) \mathbf{y}} \leq \alpha\norm{\mathbf{x}} + (1-\alpha)\norm{\mathbf{y}} = \alpha f(\mathbf{x}) + (1-\alpha) f(\mathbf{y}).
        \end{equation*}
        Therefore, $f$ is convex.
    \end{eg}
    \begin{rem}
        $f(\mathbf{x}) = \norm{\mathbf{x}}$ is convex for any norm on $V$.
    \end{rem}
    \begin{eg}
        Let $V$ be a vector space with norm $\norm{\cdot}$, and let $f: V \to \mathbb{R}$ be defined by:
        \begin{equation*}
            f(\mathbf{x}) = \norm{\mathbf{x}}^{2}, \qquad \text{for } \mathbf{x} \in V.
        \end{equation*}
        For all $\mathbf{x}, \mathbf{y} \in V$ and $\alpha \in [0,1]$,
        \begin{align*}
            f(\alpha \mathbf{x} + (1-\alpha)\mathbf{y}) &= \norm{\alpha \mathbf{x} + (1-\alpha)\mathbf{y}}^{2}\\
            &= \alpha^{2}\norm{\mathbf{x}}^{2} + (1-\alpha)^{2}\norm{\mathbf{y}}^{2} + 2\alpha(1-\alpha)\inprod{\mathbf{x}}{\mathbf{y}}\\
            &= \alpha\norm{\mathbf{x}}^{2} + (1-\alpha)\norm{\mathbf{y}}^{2} + 2\alpha(1-\alpha)\inprod{\mathbf{x}}{\mathbf{y}} + (\alpha^{2}-\alpha)\norm{\mathbf{x}}^{2} + (\alpha^{2}-\alpha)\norm{\mathbf{y}}^{2}\\
            &= \alpha f(\mathbf{x}) + (1-\alpha) f(\mathbf{y}) - \alpha(1-\alpha)(\norm{\mathbf{x}}^{2} - 2\inprod{\mathbf{x}}{\mathbf{y}} + \norm{\mathbf{y}}^{2})\\
            &= \alpha f(\mathbf{x}) + (1-\alpha) f(\mathbf{y}) - \alpha(1-\alpha)\norm{\mathbf{x}-\mathbf{y}}^{2}\\
            &\leq \alpha f(\mathbf{x}) + (1-\alpha) f(\mathbf{y}).
        \end{align*}
        Therefore, $f$ is convex.
    \end{eg}
    \begin{eg}
        Let $V$ be a vector space, and let $f: V \to \mathbb{R}$ be defined by:
        \begin{equation*}
            f(\mathbf{x}) = g(\mathbf{x}) + b, \qquad \text{for } \mathbf{x} \in V,
        \end{equation*}
        where $g(\mathbf{x})$ is linear and $b$ is a constant. For all $\mathbf{x}, \mathbf{y} \in V$ and $\alpha \in [0,1]$,
        \begin{align*}
            f(\alpha \mathbf{x} + (1-\alpha)\mathbf{y}) &= g(\alpha \mathbf{x} + (1-\alpha)\mathbf{y}) + b\\
            &= \alpha g(\mathbf{x}) + (1-\alpha) g(\mathbf{y}) + (\alpha b + (1-\alpha) b)\\
            &= \alpha (g(\mathbf{x}) + b) + (1-\alpha)(g(\mathbf{y}) + b)\\
            &= \alpha f(\mathbf{x}) + (1-\alpha)f(\mathbf{y}).
        \end{align*}
        Therefore, $f$ is convex.
    \end{eg}
    \newpage

    \begin{thm}
        Let $V$ be a vector space, and let $f_{1}, f_{2}, \cdots, f_{n}: V \to \mathbb{R}$ be convex functions. Let $f: V \to \mathbb{R}$ be defined by:
        \begin{equation*}
            f(\mathbf{x}) = \sum_{i=1}^{n}c_{i}f_{i}(\mathbf{x}), \qquad \text{for } \mathbf{x} \in V,
        \end{equation*}
        where $c_{i} \geq 0$ for $i=1, \cdots, n$. Then $f$ is convex.
    \end{thm}
    \begin{proofing}
        For all $\mathbf{x}, \mathbf{y} \in V$ and $\alpha \in [0,1]$, since $c_{i} \geq 0$,
        \begin{align*}
            f(\alpha \mathbf{x} + (1-\alpha)\mathbf{y}) &= \sum_{i=1}^{n}c_{i}f_{i}(\alpha \mathbf{x} + (1-\alpha)\mathbf{y})\\
            &\leq \sum_{i=1}^{n}c_{i}(\alpha f_{i}(\mathbf{x}) + (1-\alpha)f_{i}(\mathbf{y}))\\
            &= \alpha\sum_{i=1}^{n}c_{i}f_{i}(\mathbf{x}) + (1-\alpha)\sum_{i=1}^{n}c_{i}f_{i}(\mathbf{y})\\
            &= \alpha f(\mathbf{x}) + (1-\alpha) f(\mathbf{y}).
        \end{align*}
        Therefore, $f$ is convex.
    \end{proofing}
    \begin{rem}
        Both $f_{1}$ and $f_{2}$ being convex does not necessarily imply that $f_{1}-f_{2}$ is convex.
    \end{rem}
    \begin{eg}
        Let $f_{1}, f_{2}: \mathbb{R} \to \mathbb{R}$ be defined by:
        \begin{align*}
            f_{1}(x) &= x, & f_{2}(x) &= x^{2}, \qquad \text{for } x \in \mathbb{R}.
        \end{align*}
        If we set $x = 0$, $y = 1$, $\alpha = 0.5$,
        \begin{align*}
            (f_{1}-f_{2})(\alpha x + (1-\alpha)y) &= 0.25, & \alpha(f_{1}-f_{2})(x) + (1-\alpha)(f_{1}-f_{2})(y) &= 0 < (f_{1}-f_{2})(\alpha x + (1-\alpha)y).
        \end{align*}
        Therefore, $f_{1}-f_{2}$ is not convex.
    \end{eg}
    \begin{thm}
        Let $V$ be a vector space. Let $f: \mathbb{R} \to \mathbb{R}$ be convex, and let $g: V \to \mathbb{R}$ be affine. Then $f \circ g$ is convex.
    \end{thm}
    \begin{proofing}
        For all $\mathbf{x}, \mathbf{y} \in V$ and $\alpha \in [0,1]$,
        \begin{equation*}
            (f \circ g)(\alpha \mathbf{x} + (1-\alpha)\mathbf{y}) = f(\alpha g(\mathbf{x}) + (1-\alpha) g(\mathbf{y})) \leq \alpha (f \circ g)(\mathbf{x}) + (1-\alpha) (f \circ g)(\mathbf{y}).
        \end{equation*}
        Therefore, $f \circ g$ is convex.
    \end{proofing}
    \begin{rem}
        $f$ and $g$ being convex does not necessarily imply that $f \circ g$ is convex.
    \end{rem}
    \begin{eg}
        Let $f, g: \mathbb{R} \to \mathbb{R}$ be defined by:
        \begin{align*}
            f(x) &= -\ln(x), & g(x) &= \frac{1}{x}, \qquad \text{for } x > 0.
        \end{align*}
        We know that both $f$ and $g$ are convex since they are decreasing. However,
        \begin{equation*}
            (f \circ g)(x) = \ln(x),
        \end{equation*}
        which is an increasing function. Therefore, $f \circ g$ is not convex.
    \end{eg}
    \newpage

    Before proving that $f$ being convex is sufficient to find a solution, we first need the following lemma.
    \begin{lem}
        \label{Case Study G (Lemma) Convex Function is Equivalent to Linear Approximation Support}
        Let $H$ be a Hilbert space. If $f: H \to \mathbb{R}$ is differentiable, then:
        \begin{equation*}
            f \text{ is convex} \iff f(\mathbf{y}) \geq f(\mathbf{x}) + \inprod{\nabla f(\mathbf{x})}{\mathbf{y}-\mathbf{x}}, \qquad \text{for } \mathbf{x}, \mathbf{y} \in H.
        \end{equation*}
    \end{lem}
    \begin{proofing}
        \begin{itemize}
            \item[$\Longrightarrow$] We first prove the case when $H = \mathbb{R}$. Assume that $f: \mathbb{R} \to \mathbb{R}$ is convex. Then, for any $x, y \in \mathbb{R}$ and $\alpha \in [0,1)$,
            \begin{align*}
                f(y) &\geq \frac{f(\alpha x + (1-\alpha)y) - \alpha f(x)}{1-\alpha}\\
                &\geq f(x) + \frac{f(\alpha x + (1-\alpha)y) - f(x)}{1-\alpha}\\
                &\geq f(x) + \frac{f(x + (1-\alpha)(y-x)) - f(x)}{(1-\alpha)(y-x)}(y-x).
            \end{align*}
            As $\alpha \to 1^{-}$, for all $x, y \in \mathbb{R}$,
            \begin{equation*}
                f(y) \geq f(x) + f'(x)(y-x).
            \end{equation*}
            Now, we prove the general case. Assume that $f: H \to \mathbb{R}$ is convex. For any $\mathbf{x}, \mathbf{y} \in H$, let:
            \begin{equation*}
                g(t) = f(t \mathbf{x} + (1-t) \mathbf{y}), \qquad \text{for } t \in \mathbb{R}.
            \end{equation*}
            For any $s, t \in \mathbb{R}$ and $\alpha \in [0,1]$,
            \begin{align*}
                g(\alpha s + (1-\alpha)t) &= f((\alpha s + (1-\alpha)t)\mathbf{x} + (1-\alpha s - (1-\alpha)t)\mathbf{y})\\
                &= f(\alpha (s \mathbf{x}+(1-s)\mathbf{y}) + (1-\alpha)(t \mathbf{x}) + (1-t) \mathbf{y})\\
                &\leq \alpha f(s \mathbf{x} + (1-s) \mathbf{y}) + (1-\alpha) f(t \mathbf{x} + (1-t) \mathbf{y}) = \alpha g(s) + (1-\alpha) g(t).
            \end{align*}
            Therefore, $g$ is convex. Moreover, by Theorem \ref{Chapter 4 (Theorem) Derivative on not t=0},
            \begin{equation*}
                g'(t) = \inprod{\nabla f(t\mathbf{x} + (1-t)\mathbf{y})}{\mathbf{x}-\mathbf{y}}.
            \end{equation*}
            Using our previous result,
            \begin{align*}
                g(0) &\geq g(1) + g'(1)(0-1)\\
                f(\mathbf{y}) &\geq f(\mathbf{x}) + \inprod{\nabla f(\mathbf{x})}{\mathbf{y}-\mathbf{x}}.
            \end{align*}
            \item[$\Longleftarrow$] Assume that $f(\mathbf{y}) \geq f(\mathbf{x}) + \inprod{\nabla f(\mathbf{x})}{\mathbf{y}-\mathbf{x}}$ for all $\mathbf{x}, \mathbf{y} \in H$. For any $\alpha \in [0,1]$, we define:
            \begin{equation*}
                \mathbf{z} = \alpha \mathbf{x} + (1-\alpha) \mathbf{y}.
            \end{equation*}
            Therefore, applying the inequality,
            \begin{align*}
                f(\mathbf{x}) &\geq f(\mathbf{z}) + \inprod{\nabla f(\mathbf{z})}{\mathbf{x}-\mathbf{z}} = f(\mathbf{z}) + (1-\alpha) \inprod{\nabla f(\mathbf{z})}{\mathbf{x}-\mathbf{y}}\\
                f(\mathbf{y}) &\geq f(\mathbf{z}) + \inprod{\nabla f(\mathbf{z})}{\mathbf{y}-\mathbf{z}} = f(\mathbf{z}) + \alpha \inprod{\nabla f(\mathbf{z})}{\mathbf{y}-\mathbf{x}}.
            \end{align*}
            We have:
            \begin{align*}
                \alpha f(\mathbf{x}) + (1-\alpha) f(\mathbf{y}) &\geq f(\mathbf{z}) + \alpha(1-\alpha) \inprod{\nabla f(\mathbf{z})}{\mathbf{x}-\mathbf{y}} + \alpha(1-\alpha) \inprod{\nabla f(\mathbf{z})}{\mathbf{y}-\mathbf{x}}\\
                &\geq f(\mathbf{z}) = f(\alpha \mathbf{x} + (1-\alpha) \mathbf{y}).
            \end{align*}
            Therefore, $f$ is convex.
        \end{itemize}
    \end{proofing}
    \begin{thm}\named{1st order optimality condition}
        If $f: H \to \mathbb{R}$ is convex and differentiable at $\mathbf{x}^{*} \in H$, then:
        \begin{equation*}
            \mathbf{x}^{*} = \argmin_{\mathbf{x} \in H} f(\mathbf{x}) \iff \nabla f(\mathbf{x}^{*}) = \mathbf{0}.
        \end{equation*}
    \end{thm}
    \begin{proofing}
        \begin{itemize}
            \item[$\Longrightarrow$] We have proven this in Theorem \ref{Case Study G (Theorem) Solution has Gradient=0}.
            \item[$\Longleftarrow$] We have $\nabla f(\mathbf{x}^{*}) = \mathbf{0}$ for some $\mathbf{x}^{*} \in H$. Since $f$ is convex and differentiable at $\mathbf{x}^{*}$, by Lemma \ref{Case Study G (Lemma) Convex Function is Equivalent to Linear Approximation Support},
            \begin{equation*}
                f(\mathbf{x}) \geq f(\mathbf{x}^{*}) + \inprod{\nabla f(\mathbf{x}^{*})}{\mathbf{x}-\mathbf{x}^{*}} = f(\mathbf{x}^{*}), \qquad \text{for } \mathbf{x} \in H.
            \end{equation*}
            Therefore, by the 0th order optimality condition,
            \begin{equation*}
                \mathbf{x}^{*} = \argmin_{\mathbf{x} \in H} f(\mathbf{x}).
            \end{equation*}
        \end{itemize}
    \end{proofing}

\chapter{Gradient Descent}
    \label{Case Study H: Gradient Descent}
    This case study assumes that you have already read Chapter \ref{Chapter 4: Linear Functions and Differentiation} and Appendix \ref{Case Study G: Solvability and Optimality}.

    Let $H$ be a Hilbert space. Suppose we have a function $f: H \to \mathbb{R}$. We aim to find:
    \begin{equation*}
        \mathbf{x}^{*} = \argmin_{\mathbf{x} \in H} f(\mathbf{x}).
    \end{equation*}
    How can we find the solution numerically? Assume we have an estimate $\mathbf{x}^{(k)}$ of $\mathbf{x}^{*}$. How can we find a better estimate $\mathbf{x}^{(k+1)}$? If $f$ is differentiable at $\mathbf{x}^{(k)}$, we can use an affine approximation:
    \begin{equation*}
        f(\mathbf{x}) \approx f(\mathbf{x}^{(k)}) + \inprod{\nabla f(\mathbf{x}^{(k)})}{\mathbf{x}-\mathbf{x}^{(k)}}.
    \end{equation*}
    This approximation is accurate only if $\snorm{\mathbf{x}-\mathbf{x}^{(k)}}$ is small. Once $\mathbf{x}^{(k)} = \mathbf{x}^{*}$, we have $f(\mathbf{x}) \approx f(\mathbf{x}^{*})$. Instead of finding $\mathbf{x}^{(k+1)}$ globally, we can focus locally:
    \begin{equation*}
        \mathbf{x}^{(k+1)} = \argmin_{\mathbf{x} \in H} \left(f(\mathbf{x}^{(k)}) + \inprod{\nabla f(\mathbf{x}^{(k)})}{\mathbf{x}-\mathbf{x}^{(k)}}\right) \text{ such that } \snorm{\mathbf{x}-\mathbf{x}^{(k)}} \leq \alpha_{k}\snorm{\nabla f(\mathbf{x}^{(k)})},
    \end{equation*}
    where $\alpha_{k} > 0$ is a small number. The idea is that $\snorm{\nabla f(\mathbf{x}^{(k)})}$ will slowly approach $0$ as $\mathbf{x}^{(k)}$ gets closer to $\mathbf{x}^{*}$, requiring smaller steps to converge gradually with a sufficiently small $\alpha_{k}$. This is equivalent to solving:
    \begin{equation*}
        \min_{\mathbf{x} \in H} \inprod{\nabla f(\mathbf{x}^{(k)})}{\mathbf{x}-\mathbf{x}^{(k)}} \text{ such that } \snorm{\mathbf{x}-\mathbf{x}^{(k)}} \leq \alpha_{k}\snorm{\nabla f(\mathbf{x}^{(k)})}.
    \end{equation*}
    By the Cauchy-Schwarz inequality,
    \begin{equation*}
        \inprod{\nabla f(\mathbf{x}^{(k)})}{\mathbf{x}-\mathbf{x}^{(k)}} \geq -\snorm{\nabla f(\mathbf{x}^{(k)})}\snorm{\mathbf{x}-\mathbf{x}^{(k)}} \geq -\alpha_{k}\snorm{\nabla f(\mathbf{x}^{(k)})}^{2}.
    \end{equation*}
    To attain equality, we require:
    \begin{equation*}
        \mathbf{x} - \mathbf{x}^{(k)} = -\alpha_{k} \nabla f(\mathbf{x}^{(k)}).
    \end{equation*}
    Therefore, we can rewrite the formula as:
    \begin{equation*}
        \mathbf{x}^{(k+1)} = \mathbf{x}^{(k)} - \alpha_{k} \nabla f(\mathbf{x}^{(k)}).
    \end{equation*}
    We have the following algorithm:
    \begin{defn}
        The \textbf{Gradient Descent algorithm} is a method to minimize a differentiable function $f: H \to \mathbb{R}$. The steps of the algorithm are as follows:
        \begin{itemize}
            \item[0:] Initialize $\mathbf{x}^{(0)}$.
            \item[1:] For $k = 0, 1, \cdots$,
            \begin{enumerate}
                \item Choose the step size $\alpha_{k}$.
                \item Update $\mathbf{x}^{(k+1)}$ by:
                \begin{equation*}
                    \mathbf{x}^{(k+1)} = \mathbf{x}^{(k)} - \alpha_{k} \nabla f(\mathbf{x}^{(k)}).
                \end{equation*}
            \end{enumerate}
            \item[] Repeat until convergence is achieved.
        \end{itemize}
    \end{defn}
    If it converges, then $\nabla f(\mathbf{x}^{(\infty)}) = \mathbf{0}$. From Appendix \ref{Case Study G: Solvability and Optimality},
    \begin{enumerate}
        \item If $f$ is convex, then $\mathbf{x}^{(\infty)}$ is the solution.
        \item If $f$ is non-convex, then $\mathbf{x}^{(\infty)}$ may not be a global minimizer.
    \end{enumerate}
    \newpage

    We can focus on a prominent example: optimization in least squares:
    \begin{equation*}
        \tag{LS}
        \min_{\mathbf{x} \in \mathbb{R}^{n}} \frac{1}{2}\pnorm[2]{\mathbf{A}\mathbf{x}-\mathbf{b}}^{2},
    \end{equation*}
    where $\mathbf{A} \in \mathbb{R}^{m \times n}$ and $\mathbf{b} \in \mathbb{R}^{m}$ are given. This problem often arises in regressions (Appendix \ref{Case Study D: Linear Regression}), such as:
    \begin{enumerate}
        \item Linear regression: For given $\mathbf{X} \in \mathbb{R}^{N \times (n+1)}$ and $\mathbf{y} \in \mathbb{R}^{N}$,
        \begin{equation*}
            \min_{\boldsymbol{\beta} \in \mathbb{R}^{n+1}} \pnorm[2]{\mathbf{X}\boldsymbol{\beta} - \mathbf{y}}^{2}.
        \end{equation*}
        \item Ridge regression: For given $\mathbf{X} \in \mathbb{R}^{N \times (n+1)}$, $\mathbf{y} \in \mathbb{R}^{N}$, and $\lambda > 0$,
        \begin{equation*}
            \min_{\boldsymbol{\beta} = \begin{pmatrix}
                    \mathbf{a}\\
                    b
                \end{pmatrix} \in \mathbb{R}^{n+1}} \left(\pnorm[2]{\mathbf{X}\boldsymbol{\beta} - \mathbf{y}}^{2} + \lambda \pnorm[2]{\mathbf{a}}^{2}\right).
        \end{equation*}
    \end{enumerate}
    Let $f(\mathbf{x}) = \frac{1}{2}\pnorm[2]{\mathbf{A}\mathbf{x} - \mathbf{b}}^{2}$ for $\mathbf{x} \in \mathbb{R}^{n}$. For any $\mathbf{x}, \mathbf{y} \in \mathbb{R}^{n}$ and $\alpha \in [0,1]$,
    \begin{align*}
        f(\alpha \mathbf{x} + (1-\alpha) \mathbf{y}) &= \frac{1}{2}\pnorm[2]{\mathbf{A}(\alpha \mathbf{x} + (1-\alpha) \mathbf{y}) - \mathbf{b}}^{2}\\
        &= \frac{1}{2}\pnorm[2]{\alpha(\mathbf{A}\mathbf{x} - \mathbf{b}) + (1-\alpha)(\mathbf{A}\mathbf{y} - \mathbf{b})}^{2}\\
        &\leq \frac{\alpha}{2}\pnorm[2]{\mathbf{A}\mathbf{x} - \mathbf{b}}^{2} + \frac{1-\alpha}{2}\pnorm[2]{\mathbf{A}\mathbf{y} - \mathbf{b}}^{2} = \alpha f(\mathbf{x}) + (1-\alpha) f(\mathbf{y}).
    \end{align*}
    Therefore, $f$ is convex. Before proving that $f$ is differentiable, we must first prove the following theorem.
    \begin{thm}
        Let $g: \mathbb{R}^{m} \to \mathbb{R}$ be a differentiable function, and let $\mathbf{A} \in \mathbb{R}^{m \times n}$. Define $h: \mathbb{R}^{n} \to \mathbb{R}$ by:
        \begin{equation*}
            h(\mathbf{x}) = g(\mathbf{A}\mathbf{x}), \qquad \text{for } \mathbf{x} \in \mathbb{R}^{n}.
        \end{equation*}
        Then, for any $\mathbf{x} \in \mathbb{R}^{n}$:
        \begin{equation*}
            \nabla h(\mathbf{x}) = \mathbf{A}^{T}\nabla g(\mathbf{A}\mathbf{x}).
        \end{equation*}
    \end{thm}
    \begin{proofing}
        We have $\pnorm[2]{\mathbf{A}\mathbf{y} - \mathbf{A}\mathbf{x}} \leq \pnorm[2]{\mathbf{A}}\pnorm[2]{\mathbf{y} - \mathbf{x}}$. As $\pnorm[2]{\mathbf{y}-\mathbf{x}} \to 0$, by the Squeeze Theorem, $\pnorm[2]{\mathbf{A}\mathbf{y} - \mathbf{A}\mathbf{x}} \to 0$. Therefore,
        \begin{align*}
            0 &\leq \lim_{\pnorm[2]{\mathbf{y} - \mathbf{x}} \to 0} \frac{\abs{h(\mathbf{y}) - (h(\mathbf{x}) + \inprod{\mathbf{A}^{T}\nabla g(\mathbf{A}\mathbf{x})}{\mathbf{y}-\mathbf{x}})}}{\pnorm[2]{\mathbf{y} - \mathbf{x}}}\\
            &\leq \lim_{\pnorm[2]{\mathbf{A}\mathbf{y} - \mathbf{A}\mathbf{x}} \to 0} \frac{\abs{g(\mathbf{A}\mathbf{y}) - (g(\mathbf{A}\mathbf{x}) + \inprod{\mathbf{A}^{T}\nabla g(\mathbf{A}\mathbf{x})}{\mathbf{y}-\mathbf{x}})}}{\pnorm[2]{\mathbf{y} - \mathbf{x}}}\\
            &\leq \pnorm[2]{\mathbf{A}}\lim_{\pnorm[2]{\mathbf{A}\mathbf{y} - \mathbf{A}\mathbf{x}} \to 0} \frac{\abs{g(\mathbf{A}\mathbf{y}) - (g(\mathbf{A}\mathbf{x}) + \inprod{\mathbf{A}^{T}\nabla g(\mathbf{A}\mathbf{x})}{\mathbf{y}-\mathbf{x}})}}{\pnorm[2]{\mathbf{A}\mathbf{y} - \mathbf{A}\mathbf{x}}} = 0.
        \end{align*}
        Thus, by definition, $\nabla h(\mathbf{x}) = \mathbf{A}^{T} \nabla g(\mathbf{A}\mathbf{x})$.
    \end{proofing}

    Based on this theorem, let $g(\mathbf{y}) = \frac{1}{2}\pnorm[2]{\mathbf{y}-\mathbf{b}}^{2}$ for any $\mathbf{y} \in \mathbb{R}^{m}$. We have:
    \begin{equation*}
        \nabla f(\mathbf{x}) = \mathbf{A}^{T}\nabla g(\mathbf{A}\mathbf{x}) = \mathbf{A}^{T}(\mathbf{A}\mathbf{x} - \mathbf{b}).
    \end{equation*}
    Thus, $f$ is differentiable. By the 1st order optimality condition,
    \begin{equation*}
        \mathbf{x}^{*} = \argmin_{\mathbf{x} \in \mathbb{R}^{n}} \frac{1}{2}\pnorm[2]{\mathbf{A}\mathbf{x} - \mathbf{b}}^{2} \iff \nabla f(\mathbf{x}^{*}) = \mathbf{0} \iff \mathbf{A}^{T}\mathbf{A}\mathbf{x} = \mathbf{A}^{T}\mathbf{b}.
    \end{equation*}
    \newpage

    If we apply the gradient descent algorithm to least squares, for $k = 0, 1, 2, \cdots$,
    \begin{equation*}
        \mathbf{x}^{(k+1)} = \mathbf{x}^{(k)} - \alpha_{k}\mathbf{A}^{T}(\mathbf{A}\mathbf{x}^{(k)} - \mathbf{b}).
    \end{equation*}
    How do we choose the step sizes $\alpha_{k}$? By substituting the gradient descent formula into $f$, we get:
    \begin{equation*}
        \alpha_{k} = \argmin_{\alpha \in \mathbb{R}} \frac{1}{2}\spnorm[2]{\mathbf{A}(\mathbf{x}^{(k)} - \alpha \mathbf{A}^{T}(\mathbf{A}\mathbf{x}^{(k)} - \mathbf{b})) - \mathbf{b}}^{2},
    \end{equation*}
    which finds the optimal step size such that $f$ is minimized in the next step of the gradient descent. Denote:
    \begin{align*}
        g(\alpha) &= \frac{1}{2}\spnorm[2]{\mathbf{A}(\mathbf{x}^{(k)} - \alpha \mathbf{A}^{T}(\mathbf{A}\mathbf{x}^{(k)} - \mathbf{b})) - \mathbf{b}}^{2}\\
        &= \frac{1}{2}\spnorm[2]{\mathbf{A}\mathbf{x}^{(k)} - \mathbf{b}}^{2} - \alpha (\mathbf{A}\mathbf{x}^{(k)} - \mathbf{b})^{T}\mathbf{A}\mathbf{A}^{T}(\mathbf{A}\mathbf{x}^{(k)} - \mathbf{b}) + \frac{\alpha^{2}}{2} \spnorm[2]{\mathbf{A}\mathbf{A}^{T}(\mathbf{A}\mathbf{x}^{(k)} - \mathbf{b})}^{2}\\
        &= \frac{1}{2}\spnorm[2]{\mathbf{A}\mathbf{x}^{(k)} - \mathbf{b}}^{2} - \alpha \spnorm[2]{\mathbf{A}^{T}(\mathbf{A}\mathbf{x}^{(k)} - \mathbf{b})}^{2} + \frac{\alpha^{2}}{2} \spnorm[2]{\mathbf{A}\mathbf{A}^{T}(\mathbf{A}\mathbf{x}^{(k)} - \mathbf{b})}^{2},\\
        g'(\alpha) &= -\spnorm[2]{\mathbf{A}^{T}(\mathbf{A}\mathbf{x}^{(k)} - \mathbf{b})}^{2} + \alpha \spnorm[2]{\mathbf{A}\mathbf{A}^{T}(\mathbf{A}\mathbf{x}^{(k)} - \mathbf{b})}^{2}.
    \end{align*}
    By substituting $g'(\alpha_{k}) = 0$,
    \begin{align*}
        0 &= g'(\alpha_{k}) = -\spnorm[2]{\mathbf{A}^{T}(\mathbf{A}\mathbf{x}^{(k)} - \mathbf{b})}^{2} + \alpha_{k} \spnorm[2]{\mathbf{A}\mathbf{A}^{T}(\mathbf{A}\mathbf{x}^{(k)} - \mathbf{b})}^{2},\\
        \alpha_{k} &= \frac{\spnorm[2]{\mathbf{A}^{T}(\mathbf{A}\mathbf{x}^{(k)} - \mathbf{b})}^{2}}{\spnorm[2]{\mathbf{A}\mathbf{A}^{T}(\mathbf{A}\mathbf{x}^{(k)} - \mathbf{b})}^{2}}.
    \end{align*}
    We have the following algorithm:
    \begin{defn}
        The \textbf{Steepest Descent algorithm} is a method to find the solution of $\mathbf{A}\mathbf{x} = \mathbf{b}$ for given $\mathbf{A} \in \mathbb{R}^{m \times n}$ and $\mathbf{b} \in \mathbb{R}^{m}$. The steps of the algorithm are as follows:
        \begin{itemize}
            \item[0:] Initialize $\mathbf{x}^{(0)}$.
            \item[1:] For $k = 0, 1, \cdots$,
            \begin{enumerate}
                \item Compute $\mathbf{g}^{(k)}$ by:
                \begin{equation*}
                    \mathbf{g}^{(k)} = \mathbf{A}^{T}(\mathbf{A}\mathbf{x}^{(k)} - \mathbf{b}).
                \end{equation*}
                \item Compute $\alpha_{k}$ by:
                \begin{equation*}
                    \alpha_{k} = \frac{\pnorm[2]{\mathbf{g}^{(k)}}^{2}}{\pnorm[2]{\mathbf{A}\mathbf{g}^{(k)}}^{2}}.
                \end{equation*}
                \item Update $\mathbf{x}^{(k+1)}$ by:
                \begin{equation*}
                    \mathbf{x}^{(k+1)} = \mathbf{x}^{(k)} - \alpha_{k}\mathbf{g}^{(k)}.
                \end{equation*}
            \end{enumerate}
            \item[] Repeat until convergence is achieved.
        \end{itemize}
    \end{defn}
\end{document}